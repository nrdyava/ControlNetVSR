{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3ad208-d4da-46e5-9d10-4d511c2c73e1",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5339f56-f8db-40a5-a63a-04ac1fc166c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import requests\n",
    "from transformers import AutoTokenizer, PretrainedConfig, CLIPTextModel, CLIPImageProcessor\n",
    "import diffusers\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    ControlNetModel,\n",
    "    DDPMScheduler,\n",
    "    StableDiffusionControlNetPipeline, \n",
    "    StableDiffusionControlNetImg2ImgPipeline,\n",
    "    UNet2DConditionModel,\n",
    "    UniPCMultistepScheduler,\n",
    ")\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torch, torchvision\n",
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import lightning as L\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "import torchvision.transforms as Tvt\n",
    "from torchvision.models.optical_flow import raft_small, raft_large\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74618f5d-d324-4920-8f83-87ee3c6d3772",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd033f1e-3a92-4405-80cc-17473cfb0f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_dataset(Dataset):\n",
    "    def __init__(self, train_dir, temporal_radius = 1):\n",
    "        self.train_dir = train_dir\n",
    "        self.temporal_radius = temporal_radius\n",
    "        self.video_names = os.listdir(os.path.join(train_dir, \"test_sharp\"))\n",
    "        self.eligible_frames = [i for i in range(self.temporal_radius, 100-self.temporal_radius)]\n",
    "        self.n_videos = len(self.video_names)\n",
    "        self.n_eligible_frames = len(self.eligible_frames)\n",
    "        self.n_total_eligible_images = self.n_videos * self.n_eligible_frames\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stable-diffusion-x4-upscaler\", subfolder=\"tokenizer\")\n",
    "        \n",
    "        self.lr_h_bound = 180 - 128\n",
    "        self.lr_w_bound = 320 - 128\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_total_eligible_images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid_name = '{:03d}'.format(idx//self.n_eligible_frames)\n",
    "        frame_name = '{:08d}.png'.format(self.temporal_radius + idx%self.n_eligible_frames)\n",
    "        lr_iminus1_frame_name = '{:08d}.png'.format(self.temporal_radius + (idx%self.n_eligible_frames)-1)\n",
    "        lr_iplus1_frame_name = '{:08d}.png'.format(self.temporal_radius + (idx%self.n_eligible_frames)+1)\n",
    "        \n",
    "        hr_frame = os.path.join(self.train_dir, \"test_sharp\", vid_name, frame_name)\n",
    "        lr_frame = os.path.join(self.train_dir, \"test_sharp_bicubic\", \"X4\", vid_name, frame_name)\n",
    "        lr_iminus1_frame = os.path.join(self.train_dir, \"test_sharp_bicubic\", \"X4\", vid_name, lr_iminus1_frame_name)\n",
    "        lr_iplus1_frame = os.path.join(self.train_dir, \"test_sharp_bicubic\", \"X4\", vid_name, lr_iplus1_frame_name)\n",
    "\n",
    "        hr_img = torchvision.io.read_image(hr_frame)\n",
    "        lr_img = torchvision.io.read_image(lr_frame)\n",
    "        lr_iminus1_img = torchvision.io.read_image(lr_iminus1_frame)\n",
    "        lr_iplus1_img = torchvision.io.read_image(lr_iplus1_frame)\n",
    "\n",
    "        ## Random Crop\n",
    "        x = random.randint(0, self.lr_h_bound)\n",
    "        y = random.randint(0, self.lr_w_bound)\n",
    "\n",
    "        hr_img = hr_img[:, x*4:(x*4)+512, y*4:y*4+512]\n",
    "        lr_img = lr_img[:, x:x+128, y:y+128]\n",
    "        lr_iminus1_img = lr_iminus1_img[:, x:x+128, y:y+128]\n",
    "        lr_iplus1_img = lr_iplus1_img[:, x:x+128, y:y+128]\n",
    "\n",
    "        captions = [\"\"]\n",
    "        text_inputs = self.tokenizer(captions, max_length=self.tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "        \n",
    "        return {\"hr_img\": hr_img, \"lr_img\": lr_img, \"lr_iminus1_img\": lr_iminus1_img, \"lr_iplus1_img\": lr_iplus1_img, \"text_encoder_inp_ids\": text_inputs}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dba040-01bd-4206-add1-60da23391bdc",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0283d-522c-4600-a0ea-3bcf70ada2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlNetConditioningEmbeddingCustom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(6, 12, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(12, 24, kernel_size=3, padding=1)\n",
    "        self.conv3 = torch.nn.Conv2d(24, 96, kernel_size=3, padding=1)\n",
    "        self.conv4 = torch.nn.Conv2d(96, 256, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, conditioning):\n",
    "        embedding = self.conv1(conditioning)\n",
    "        embedding = torch.nn.functional.silu(embedding)\n",
    "        embedding = self.conv2(embedding)\n",
    "        embedding = torch.nn.functional.silu(embedding)\n",
    "        embedding = self.conv3(embedding)\n",
    "        embedding = torch.nn.functional.silu(embedding)\n",
    "        embedding = self.conv4(embedding)\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac10abb2-e337-40eb-a51a-a57b2f0a37e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VSRDiffuser(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model_id = \"stabilityai/stable-diffusion-x4-upscaler\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, subfolder=\"tokenizer\")\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(self.model_id, subfolder=\"text_encoder\")\n",
    "        self.noise_scheduler = DDPMScheduler.from_pretrained(self.model_id, subfolder=\"scheduler\")\n",
    "        self.vae = AutoencoderKL.from_pretrained(self.model_id, subfolder=\"vae\")\n",
    "        self.unet = UNet2DConditionModel.from_pretrained(self.model_id, subfolder=\"unet\")\n",
    "        self.controlnet = ControlNetModel.from_unet(self.unet)\n",
    "        self.controlnet.controlnet_cond_embedding = ControlNetConditioningEmbeddingCustom()\n",
    "\n",
    "        self.weight_dtype = torch.float32 \n",
    "        \n",
    "        #RAFT Model for Optical Flow estimation model (RAFT SMALL/RAFT LARGE)\n",
    "        #self.RAFT = raft_small(pretrained=True, progress=False)\n",
    "        self.RAFT = raft_large(pretrained=True, progress=False)\n",
    "\n",
    "        self.transforms1 = Tvt.Compose(\n",
    "            [\n",
    "                Tvt.ConvertImageDtype(self.weight_dtype),\n",
    "                Tvt.Normalize(mean=0.5, std=0.5)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Steps\n",
    "        # 1. Optical Flow b/w i and i-1th frames and motion compensation\n",
    "        # 2. Optical Flow b/w i and i+1th frames and motion compensation\n",
    "        # 3. Depthwise Sepearable and Pointwise seperable covolutions - conv1 to conv4\n",
    "        # 4. HR image -> VAE Encoder -> HR latent\n",
    "        # 5. Sample noise and convert HR latents -> noised latents\n",
    "        # 6. Take the denoising step with Unet\n",
    "        # 7. Calculate the loss and do back propagation\n",
    "\n",
    "        batch_lr_i = self.transforms1(batch[\"lr_img\"])\n",
    "        batch_lr_iminus1 = self.transforms1(batch[\"lr_iminus1_img\"])\n",
    "        batch_lr_iplus1 = self.transforms1(batch[\"lr_iplus1_img\"])\n",
    "        batch_hr_i = self.transforms1(batch[\"hr_img\"])\n",
    "        batch_text_input_ids = batch[\"text_encoder_inp_ids\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            list_of_flows_iminus1 = self.RAFT(batch_lr_iminus1, batch_lr_i)[-1]\n",
    "            list_of_flows_iplus1 = self.RAFT(batch_lr_iplus1, batch_lr_i)[-1]\n",
    "        \n",
    "        LRiminus1_hat = torch.nn.functional.grid_sample(batch_lr_iminus1, list_of_flows_iminus1.permute(0, 2, 3, 1))\n",
    "        LRiplus1_hat = torch.nn.functional.grid_sample(batch_lr_iplus1, list_of_flows_iplus1.permute(0, 2, 3, 1))\n",
    "\n",
    "        # Calculate Latents of Ground Truth\n",
    "        latents = self.vae.encode(batch_hr_i).latent_dist.sample()\n",
    "        latents = latents * self.vae.config.scaling_factor\n",
    "\n",
    "        # Generate Noise\n",
    "        noise = torch.randn_like(latents)\n",
    "\n",
    "        bsz = latents.shape[0]\n",
    "\n",
    "        # Randomly Generate Timesteps\n",
    "        timesteps = torch.randint(0, self.noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "        timesteps = timesteps.long()\n",
    "\n",
    "        # Add noise to the GT Latents\n",
    "        noisy_latents = self.noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        encoder_hidden_states = self.text_encoder(batch_text_input_ids, return_dict=False)[0]\n",
    "\n",
    "        controlnet_image = torch.cat([LRiminus1_hat, LRiplus1_hat], dim = 1)\n",
    "\n",
    "        down_block_res_samples, mid_block_res_sample = self.controlnet(\n",
    "            torch.cat([noisy_latents, batch_lr_i], dim = 1),\n",
    "            timesteps,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            controlnet_cond=controlnet_image,\n",
    "            class_labels = torch.zeros(1).to(torch.int).to('cuda'),\n",
    "            return_dict=False,\n",
    "        )\n",
    "\n",
    "        model_pred = self.unet(\n",
    "            torch.cat([noisy_latents, batch_lr_i], dim = 1),\n",
    "            timesteps,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            down_block_additional_residuals=[sample.to(dtype=self.weight_dtype) for sample in down_block_res_samples],\n",
    "            mid_block_additional_residual=mid_block_res_sample.to(dtype=self.weight_dtype),\n",
    "            class_labels = torch.zeros(1).to(torch.int).to('cuda'),\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "\n",
    "        target = self.noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "        \n",
    "        print(\"batch: {} => Loss: {}\".format(batch_idx, loss))\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "        optimizer = optimizer_class(\n",
    "            self.controlnet.parameters(),\n",
    "            lr=1e-5,\n",
    "            betas=(0.9, 0.999),\n",
    "            weight_decay=1e-2,\n",
    "            eps=1e-08,\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9830bc5d-1914-40b5-800a-85c495cc3b6b",
   "metadata": {},
   "source": [
    "## Load the Model from Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9902ab-7cf0-46ad-b3d8-20e4c79b8bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"checkpoints/control-net-v2/epoch=3-step=5216.ckpt\"\n",
    "model = VSRDiffuser.load_from_checkpoint(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5827ab6b-a31b-49e8-9069-0f3790439159",
   "metadata": {},
   "source": [
    "## Predict on Image Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b94d95b-cb68-44a5-a65a-6a8bc1041a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_frame(model, hr_img, lr_img, lr_iminus1_img, lr_iplus1_img, nsteps=50):\n",
    "    tokenizer = model.tokenizer\n",
    "    text_encoder = model.text_encoder\n",
    "    noise_scheduler = model.noise_scheduler\n",
    "    vae = model.vae\n",
    "    unet = model.unet\n",
    "    controlnet = model.controlnet\n",
    "    weight_dtype = torch.float32 \n",
    "    RAFTm = model.RAFT\n",
    "    transforms1 = model.transforms1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        captions = [\"\"]\n",
    "        text_inputs = tokenizer(captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "        batch_text_input_ids = text_inputs.to('cuda')\n",
    "\n",
    "        batch_lr_i = torch.unsqueeze(transforms1(lr_img), dim = 0).to('cuda')\n",
    "        batch_lr_iminus1 = torch.unsqueeze(transforms1(lr_iminus1_img), dim = 0).to('cuda')\n",
    "        batch_lr_iplus1 = torch.unsqueeze(transforms1(lr_iplus1_img), dim = 0).to('cuda')\n",
    "        batch_hr_i = torch.unsqueeze(transforms1(hr_img), dim = 0).to('cuda')\n",
    "\n",
    "        encoder_hidden_states = text_encoder(batch_text_input_ids, return_dict=False)[0]\n",
    "\n",
    "        list_of_flows_iminus1 = RAFTm(batch_lr_iminus1, batch_lr_i)[-1]\n",
    "        list_of_flows_iplus1 = RAFTm(batch_lr_iplus1, batch_lr_i)[-1]\n",
    "\n",
    "        LRiminus1_hat = torch.nn.functional.grid_sample(batch_lr_iminus1, list_of_flows_iminus1.permute(0, 2, 3, 1))\n",
    "        LRiplus1_hat = torch.nn.functional.grid_sample(batch_lr_iplus1, list_of_flows_iplus1.permute(0, 2, 3, 1))\n",
    "\n",
    "        latents = vae.encode(batch_hr_i).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor\n",
    "\n",
    "        noise_scheduler.set_timesteps(nsteps)\n",
    "\n",
    "        noise = torch.randn_like(latents)\n",
    "        inp = noise\n",
    "\n",
    "        controlnet_image = torch.cat([LRiminus1_hat, LRiplus1_hat], dim = 1)\n",
    "\n",
    "        for t in noise_scheduler.timesteps:\n",
    "            down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "                torch.cat([inp, batch_lr_i], dim = 1),\n",
    "                t,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                controlnet_cond=controlnet_image,\n",
    "                class_labels = torch.zeros(1).to(torch.int).to('cuda'),\n",
    "                return_dict=False,\n",
    "            )\n",
    "            \n",
    "            model_pred = unet(\n",
    "                torch.cat([inp, batch_lr_i], dim = 1),\n",
    "                t,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                down_block_additional_residuals=[sample.to(dtype=weight_dtype) for sample in down_block_res_samples],\n",
    "                mid_block_additional_residual=mid_block_res_sample.to(dtype=weight_dtype),\n",
    "                class_labels = torch.zeros(1).to(torch.int).to('cuda'),\n",
    "                return_dict=False,\n",
    "            )[0]\n",
    "            \n",
    "            inp = noise_scheduler.step(model_pred, t, inp, return_dict=False)[0]\n",
    "\n",
    "        inp = inp/vae.config.scaling_factor\n",
    "\n",
    "        image = vae.decode(inp, return_dict=False)[0]\n",
    "        image_out = F.to_pil_image((image/2+0.5).clamp(0,1).squeeze())\n",
    "\n",
    "    return image_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae142ed-4a4f-4108-8a41-64707bb486a0",
   "metadata": {},
   "source": [
    "## Test Images Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640ca5b-380d-4d85-8da8-b42ec80787bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_nums = [i for i in range(100)]\n",
    "root = 'frames'\n",
    "root_save = \"frames/cn-75\"\n",
    "nsteps = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b476090c-6fda-403e-aeb4-bcb75787db8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394804d-e01c-4312-8934-a5b6ecb66632",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_num = '000'\n",
    "for img in img_nums:\n",
    "    if img == 0:\n",
    "        left = img\n",
    "        centre = img\n",
    "        right = img + 1\n",
    "    elif img == 99:\n",
    "        left = img-1\n",
    "        centre = img\n",
    "        right = img\n",
    "    else:\n",
    "        left = img-1\n",
    "        centre = img\n",
    "        right = img+1\n",
    "    \n",
    "    hr_img = torchvision.io.read_image(os.path.join(root, 'hr', vid_num, '{:08d}.png'.format(centre)))\n",
    "    lr_img = torchvision.io.read_image(os.path.join(root, 'lr', vid_num, '{:08d}.png'.format(centre)))\n",
    "    lr_iminus1_img = torchvision.io.read_image(os.path.join(root, 'lr', vid_num, '{:08d}.png'.format(left)))\n",
    "    lr_iplus1_img = torchvision.io.read_image(os.path.join(root, 'lr', vid_num, '{:08d}.png'.format(right)))\n",
    "    \n",
    "    img_out = predict_on_frame(model, hr_img, lr_img, lr_iminus1_img, lr_iplus1_img, nsteps=nsteps)\n",
    "    \n",
    "    img_out.save(os.path.join(root_save, vid_num, '{:08d}.png'.format(img)), format=\"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d119f3e9-6dde-481f-8a8f-71ae9eba8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_num = '011'\n",
    "for img in img_nums:\n",
    "    if img == 0:\n",
    "        left = img\n",
    "        centre = img\n",
    "        right = img + 1\n",
    "    elif img == 99:\n",
    "        left = img-1\n",
    "        centre = img\n",
    "        right = img\n",
    "    else:\n",
    "        left = img-1\n",
    "        centre = img\n",
    "        right = img+1\n",
    "    \n",
    "    hr_img = torchvision.io.read_image(os.path.join(root, 'hr', vid_num, '{:08d}.png'.format(centre)))\n",
    "    lr_img = torchvision.io.read_image(os.path.join(root, 'lr', vid_num, '{:08d}.png'.format(centre)))\n",
    "    lr_iminus1_img = torchvision.io.read_image(os.path.join(root, 'lr', vid_num, '{:08d}.png'.format(left)))\n",
    "    lr_iplus1_img = torchvision.io.read_image(os.path.join(root, 'lr', vid_num, '{:08d}.png'.format(right)))\n",
    "    \n",
    "    img_out = predict_on_frame(model, hr_img, lr_img, lr_iminus1_img, lr_iplus1_img, nsteps=nsteps)\n",
    "    \n",
    "    img_out.save(os.path.join(root_save, vid_num, '{:08d}.png'.format(img)), format=\"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dde644-e7e4-4aa9-b863-136e4e00ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_num = '015'\n",
    "for img in img_nums:\n",
    "    if img == 0:\n",
    "        left = img\n",
    "        centre = img\n",
    "        right = img + 1\n",
    "    elif img == 99:\n",
    "        left = img-1\n",
    "        centre = img\n",
    "        right = img\n",
    "    else:\n",
    "        left = img-1\n",
    "        centre = img\n",
    "        right = img+1\n",
    "    \n",
    "    hr_img = torchvision.io.read_image(os.path.join(root, 'hr', vid_num, '{:08d}.png'.format(centre)))\n",
    "    lr_img = torchvision.io.read_image(os.path.join(root, 'lr', vid_num, '{:08d}.png'.format(centre)))\n",
    "    lr_iminus1_img = torchvision.io.read_image(os.path.join(root, 'lr', vid_num, '{:08d}.png'.format(left)))\n",
    "    lr_iplus1_img = torchvision.io.read_image(os.path.join(root, 'lr', vid_num, '{:08d}.png'.format(right)))\n",
    "    \n",
    "    img_out = predict_on_frame(model, hr_img, lr_img, lr_iminus1_img, lr_iplus1_img, nsteps=nsteps)\n",
    "    \n",
    "    img_out.save(os.path.join(root_save, vid_num, '{:08d}.png'.format(img)), format=\"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ac01b-045b-46a6-a1ee-2c798f1b451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_num = '020'\n",
    "for img in img_nums:\n",
    "    if img == 0:\n",
    "        left = img\n",
    "        centre = img\n",
    "        right = img + 1\n",
    "    elif img == 99:\n",
    "        left = img-1\n",
    "        centre = img\n",
    "        right = img\n",
    "    else:\n",
    "        left = img-1\n",
    "        centre = img\n",
    "        right = img+1\n",
    "    \n",
    "    hr_img = torchvision.io.read_image(os.path.join(root, 'hr', vid_num, '{:08d}.png'.format(centre)))\n",
    "    lr_img = torchvision.io.read_image(os.path.join(root, 'lr', vid_num, '{:08d}.png'.format(centre)))\n",
    "    lr_iminus1_img = torchvision.io.read_image(os.path.join(root, 'lr', vid_num, '{:08d}.png'.format(left)))\n",
    "    lr_iplus1_img = torchvision.io.read_image(os.path.join(root, 'lr', vid_num, '{:08d}.png'.format(right)))\n",
    "    \n",
    "    img_out = predict_on_frame(model, hr_img, lr_img, lr_iminus1_img, lr_iplus1_img, nsteps=nsteps)\n",
    "    \n",
    "    img_out.save(os.path.join(root_save, vid_num, '{:08d}.png'.format(img)), format=\"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895ee3bb-58f6-4b76-adfc-915eeb9b5304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa571c8-a36d-420b-a7d1-1032d3d61600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
