{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "472bd508-eefb-44cf-b137-edf503d5466a",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb0d977-e565-431b-98d4-19e7a2280bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import requests\n",
    "from transformers import AutoTokenizer, PretrainedConfig, CLIPTextModel, CLIPImageProcessor\n",
    "import diffusers\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    ControlNetModel,\n",
    "    DDPMScheduler,\n",
    "    StableDiffusionControlNetPipeline, \n",
    "    StableDiffusionControlNetImg2ImgPipeline,\n",
    "    UNet2DConditionModel,\n",
    "    UniPCMultistepScheduler,\n",
    ")\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torch, torchvision\n",
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import lightning as L\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "import torchvision.transforms as Tvt\n",
    "from torchvision.models.optical_flow import raft_small, raft_large\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54940875-fbb9-4f73-8314-0df92325358e",
   "metadata": {},
   "source": [
    "## Dataset & Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96897216-c4d7-48f6-8445-1eb2c86706ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_dataset(Dataset):\n",
    "    def __init__(self, train_dir, temporal_radius = 1):\n",
    "        self.train_dir = train_dir\n",
    "        self.temporal_radius = temporal_radius\n",
    "        self.video_names = os.listdir(os.path.join(train_dir, \"train_sharp\"))\n",
    "        self.eligible_frames = [i for i in range(self.temporal_radius, 100-self.temporal_radius)]\n",
    "        self.n_videos = len(self.video_names)\n",
    "        self.n_eligible_frames = len(self.eligible_frames)\n",
    "        self.n_total_eligible_images = self.n_videos * self.n_eligible_frames\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stable-diffusion-x4-upscaler\", subfolder=\"tokenizer\")\n",
    "        \n",
    "        self.lr_h_bound = 180 - 128\n",
    "        self.lr_w_bound = 320 - 128\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_total_eligible_images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid_name = '{:03d}'.format(idx//self.n_eligible_frames)\n",
    "        frame_name = '{:08d}.png'.format(self.temporal_radius + idx%self.n_eligible_frames)\n",
    "        lr_iminus1_frame_name = '{:08d}.png'.format(self.temporal_radius + (idx%self.n_eligible_frames)-1)\n",
    "        lr_iplus1_frame_name = '{:08d}.png'.format(self.temporal_radius + (idx%self.n_eligible_frames)+1)\n",
    "        \n",
    "        hr_frame = os.path.join(self.train_dir, \"train_sharp\", vid_name, frame_name)\n",
    "        lr_frame = os.path.join(self.train_dir, \"train_sharp_bicubic\", \"X4\", vid_name, frame_name)\n",
    "        lr_iminus1_frame = os.path.join(self.train_dir, \"train_sharp_bicubic\", \"X4\", vid_name, lr_iminus1_frame_name)\n",
    "        lr_iplus1_frame = os.path.join(self.train_dir, \"train_sharp_bicubic\", \"X4\", vid_name, lr_iplus1_frame_name)\n",
    "\n",
    "        hr_img = torchvision.io.read_image(hr_frame)\n",
    "        lr_img = torchvision.io.read_image(lr_frame)\n",
    "        lr_iminus1_img = torchvision.io.read_image(lr_iminus1_frame)\n",
    "        lr_iplus1_img = torchvision.io.read_image(lr_iplus1_frame)\n",
    "\n",
    "        ## Random Crop\n",
    "        x = random.randint(0, self.lr_h_bound)\n",
    "        y = random.randint(0, self.lr_w_bound)\n",
    "\n",
    "        hr_img = hr_img[:, x*4:(x*4)+512, y*4:y*4+512]\n",
    "        lr_img = lr_img[:, x:x+128, y:y+128]\n",
    "        lr_iminus1_img = lr_iminus1_img[:, x:x+128, y:y+128]\n",
    "        lr_iplus1_img = lr_iplus1_img[:, x:x+128, y:y+128]\n",
    "\n",
    "        captions = [\"\"]\n",
    "        text_inputs = self.tokenizer(captions, max_length=self.tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "        \n",
    "        return {\"hr_img\": hr_img, \"lr_img\": lr_img, \"lr_iminus1_img\": lr_iminus1_img, \"lr_iplus1_img\": lr_iplus1_img, \"text_encoder_inp_ids\": text_inputs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3a2456-57a1-4405-b01a-8a9573506af3",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cce10bd-a276-4e85-8914-140df8693c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlNetConditioningEmbeddingCustom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(6, 12, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(12, 24, kernel_size=3, padding=1)\n",
    "        self.conv3 = torch.nn.Conv2d(24, 96, kernel_size=3, padding=1)\n",
    "        self.conv4 = torch.nn.Conv2d(96, 256, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, conditioning):\n",
    "        embedding = self.conv1(conditioning)\n",
    "        embedding = torch.nn.functional.silu(embedding)\n",
    "        embedding = self.conv2(embedding)\n",
    "        embedding = torch.nn.functional.silu(embedding)\n",
    "        embedding = self.conv3(embedding)\n",
    "        embedding = torch.nn.functional.silu(embedding)\n",
    "        embedding = self.conv4(embedding)\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a59cc88-8315-4054-a14d-f2e2dc186712",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VSRDiffuser(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model_id = \"stabilityai/stable-diffusion-x4-upscaler\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, subfolder=\"tokenizer\")\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(self.model_id, subfolder=\"text_encoder\")\n",
    "        self.noise_scheduler = DDPMScheduler.from_pretrained(self.model_id, subfolder=\"scheduler\")\n",
    "        self.vae = AutoencoderKL.from_pretrained(self.model_id, subfolder=\"vae\")\n",
    "        self.unet = UNet2DConditionModel.from_pretrained(self.model_id, subfolder=\"unet\")\n",
    "        self.controlnet = ControlNetModel.from_unet(self.unet)\n",
    "        self.controlnet.controlnet_cond_embedding = ControlNetConditioningEmbeddingCustom()\n",
    "\n",
    "        self.weight_dtype = torch.float32 \n",
    "        \n",
    "        #RAFT Model for Optical Flow estimation model (RAFT SMALL/RAFT LARGE)\n",
    "        #self.RAFT = raft_small(pretrained=True, progress=False)\n",
    "        self.RAFT = raft_large(pretrained=True, progress=False)\n",
    "\n",
    "        self.transforms1 = Tvt.Compose(\n",
    "            [\n",
    "                Tvt.ConvertImageDtype(self.weight_dtype),\n",
    "                Tvt.Normalize(mean=0.5, std=0.5)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Steps\n",
    "        # 1. Optical Flow b/w i and i-1th frames and motion compensation\n",
    "        # 2. Optical Flow b/w i and i+1th frames and motion compensation\n",
    "        # 3. Depthwise Sepearable and Pointwise seperable covolutions - conv1 to conv4\n",
    "        # 4. HR image -> VAE Encoder -> HR latent\n",
    "        # 5. Sample noise and convert HR latents -> noised latents\n",
    "        # 6. Take the denoising step with Unet\n",
    "        # 7. Calculate the loss and do back propagation\n",
    "\n",
    "        batch_lr_i = self.transforms1(batch[\"lr_img\"])\n",
    "        batch_lr_iminus1 = self.transforms1(batch[\"lr_iminus1_img\"])\n",
    "        batch_lr_iplus1 = self.transforms1(batch[\"lr_iplus1_img\"])\n",
    "        batch_hr_i = self.transforms1(batch[\"hr_img\"])\n",
    "        batch_text_input_ids = batch[\"text_encoder_inp_ids\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            list_of_flows_iminus1 = self.RAFT(batch_lr_iminus1, batch_lr_i)[-1]\n",
    "            list_of_flows_iplus1 = self.RAFT(batch_lr_iplus1, batch_lr_i)[-1]\n",
    "        \n",
    "        LRiminus1_hat = torch.nn.functional.grid_sample(batch_lr_iminus1, list_of_flows_iminus1.permute(0, 2, 3, 1))\n",
    "        LRiplus1_hat = torch.nn.functional.grid_sample(batch_lr_iplus1, list_of_flows_iplus1.permute(0, 2, 3, 1))\n",
    "\n",
    "        # Calculate Latents of Ground Truth\n",
    "        latents = self.vae.encode(batch_hr_i).latent_dist.sample()\n",
    "        latents = latents * self.vae.config.scaling_factor\n",
    "\n",
    "        # Generate Noise\n",
    "        noise = torch.randn_like(latents)\n",
    "\n",
    "        bsz = latents.shape[0]\n",
    "\n",
    "        # Randomly Generate Timesteps\n",
    "        timesteps = torch.randint(0, self.noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "        timesteps = timesteps.long()\n",
    "\n",
    "        # Add noise to the GT Latents\n",
    "        noisy_latents = self.noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        encoder_hidden_states = self.text_encoder(batch_text_input_ids, return_dict=False)[0]\n",
    "\n",
    "        controlnet_image = torch.cat([LRiminus1_hat, LRiplus1_hat], dim = 1)\n",
    "\n",
    "        down_block_res_samples, mid_block_res_sample = self.controlnet(\n",
    "            torch.cat([noisy_latents, batch_lr_i], dim = 1),\n",
    "            timesteps,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            controlnet_cond=controlnet_image,\n",
    "            class_labels = torch.zeros(1).to(torch.int).to('cuda'),\n",
    "            return_dict=False,\n",
    "        )\n",
    "\n",
    "        model_pred = self.unet(\n",
    "            torch.cat([noisy_latents, batch_lr_i], dim = 1),\n",
    "            timesteps,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            down_block_additional_residuals=[sample.to(dtype=self.weight_dtype) for sample in down_block_res_samples],\n",
    "            mid_block_additional_residual=mid_block_res_sample.to(dtype=self.weight_dtype),\n",
    "            class_labels = torch.zeros(1).to(torch.int).to('cuda'),\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "\n",
    "        target = self.noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "        \n",
    "        print(\"batch: {} => Loss: {}\".format(batch_idx, loss))\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "        optimizer = optimizer_class(\n",
    "            self.controlnet.parameters(),\n",
    "            lr=1e-5,\n",
    "            betas=(0.9, 0.999),\n",
    "            weight_decay=1e-2,\n",
    "            eps=1e-08,\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a8ba1-d8d5-453c-b7ed-cd90f5d26f7d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70e3f4e9-27f4-47d5-8476-4b70452d9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_dataset(train_dir = \"data/train\", temporal_radius = 1)\n",
    "batch_size = 40\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96247387-6914-4db8-975d-2e172ab06845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "874bb225-aa50-4768-9a57-ad0a6be30483",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VSRDiffuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b920a0b-e206-4c7e-9ae2-144f5494654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.text_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in model.vae.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in model.unet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.RAFT.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.controlnet.enable_gradient_checkpointing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b416b-bb63-47c8-b492-90b62384d8db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f97a5197-6e42-4fac-8bf9-51d9e4647b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnd2794\u001b[0m (\u001b[33muniir\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nd2794/vsr/wandb/run-20240428_044328-1aay3071</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uniir/VSR_Control_Net/runs/1aay3071' target=\"_blank\">apricot-butterfly-7</a></strong> to <a href='https://wandb.ai/uniir/VSR_Control_Net' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uniir/VSR_Control_Net' target=\"_blank\">https://wandb.ai/uniir/VSR_Control_Net</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uniir/VSR_Control_Net/runs/1aay3071' target=\"_blank\">https://wandb.ai/uniir/VSR_Control_Net/runs/1aay3071</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"VSR_Control_Net\")\n",
    "wandb_logger = WandbLogger(project='VSR_Control_Net', log_model=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebbc9fcd-05cb-4c78-a15d-54332e7d529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(dirpath='checkpoints/control_net_v1', every_n_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d508b04-09b5-4871-90da-6a0ba39e403f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\", \n",
    "    devices=2, \n",
    "    strategy=\"ddp_find_unused_parameters_true\", \n",
    "    max_epochs=20, \n",
    "    logger=wandb_logger, \n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed02a2-0660-4ff9-8dad-2e62d7a8f8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type                 | Params\n",
      "------------------------------------------------------\n",
      "0 | text_encoder | CLIPTextModel        | 340 M \n",
      "1 | vae          | AutoencoderKL        | 55.3 M\n",
      "2 | unet         | UNet2DConditionModel | 473 M \n",
      "3 | controlnet   | ControlNetModel      | 207 M \n",
      "4 | RAFT         | RAFT                 | 5.3 M \n",
      "------------------------------------------------------\n",
      "207 M     Trainable params\n",
      "874 M     Non-trainable params\n",
      "1.1 B     Total params\n",
      "4,328.771 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                                                             | 0/5214 [00:00<?, ?it/s]batch: 0 => Loss: 0.18053539097309113\n",
      "Epoch 0:   0%|                                                                                                | 1/5214 [00:03<4:59:11,  0.29it/s, v_num=3071, train_loss_step=0.181]batch: 1 => Loss: 0.14029157161712646\n",
      "Epoch 0:   0%|                                                                                                | 2/5214 [00:06<4:29:47,  0.32it/s, v_num=3071, train_loss_step=0.140]batch: 2 => Loss: 0.25354087352752686\n",
      "Epoch 0:   0%|                                                                                                | 3/5214 [00:08<4:19:20,  0.33it/s, v_num=3071, train_loss_step=0.254]batch: 3 => Loss: 0.14851276576519012\n",
      "Epoch 0:   0%|                                                                                                | 4/5214 [00:11<4:14:11,  0.34it/s, v_num=3071, train_loss_step=0.149]batch: 4 => Loss: 0.06477628648281097\n",
      "Epoch 0:   0%|                                                                                               | 5/5214 [00:14<4:10:45,  0.35it/s, v_num=3071, train_loss_step=0.0648]batch: 5 => Loss: 0.1321592777967453\n",
      "Epoch 0:   0%|                                                                                                | 6/5214 [00:17<4:08:43,  0.35it/s, v_num=3071, train_loss_step=0.132]batch: 6 => Loss: 0.3625689446926117\n",
      "Epoch 0:   0%|▏                                                                                               | 7/5214 [00:19<4:07:18,  0.35it/s, v_num=3071, train_loss_step=0.363]batch: 7 => Loss: 0.10391630232334137\n",
      "Epoch 0:   0%|▏                                                                                               | 8/5214 [00:22<4:06:09,  0.35it/s, v_num=3071, train_loss_step=0.104]batch: 8 => Loss: 0.18041172623634338\n",
      "Epoch 0:   0%|▏                                                                                               | 9/5214 [00:25<4:05:22,  0.35it/s, v_num=3071, train_loss_step=0.180]batch: 9 => Loss: 0.16975107789039612\n",
      "Epoch 0:   0%|▏                                                                                              | 10/5214 [00:28<4:04:37,  0.35it/s, v_num=3071, train_loss_step=0.170]batch: 10 => Loss: 0.21381624042987823\n",
      "Epoch 0:   0%|▏                                                                                              | 11/5214 [00:30<4:04:06,  0.36it/s, v_num=3071, train_loss_step=0.214]batch: 11 => Loss: 0.12058105319738388\n",
      "Epoch 0:   0%|▏                                                                                              | 12/5214 [00:33<4:03:34,  0.36it/s, v_num=3071, train_loss_step=0.121]batch: 12 => Loss: 0.18573801219463348\n",
      "Epoch 0:   0%|▏                                                                                              | 13/5214 [00:36<4:03:18,  0.36it/s, v_num=3071, train_loss_step=0.186]batch: 13 => Loss: 0.1706656813621521\n",
      "Epoch 0:   0%|▎                                                                                              | 14/5214 [00:39<4:02:58,  0.36it/s, v_num=3071, train_loss_step=0.171]batch: 14 => Loss: 0.19804827868938446\n",
      "Epoch 0:   0%|▎                                                                                              | 15/5214 [00:42<4:02:40,  0.36it/s, v_num=3071, train_loss_step=0.198]batch: 15 => Loss: 0.2264765053987503\n",
      "Epoch 0:   0%|▎                                                                                              | 16/5214 [00:44<4:02:23,  0.36it/s, v_num=3071, train_loss_step=0.226]batch: 16 => Loss: 0.1924799233675003\n",
      "Epoch 0:   0%|▎                                                                                              | 17/5214 [00:47<4:02:12,  0.36it/s, v_num=3071, train_loss_step=0.192]batch: 17 => Loss: 0.14965055882930756\n",
      "Epoch 0:   0%|▎                                                                                              | 18/5214 [00:50<4:02:03,  0.36it/s, v_num=3071, train_loss_step=0.150]batch: 18 => Loss: 0.16172242164611816\n",
      "Epoch 0:   0%|▎                                                                                              | 19/5214 [00:53<4:01:49,  0.36it/s, v_num=3071, train_loss_step=0.162]batch: 19 => Loss: 0.30289149284362793\n",
      "Epoch 0:   0%|▎                                                                                              | 20/5214 [00:55<4:01:38,  0.36it/s, v_num=3071, train_loss_step=0.303]batch: 20 => Loss: 0.1935465931892395\n",
      "Epoch 0:   0%|▍                                                                                              | 21/5214 [00:58<4:01:32,  0.36it/s, v_num=3071, train_loss_step=0.194]batch: 21 => Loss: 0.27835607528686523\n",
      "Epoch 0:   0%|▍                                                                                              | 22/5214 [01:01<4:01:32,  0.36it/s, v_num=3071, train_loss_step=0.278]batch: 22 => Loss: 0.20420949161052704\n",
      "Epoch 0:   0%|▍                                                                                              | 23/5214 [01:04<4:01:31,  0.36it/s, v_num=3071, train_loss_step=0.204]batch: 23 => Loss: 0.17458844184875488\n",
      "Epoch 0:   0%|▍                                                                                              | 24/5214 [01:07<4:01:31,  0.36it/s, v_num=3071, train_loss_step=0.175]batch: 24 => Loss: 0.2056010514497757\n",
      "Epoch 0:   0%|▍                                                                                              | 25/5214 [01:09<4:01:24,  0.36it/s, v_num=3071, train_loss_step=0.206]batch: 25 => Loss: 0.1692352145910263\n",
      "Epoch 0:   0%|▍                                                                                              | 26/5214 [01:12<4:01:29,  0.36it/s, v_num=3071, train_loss_step=0.169]batch: 26 => Loss: 0.17834815382957458\n",
      "Epoch 0:   1%|▍                                                                                              | 27/5214 [01:15<4:01:25,  0.36it/s, v_num=3071, train_loss_step=0.178]batch: 27 => Loss: 0.16158756613731384\n",
      "Epoch 0:   1%|▌                                                                                              | 28/5214 [01:18<4:01:23,  0.36it/s, v_num=3071, train_loss_step=0.162]batch: 28 => Loss: 0.17556415498256683\n",
      "Epoch 0:   1%|▌                                                                                              | 29/5214 [01:21<4:01:23,  0.36it/s, v_num=3071, train_loss_step=0.176]batch: 29 => Loss: 0.2323637306690216\n",
      "Epoch 0:   1%|▌                                                                                              | 30/5214 [01:23<4:01:21,  0.36it/s, v_num=3071, train_loss_step=0.232]batch: 30 => Loss: 0.12741611897945404\n",
      "Epoch 0:   1%|▌                                                                                              | 31/5214 [01:26<4:01:19,  0.36it/s, v_num=3071, train_loss_step=0.127]batch: 31 => Loss: 0.11168456077575684\n",
      "Epoch 0:   1%|▌                                                                                              | 32/5214 [01:29<4:01:18,  0.36it/s, v_num=3071, train_loss_step=0.112]batch: 32 => Loss: 0.19746826589107513\n",
      "Epoch 0:   1%|▌                                                                                              | 33/5214 [01:32<4:01:16,  0.36it/s, v_num=3071, train_loss_step=0.197]batch: 33 => Loss: 0.19758759438991547\n",
      "Epoch 0:   1%|▌                                                                                              | 34/5214 [01:35<4:01:15,  0.36it/s, v_num=3071, train_loss_step=0.198]batch: 34 => Loss: 0.22470489144325256\n",
      "Epoch 0:   1%|▋                                                                                              | 35/5214 [01:37<4:01:14,  0.36it/s, v_num=3071, train_loss_step=0.225]batch: 35 => Loss: 0.22988979518413544\n",
      "Epoch 0:   1%|▋                                                                                              | 36/5214 [01:40<4:01:12,  0.36it/s, v_num=3071, train_loss_step=0.230]batch: 36 => Loss: 0.13711880147457123\n",
      "Epoch 0:   1%|▋                                                                                              | 37/5214 [01:43<4:01:12,  0.36it/s, v_num=3071, train_loss_step=0.137]batch: 37 => Loss: 0.21519818902015686\n",
      "Epoch 0:   1%|▋                                                                                              | 38/5214 [01:46<4:01:11,  0.36it/s, v_num=3071, train_loss_step=0.215]batch: 38 => Loss: 0.3298501670360565\n",
      "Epoch 0:   1%|▋                                                                                              | 39/5214 [01:49<4:01:11,  0.36it/s, v_num=3071, train_loss_step=0.330]batch: 39 => Loss: 0.09408967196941376\n",
      "Epoch 0:   1%|▋                                                                                             | 40/5214 [01:51<4:01:09,  0.36it/s, v_num=3071, train_loss_step=0.0941]batch: 40 => Loss: 0.1699173003435135\n",
      "Epoch 0:   1%|▋                                                                                              | 41/5214 [01:54<4:01:06,  0.36it/s, v_num=3071, train_loss_step=0.170]batch: 41 => Loss: 0.1794440746307373\n",
      "Epoch 0:   1%|▊                                                                                              | 42/5214 [01:57<4:01:05,  0.36it/s, v_num=3071, train_loss_step=0.179]batch: 42 => Loss: 0.1548461765050888\n",
      "Epoch 0:   1%|▊                                                                                              | 43/5214 [02:00<4:01:06,  0.36it/s, v_num=3071, train_loss_step=0.155]batch: 43 => Loss: 0.20045237243175507\n",
      "Epoch 0:   1%|▊                                                                                              | 44/5214 [02:03<4:01:06,  0.36it/s, v_num=3071, train_loss_step=0.200]batch: 44 => Loss: 0.3258328139781952\n",
      "Epoch 0:   1%|▊                                                                                              | 45/5214 [02:05<4:01:05,  0.36it/s, v_num=3071, train_loss_step=0.326]batch: 45 => Loss: 0.11070866882801056\n",
      "Epoch 0:   1%|▊                                                                                              | 46/5214 [02:08<4:01:04,  0.36it/s, v_num=3071, train_loss_step=0.111]batch: 46 => Loss: 0.20184683799743652\n",
      "Epoch 0:   1%|▊                                                                                              | 47/5214 [02:11<4:01:02,  0.36it/s, v_num=3071, train_loss_step=0.202]batch: 47 => Loss: 0.1500498503446579\n",
      "Epoch 0:   1%|▊                                                                                              | 48/5214 [02:14<4:01:03,  0.36it/s, v_num=3071, train_loss_step=0.150]batch: 48 => Loss: 0.20442748069763184\n",
      "Epoch 0:   1%|▉                                                                                              | 49/5214 [02:17<4:01:05,  0.36it/s, v_num=3071, train_loss_step=0.204]batch: 49 => Loss: 0.2154041826725006\n",
      "Epoch 0:   1%|▉                                                                                              | 50/5214 [02:20<4:01:06,  0.36it/s, v_num=3071, train_loss_step=0.215]batch: 50 => Loss: 0.12231335788965225\n",
      "Epoch 0:   1%|▉                                                                                              | 51/5214 [02:22<4:01:07,  0.36it/s, v_num=3071, train_loss_step=0.122]batch: 51 => Loss: 0.09512683004140854\n",
      "Epoch 0:   1%|▉                                                                                             | 52/5214 [02:25<4:01:06,  0.36it/s, v_num=3071, train_loss_step=0.0951]batch: 52 => Loss: 0.1483810395002365\n",
      "Epoch 0:   1%|▉                                                                                              | 53/5214 [02:28<4:01:07,  0.36it/s, v_num=3071, train_loss_step=0.148]batch: 53 => Loss: 0.18107232451438904\n",
      "Epoch 0:   1%|▉                                                                                              | 54/5214 [02:31<4:01:06,  0.36it/s, v_num=3071, train_loss_step=0.181]batch: 54 => Loss: 0.2361842691898346\n",
      "Epoch 0:   1%|█                                                                                              | 55/5214 [02:34<4:01:08,  0.36it/s, v_num=3071, train_loss_step=0.236]batch: 55 => Loss: 0.16037988662719727\n",
      "Epoch 0:   1%|█                                                                                              | 56/5214 [02:37<4:01:10,  0.36it/s, v_num=3071, train_loss_step=0.160]batch: 56 => Loss: 0.15456603467464447\n",
      "Epoch 0:   1%|█                                                                                              | 57/5214 [02:39<4:01:10,  0.36it/s, v_num=3071, train_loss_step=0.155]batch: 57 => Loss: 0.22853942215442657\n",
      "Epoch 0:   1%|█                                                                                              | 58/5214 [02:42<4:01:12,  0.36it/s, v_num=3071, train_loss_step=0.229]batch: 58 => Loss: 0.12662313878536224\n",
      "Epoch 0:   1%|█                                                                                              | 59/5214 [02:45<4:01:12,  0.36it/s, v_num=3071, train_loss_step=0.127]batch: 59 => Loss: 0.18991433084011078\n",
      "Epoch 0:   1%|█                                                                                              | 60/5214 [02:48<4:01:15,  0.36it/s, v_num=3071, train_loss_step=0.190]batch: 60 => Loss: 0.18700014054775238\n",
      "Epoch 0:   1%|█                                                                                              | 61/5214 [02:51<4:01:14,  0.36it/s, v_num=3071, train_loss_step=0.187]batch: 61 => Loss: 0.14747558534145355\n",
      "Epoch 0:   1%|█▏                                                                                             | 62/5214 [02:54<4:01:15,  0.36it/s, v_num=3071, train_loss_step=0.147]batch: 62 => Loss: 0.15077315270900726\n",
      "Epoch 0:   1%|█▏                                                                                             | 63/5214 [02:57<4:01:15,  0.36it/s, v_num=3071, train_loss_step=0.151]batch: 63 => Loss: 0.12144728004932404\n",
      "Epoch 0:   1%|█▏                                                                                             | 64/5214 [02:59<4:01:14,  0.36it/s, v_num=3071, train_loss_step=0.121]batch: 64 => Loss: 0.15108712017536163\n",
      "Epoch 0:   1%|█▏                                                                                             | 65/5214 [03:02<4:01:14,  0.36it/s, v_num=3071, train_loss_step=0.151]batch: 65 => Loss: 0.21148991584777832\n",
      "Epoch 0:   1%|█▏                                                                                             | 66/5214 [03:05<4:01:13,  0.36it/s, v_num=3071, train_loss_step=0.211]batch: 66 => Loss: 0.1432422399520874\n",
      "Epoch 0:   1%|█▏                                                                                             | 67/5214 [03:08<4:01:12,  0.36it/s, v_num=3071, train_loss_step=0.143]batch: 67 => Loss: 0.2876144349575043\n",
      "Epoch 0:   1%|█▏                                                                                             | 68/5214 [03:11<4:01:12,  0.36it/s, v_num=3071, train_loss_step=0.288]batch: 68 => Loss: 0.14100274443626404\n",
      "Epoch 0:   1%|█▎                                                                                             | 69/5214 [03:14<4:01:11,  0.36it/s, v_num=3071, train_loss_step=0.141]batch: 69 => Loss: 0.20478759706020355\n",
      "Epoch 0:   1%|█▎                                                                                             | 70/5214 [03:16<4:01:11,  0.36it/s, v_num=3071, train_loss_step=0.205]batch: 70 => Loss: 0.1374918669462204\n",
      "Epoch 0:   1%|█▎                                                                                             | 71/5214 [03:19<4:01:12,  0.36it/s, v_num=3071, train_loss_step=0.137]batch: 71 => Loss: 0.1347571462392807\n",
      "Epoch 0:   1%|█▎                                                                                             | 72/5214 [03:22<4:01:12,  0.36it/s, v_num=3071, train_loss_step=0.135]batch: 72 => Loss: 0.17791755497455597\n",
      "Epoch 0:   1%|█▎                                                                                             | 73/5214 [03:25<4:01:11,  0.36it/s, v_num=3071, train_loss_step=0.178]batch: 73 => Loss: 0.1892285943031311\n",
      "Epoch 0:   1%|█▎                                                                                             | 74/5214 [03:28<4:01:12,  0.36it/s, v_num=3071, train_loss_step=0.189]batch: 74 => Loss: 0.1950334757566452\n",
      "Epoch 0:   1%|█▎                                                                                             | 75/5214 [03:31<4:01:13,  0.36it/s, v_num=3071, train_loss_step=0.195]batch: 75 => Loss: 0.13130955398082733\n",
      "Epoch 0:   1%|█▍                                                                                             | 76/5214 [03:34<4:01:12,  0.36it/s, v_num=3071, train_loss_step=0.131]batch: 76 => Loss: 0.1950984001159668\n",
      "Epoch 0:   1%|█▍                                                                                             | 77/5214 [03:36<4:01:12,  0.35it/s, v_num=3071, train_loss_step=0.195]batch: 77 => Loss: 0.2153206169605255\n",
      "Epoch 0:   1%|█▍                                                                                             | 78/5214 [03:39<4:01:11,  0.35it/s, v_num=3071, train_loss_step=0.215]batch: 78 => Loss: 0.16804122924804688\n",
      "Epoch 0:   2%|█▍                                                                                             | 79/5214 [03:42<4:01:12,  0.35it/s, v_num=3071, train_loss_step=0.168]batch: 79 => Loss: 0.12390764057636261\n",
      "Epoch 0:   2%|█▍                                                                                             | 80/5214 [03:45<4:01:13,  0.35it/s, v_num=3071, train_loss_step=0.124]batch: 80 => Loss: 0.22978834807872772\n",
      "Epoch 0:   2%|█▍                                                                                             | 81/5214 [03:48<4:01:12,  0.35it/s, v_num=3071, train_loss_step=0.230]batch: 81 => Loss: 0.17916077375411987\n",
      "Epoch 0:   2%|█▍                                                                                             | 82/5214 [03:51<4:01:12,  0.35it/s, v_num=3071, train_loss_step=0.179]batch: 82 => Loss: 0.1253044158220291\n",
      "Epoch 0:   2%|█▌                                                                                             | 83/5214 [03:54<4:01:12,  0.35it/s, v_num=3071, train_loss_step=0.125]batch: 83 => Loss: 0.1725100725889206\n",
      "Epoch 0:   2%|█▌                                                                                             | 84/5214 [03:56<4:01:12,  0.35it/s, v_num=3071, train_loss_step=0.173]batch: 84 => Loss: 0.17891857028007507\n",
      "Epoch 0:   2%|█▌                                                                                             | 85/5214 [03:59<4:01:12,  0.35it/s, v_num=3071, train_loss_step=0.179]batch: 85 => Loss: 0.1916491985321045\n",
      "Epoch 0:   2%|█▌                                                                                             | 86/5214 [04:02<4:01:10,  0.35it/s, v_num=3071, train_loss_step=0.192]batch: 86 => Loss: 0.14095330238342285\n",
      "Epoch 0:   2%|█▌                                                                                             | 87/5214 [04:05<4:01:09,  0.35it/s, v_num=3071, train_loss_step=0.141]batch: 87 => Loss: 0.1810874044895172\n",
      "Epoch 0:   2%|█▌                                                                                             | 88/5214 [04:08<4:01:09,  0.35it/s, v_num=3071, train_loss_step=0.181]batch: 88 => Loss: 0.10316373407840729\n",
      "Epoch 0:   2%|█▌                                                                                             | 89/5214 [04:11<4:01:08,  0.35it/s, v_num=3071, train_loss_step=0.103]batch: 89 => Loss: 0.182963028550148\n",
      "Epoch 0:   2%|█▋                                                                                             | 90/5214 [04:14<4:01:07,  0.35it/s, v_num=3071, train_loss_step=0.183]batch: 90 => Loss: 0.1177661195397377\n",
      "Epoch 0:   2%|█▋                                                                                             | 91/5214 [04:16<4:01:06,  0.35it/s, v_num=3071, train_loss_step=0.118]batch: 91 => Loss: 0.12341167777776718\n",
      "Epoch 0:   2%|█▋                                                                                             | 92/5214 [04:19<4:01:06,  0.35it/s, v_num=3071, train_loss_step=0.123]batch: 92 => Loss: 0.18541792035102844\n",
      "Epoch 0:   2%|█▋                                                                                             | 93/5214 [04:22<4:01:06,  0.35it/s, v_num=3071, train_loss_step=0.185]batch: 93 => Loss: 0.1424269825220108\n",
      "Epoch 0:   2%|█▋                                                                                             | 94/5214 [04:25<4:01:04,  0.35it/s, v_num=3071, train_loss_step=0.142]batch: 94 => Loss: 0.13135544955730438\n",
      "Epoch 0:   2%|█▋                                                                                             | 95/5214 [04:28<4:01:04,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 95 => Loss: 0.20241498947143555\n",
      "Epoch 0:   2%|█▋                                                                                             | 96/5214 [04:31<4:01:02,  0.35it/s, v_num=3071, train_loss_step=0.202]batch: 96 => Loss: 0.15615704655647278\n",
      "Epoch 0:   2%|█▊                                                                                             | 97/5214 [04:34<4:01:01,  0.35it/s, v_num=3071, train_loss_step=0.156]batch: 97 => Loss: 0.15461614727973938\n",
      "Epoch 0:   2%|█▊                                                                                             | 98/5214 [04:37<4:01:01,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 98 => Loss: 0.09565974771976471\n",
      "Epoch 0:   2%|█▊                                                                                            | 99/5214 [04:39<4:01:01,  0.35it/s, v_num=3071, train_loss_step=0.0957]batch: 99 => Loss: 0.10213358700275421\n",
      "Epoch 0:   2%|█▊                                                                                            | 100/5214 [04:42<4:00:58,  0.35it/s, v_num=3071, train_loss_step=0.102]batch: 100 => Loss: 0.13497690856456757\n",
      "Epoch 0:   2%|█▊                                                                                            | 101/5214 [04:45<4:00:56,  0.35it/s, v_num=3071, train_loss_step=0.135]batch: 101 => Loss: 0.23464961349964142\n",
      "Epoch 0:   2%|█▊                                                                                            | 102/5214 [04:48<4:00:55,  0.35it/s, v_num=3071, train_loss_step=0.235]batch: 102 => Loss: 0.21558402478694916\n",
      "Epoch 0:   2%|█▊                                                                                            | 103/5214 [04:51<4:00:52,  0.35it/s, v_num=3071, train_loss_step=0.216]batch: 103 => Loss: 0.15002362430095673\n",
      "Epoch 0:   2%|█▊                                                                                            | 104/5214 [04:54<4:00:51,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 104 => Loss: 0.21444907784461975\n",
      "Epoch 0:   2%|█▉                                                                                            | 105/5214 [04:56<4:00:50,  0.35it/s, v_num=3071, train_loss_step=0.214]batch: 105 => Loss: 0.12739960849285126\n",
      "Epoch 0:   2%|█▉                                                                                            | 106/5214 [04:59<4:00:49,  0.35it/s, v_num=3071, train_loss_step=0.127]batch: 106 => Loss: 0.18238885700702667\n",
      "Epoch 0:   2%|█▉                                                                                            | 107/5214 [05:02<4:00:48,  0.35it/s, v_num=3071, train_loss_step=0.182]batch: 107 => Loss: 0.1245919018983841\n",
      "Epoch 0:   2%|█▉                                                                                            | 108/5214 [05:05<4:00:47,  0.35it/s, v_num=3071, train_loss_step=0.125]batch: 108 => Loss: 0.16358917951583862\n",
      "Epoch 0:   2%|█▉                                                                                            | 109/5214 [05:08<4:00:45,  0.35it/s, v_num=3071, train_loss_step=0.164]batch: 109 => Loss: 0.200575590133667\n",
      "Epoch 0:   2%|█▉                                                                                            | 110/5214 [05:11<4:00:44,  0.35it/s, v_num=3071, train_loss_step=0.201]batch: 110 => Loss: 0.20180173218250275\n",
      "Epoch 0:   2%|██                                                                                            | 111/5214 [05:14<4:00:43,  0.35it/s, v_num=3071, train_loss_step=0.202]batch: 111 => Loss: 0.1545076221227646\n",
      "Epoch 0:   2%|██                                                                                            | 112/5214 [05:17<4:00:42,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 112 => Loss: 0.1686982363462448\n",
      "Epoch 0:   2%|██                                                                                            | 113/5214 [05:19<4:00:40,  0.35it/s, v_num=3071, train_loss_step=0.169]batch: 113 => Loss: 0.09755324572324753\n",
      "Epoch 0:   2%|██                                                                                           | 114/5214 [05:22<4:00:40,  0.35it/s, v_num=3071, train_loss_step=0.0976]batch: 114 => Loss: 0.16565629839897156\n",
      "Epoch 0:   2%|██                                                                                            | 115/5214 [05:25<4:00:39,  0.35it/s, v_num=3071, train_loss_step=0.166]batch: 115 => Loss: 0.17816345393657684\n",
      "Epoch 0:   2%|██                                                                                            | 116/5214 [05:28<4:00:37,  0.35it/s, v_num=3071, train_loss_step=0.178]batch: 116 => Loss: 0.18284325301647186\n",
      "Epoch 0:   2%|██                                                                                            | 117/5214 [05:31<4:00:35,  0.35it/s, v_num=3071, train_loss_step=0.183]batch: 117 => Loss: 0.15438640117645264\n",
      "Epoch 0:   2%|██▏                                                                                           | 118/5214 [05:34<4:00:34,  0.35it/s, v_num=3071, train_loss_step=0.154]batch: 118 => Loss: 0.22556057572364807\n",
      "Epoch 0:   2%|██▏                                                                                           | 119/5214 [05:37<4:00:32,  0.35it/s, v_num=3071, train_loss_step=0.226]batch: 119 => Loss: 0.20701280236244202\n",
      "Epoch 0:   2%|██▏                                                                                           | 120/5214 [05:39<4:00:30,  0.35it/s, v_num=3071, train_loss_step=0.207]batch: 120 => Loss: 0.16859972476959229\n",
      "Epoch 0:   2%|██▏                                                                                           | 121/5214 [05:42<4:00:28,  0.35it/s, v_num=3071, train_loss_step=0.169]batch: 121 => Loss: 0.17967581748962402\n",
      "Epoch 0:   2%|██▏                                                                                           | 122/5214 [05:45<4:00:25,  0.35it/s, v_num=3071, train_loss_step=0.180]batch: 122 => Loss: 0.16350215673446655\n",
      "Epoch 0:   2%|██▏                                                                                           | 123/5214 [05:48<4:00:24,  0.35it/s, v_num=3071, train_loss_step=0.164]batch: 123 => Loss: 0.15364030003547668\n",
      "Epoch 0:   2%|██▏                                                                                           | 124/5214 [05:51<4:00:22,  0.35it/s, v_num=3071, train_loss_step=0.154]batch: 124 => Loss: 0.11574351787567139\n",
      "Epoch 0:   2%|██▎                                                                                           | 125/5214 [05:54<4:00:20,  0.35it/s, v_num=3071, train_loss_step=0.116]batch: 125 => Loss: 0.13908663392066956\n",
      "Epoch 0:   2%|██▎                                                                                           | 126/5214 [05:57<4:00:18,  0.35it/s, v_num=3071, train_loss_step=0.139]batch: 126 => Loss: 0.11693360656499863\n",
      "Epoch 0:   2%|██▎                                                                                           | 127/5214 [05:59<4:00:16,  0.35it/s, v_num=3071, train_loss_step=0.117]batch: 127 => Loss: 0.12430691719055176\n",
      "Epoch 0:   2%|██▎                                                                                           | 128/5214 [06:02<4:00:15,  0.35it/s, v_num=3071, train_loss_step=0.124]batch: 128 => Loss: 0.15193937718868256\n",
      "Epoch 0:   2%|██▎                                                                                           | 129/5214 [06:05<4:00:13,  0.35it/s, v_num=3071, train_loss_step=0.152]batch: 129 => Loss: 0.19767098128795624\n",
      "Epoch 0:   2%|██▎                                                                                           | 130/5214 [06:08<4:00:11,  0.35it/s, v_num=3071, train_loss_step=0.198]batch: 130 => Loss: 0.20152516663074493\n",
      "Epoch 0:   3%|██▎                                                                                           | 131/5214 [06:11<4:00:10,  0.35it/s, v_num=3071, train_loss_step=0.202]batch: 131 => Loss: 0.13688267767429352\n",
      "Epoch 0:   3%|██▍                                                                                           | 132/5214 [06:14<4:00:07,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 132 => Loss: 0.12397456169128418\n",
      "Epoch 0:   3%|██▍                                                                                           | 133/5214 [06:17<4:00:06,  0.35it/s, v_num=3071, train_loss_step=0.124]batch: 133 => Loss: 0.21901412308216095\n",
      "Epoch 0:   3%|██▍                                                                                           | 134/5214 [06:19<4:00:05,  0.35it/s, v_num=3071, train_loss_step=0.219]batch: 134 => Loss: 0.21721415221691132\n",
      "Epoch 0:   3%|██▍                                                                                           | 135/5214 [06:22<4:00:03,  0.35it/s, v_num=3071, train_loss_step=0.217]batch: 135 => Loss: 0.16973893344402313\n",
      "Epoch 0:   3%|██▍                                                                                           | 136/5214 [06:25<4:00:01,  0.35it/s, v_num=3071, train_loss_step=0.170]batch: 136 => Loss: 0.19792932271957397\n",
      "Epoch 0:   3%|██▍                                                                                           | 137/5214 [06:28<3:59:59,  0.35it/s, v_num=3071, train_loss_step=0.198]batch: 137 => Loss: 0.24885521829128265\n",
      "Epoch 0:   3%|██▍                                                                                           | 138/5214 [06:31<3:59:57,  0.35it/s, v_num=3071, train_loss_step=0.249]batch: 138 => Loss: 0.2711469829082489\n",
      "Epoch 0:   3%|██▌                                                                                           | 139/5214 [06:34<3:59:55,  0.35it/s, v_num=3071, train_loss_step=0.271]batch: 139 => Loss: 0.17126546800136566\n",
      "Epoch 0:   3%|██▌                                                                                           | 140/5214 [06:37<3:59:53,  0.35it/s, v_num=3071, train_loss_step=0.171]batch: 140 => Loss: 0.1097169890999794\n",
      "Epoch 0:   3%|██▌                                                                                           | 141/5214 [06:39<3:59:51,  0.35it/s, v_num=3071, train_loss_step=0.110]batch: 141 => Loss: 0.10473088175058365\n",
      "Epoch 0:   3%|██▌                                                                                           | 142/5214 [06:42<3:59:49,  0.35it/s, v_num=3071, train_loss_step=0.105]batch: 142 => Loss: 0.22565464675426483\n",
      "Epoch 0:   3%|██▌                                                                                           | 143/5214 [06:45<3:59:47,  0.35it/s, v_num=3071, train_loss_step=0.226]batch: 143 => Loss: 0.13771039247512817\n",
      "Epoch 0:   3%|██▌                                                                                           | 144/5214 [06:48<3:59:45,  0.35it/s, v_num=3071, train_loss_step=0.138]batch: 144 => Loss: 0.20636224746704102\n",
      "Epoch 0:   3%|██▌                                                                                           | 145/5214 [06:51<3:59:43,  0.35it/s, v_num=3071, train_loss_step=0.206]batch: 145 => Loss: 0.14871080219745636\n",
      "Epoch 0:   3%|██▋                                                                                           | 146/5214 [06:54<3:59:41,  0.35it/s, v_num=3071, train_loss_step=0.149]batch: 146 => Loss: 0.19801826775074005\n",
      "Epoch 0:   3%|██▋                                                                                           | 147/5214 [06:57<3:59:39,  0.35it/s, v_num=3071, train_loss_step=0.198]batch: 147 => Loss: 0.2074364274740219\n",
      "Epoch 0:   3%|██▋                                                                                           | 148/5214 [07:00<3:59:37,  0.35it/s, v_num=3071, train_loss_step=0.207]batch: 148 => Loss: 0.21573786437511444\n",
      "Epoch 0:   3%|██▋                                                                                           | 149/5214 [07:02<3:59:35,  0.35it/s, v_num=3071, train_loss_step=0.216]batch: 149 => Loss: 0.18511366844177246\n",
      "Epoch 0:   3%|██▋                                                                                           | 150/5214 [07:05<3:59:32,  0.35it/s, v_num=3071, train_loss_step=0.185]batch: 150 => Loss: 0.14179645478725433\n",
      "Epoch 0:   3%|██▋                                                                                           | 151/5214 [07:08<3:59:30,  0.35it/s, v_num=3071, train_loss_step=0.142]batch: 151 => Loss: 0.15416908264160156\n",
      "Epoch 0:   3%|██▋                                                                                           | 152/5214 [07:11<3:59:28,  0.35it/s, v_num=3071, train_loss_step=0.154]batch: 152 => Loss: 0.1361018270254135\n",
      "Epoch 0:   3%|██▊                                                                                           | 153/5214 [07:14<3:59:26,  0.35it/s, v_num=3071, train_loss_step=0.136]batch: 153 => Loss: 0.18193796277046204\n",
      "Epoch 0:   3%|██▊                                                                                           | 154/5214 [07:17<3:59:24,  0.35it/s, v_num=3071, train_loss_step=0.182]batch: 154 => Loss: 0.09708651155233383\n",
      "Epoch 0:   3%|██▊                                                                                          | 155/5214 [07:20<3:59:22,  0.35it/s, v_num=3071, train_loss_step=0.0971]batch: 155 => Loss: 0.1352224349975586\n",
      "Epoch 0:   3%|██▊                                                                                           | 156/5214 [07:22<3:59:19,  0.35it/s, v_num=3071, train_loss_step=0.135]batch: 156 => Loss: 0.12364375591278076\n",
      "Epoch 0:   3%|██▊                                                                                           | 157/5214 [07:25<3:59:17,  0.35it/s, v_num=3071, train_loss_step=0.124]batch: 157 => Loss: 0.16364046931266785\n",
      "Epoch 0:   3%|██▊                                                                                           | 158/5214 [07:28<3:59:15,  0.35it/s, v_num=3071, train_loss_step=0.164]batch: 158 => Loss: 0.1647772490978241\n",
      "Epoch 0:   3%|██▊                                                                                           | 159/5214 [07:31<3:59:13,  0.35it/s, v_num=3071, train_loss_step=0.165]batch: 159 => Loss: 0.18142715096473694\n",
      "Epoch 0:   3%|██▉                                                                                           | 160/5214 [07:34<3:59:11,  0.35it/s, v_num=3071, train_loss_step=0.181]batch: 160 => Loss: 0.15142707526683807\n",
      "Epoch 0:   3%|██▉                                                                                           | 161/5214 [07:37<3:59:09,  0.35it/s, v_num=3071, train_loss_step=0.151]batch: 161 => Loss: 0.18519540131092072\n",
      "Epoch 0:   3%|██▉                                                                                           | 162/5214 [07:40<3:59:06,  0.35it/s, v_num=3071, train_loss_step=0.185]batch: 162 => Loss: 0.12538467347621918\n",
      "Epoch 0:   3%|██▉                                                                                           | 163/5214 [07:42<3:59:04,  0.35it/s, v_num=3071, train_loss_step=0.125]batch: 163 => Loss: 0.24052460491657257\n",
      "Epoch 0:   3%|██▉                                                                                           | 164/5214 [07:45<3:59:02,  0.35it/s, v_num=3071, train_loss_step=0.241]batch: 164 => Loss: 0.19780592620372772\n",
      "Epoch 0:   3%|██▉                                                                                           | 165/5214 [07:48<3:58:59,  0.35it/s, v_num=3071, train_loss_step=0.198]batch: 165 => Loss: 0.23113954067230225\n",
      "Epoch 0:   3%|██▉                                                                                           | 166/5214 [07:51<3:58:57,  0.35it/s, v_num=3071, train_loss_step=0.231]batch: 166 => Loss: 0.186842143535614\n",
      "Epoch 0:   3%|███                                                                                           | 167/5214 [07:54<3:58:54,  0.35it/s, v_num=3071, train_loss_step=0.187]batch: 167 => Loss: 0.12654796242713928\n",
      "Epoch 0:   3%|███                                                                                           | 168/5214 [07:57<3:58:52,  0.35it/s, v_num=3071, train_loss_step=0.127]batch: 168 => Loss: 0.09577032178640366\n",
      "Epoch 0:   3%|███                                                                                          | 169/5214 [08:00<3:58:49,  0.35it/s, v_num=3071, train_loss_step=0.0958]batch: 169 => Loss: 0.19874757528305054\n",
      "Epoch 0:   3%|███                                                                                           | 170/5214 [08:02<3:58:47,  0.35it/s, v_num=3071, train_loss_step=0.199]batch: 170 => Loss: 0.2039429247379303\n",
      "Epoch 0:   3%|███                                                                                           | 171/5214 [08:05<3:58:45,  0.35it/s, v_num=3071, train_loss_step=0.204]batch: 171 => Loss: 0.2202555239200592\n",
      "Epoch 0:   3%|███                                                                                           | 172/5214 [08:08<3:58:44,  0.35it/s, v_num=3071, train_loss_step=0.220]batch: 172 => Loss: 0.14705252647399902\n",
      "Epoch 0:   3%|███                                                                                           | 173/5214 [08:11<3:58:41,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 173 => Loss: 0.14655327796936035\n",
      "Epoch 0:   3%|███▏                                                                                          | 174/5214 [08:14<3:58:39,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 174 => Loss: 0.20274029672145844\n",
      "Epoch 0:   3%|███▏                                                                                          | 175/5214 [08:17<3:58:37,  0.35it/s, v_num=3071, train_loss_step=0.203]batch: 175 => Loss: 0.14880530536174774\n",
      "Epoch 0:   3%|███▏                                                                                          | 176/5214 [08:20<3:58:34,  0.35it/s, v_num=3071, train_loss_step=0.149]batch: 176 => Loss: 0.15528614819049835\n",
      "Epoch 0:   3%|███▏                                                                                          | 177/5214 [08:22<3:58:32,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 177 => Loss: 0.24484245479106903\n",
      "Epoch 0:   3%|███▏                                                                                          | 178/5214 [08:25<3:58:30,  0.35it/s, v_num=3071, train_loss_step=0.245]batch: 178 => Loss: 0.13694314658641815\n",
      "Epoch 0:   3%|███▏                                                                                          | 179/5214 [08:28<3:58:31,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 179 => Loss: 0.19596979022026062\n",
      "Epoch 0:   3%|███▏                                                                                          | 180/5214 [08:31<3:58:29,  0.35it/s, v_num=3071, train_loss_step=0.196]batch: 180 => Loss: 0.16579721868038177\n",
      "Epoch 0:   3%|███▎                                                                                          | 181/5214 [08:34<3:58:27,  0.35it/s, v_num=3071, train_loss_step=0.166]batch: 181 => Loss: 0.11344680935144424\n",
      "Epoch 0:   3%|███▎                                                                                          | 182/5214 [08:37<3:58:24,  0.35it/s, v_num=3071, train_loss_step=0.113]batch: 182 => Loss: 0.2112305611371994\n",
      "Epoch 0:   4%|███▎                                                                                          | 183/5214 [08:40<3:58:22,  0.35it/s, v_num=3071, train_loss_step=0.211]batch: 183 => Loss: 0.3001473546028137\n",
      "Epoch 0:   4%|███▎                                                                                          | 184/5214 [08:43<3:58:19,  0.35it/s, v_num=3071, train_loss_step=0.300]batch: 184 => Loss: 0.15911364555358887\n",
      "Epoch 0:   4%|███▎                                                                                          | 185/5214 [08:45<3:58:17,  0.35it/s, v_num=3071, train_loss_step=0.159]batch: 185 => Loss: 0.11833325773477554\n",
      "Epoch 0:   4%|███▎                                                                                          | 186/5214 [08:48<3:58:14,  0.35it/s, v_num=3071, train_loss_step=0.118]batch: 186 => Loss: 0.10293938964605331\n",
      "Epoch 0:   4%|███▎                                                                                          | 187/5214 [08:51<3:58:12,  0.35it/s, v_num=3071, train_loss_step=0.103]batch: 187 => Loss: 0.15711024403572083\n",
      "Epoch 0:   4%|███▍                                                                                          | 188/5214 [08:54<3:58:10,  0.35it/s, v_num=3071, train_loss_step=0.157]batch: 188 => Loss: 0.13743610680103302\n",
      "Epoch 0:   4%|███▍                                                                                          | 189/5214 [08:57<3:58:07,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 189 => Loss: 0.20353566110134125\n",
      "Epoch 0:   4%|███▍                                                                                          | 190/5214 [09:00<3:58:05,  0.35it/s, v_num=3071, train_loss_step=0.204]batch: 190 => Loss: 0.11808779090642929\n",
      "Epoch 0:   4%|███▍                                                                                          | 191/5214 [09:03<3:58:03,  0.35it/s, v_num=3071, train_loss_step=0.118]batch: 191 => Loss: 0.15462127327919006\n",
      "Epoch 0:   4%|███▍                                                                                          | 192/5214 [09:06<3:58:01,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 192 => Loss: 0.19182835519313812\n",
      "Epoch 0:   4%|███▍                                                                                          | 193/5214 [09:08<3:57:58,  0.35it/s, v_num=3071, train_loss_step=0.192]batch: 193 => Loss: 0.23478637635707855\n",
      "Epoch 0:   4%|███▍                                                                                          | 194/5214 [09:11<3:57:56,  0.35it/s, v_num=3071, train_loss_step=0.235]batch: 194 => Loss: 0.20901520550251007\n",
      "Epoch 0:   4%|███▌                                                                                          | 195/5214 [09:14<3:57:54,  0.35it/s, v_num=3071, train_loss_step=0.209]batch: 195 => Loss: 0.07989003509283066\n",
      "Epoch 0:   4%|███▍                                                                                         | 196/5214 [09:17<3:57:51,  0.35it/s, v_num=3071, train_loss_step=0.0799]batch: 196 => Loss: 0.12648387253284454\n",
      "Epoch 0:   4%|███▌                                                                                          | 197/5214 [09:20<3:57:48,  0.35it/s, v_num=3071, train_loss_step=0.126]batch: 197 => Loss: 0.11288400739431381\n",
      "Epoch 0:   4%|███▌                                                                                          | 198/5214 [09:23<3:57:45,  0.35it/s, v_num=3071, train_loss_step=0.113]batch: 198 => Loss: 0.16414345800876617\n",
      "Epoch 0:   4%|███▌                                                                                          | 199/5214 [09:25<3:57:43,  0.35it/s, v_num=3071, train_loss_step=0.164]batch: 199 => Loss: 0.17758864164352417\n",
      "Epoch 0:   4%|███▌                                                                                          | 200/5214 [09:28<3:57:40,  0.35it/s, v_num=3071, train_loss_step=0.178]batch: 200 => Loss: 0.15282145142555237\n",
      "Epoch 0:   4%|███▌                                                                                          | 201/5214 [09:31<3:57:38,  0.35it/s, v_num=3071, train_loss_step=0.153]batch: 201 => Loss: 0.16000790894031525\n",
      "Epoch 0:   4%|███▋                                                                                          | 202/5214 [09:34<3:57:35,  0.35it/s, v_num=3071, train_loss_step=0.160]batch: 202 => Loss: 0.19082604348659515\n",
      "Epoch 0:   4%|███▋                                                                                          | 203/5214 [09:37<3:57:33,  0.35it/s, v_num=3071, train_loss_step=0.191]batch: 203 => Loss: 0.16161350905895233\n",
      "Epoch 0:   4%|███▋                                                                                          | 204/5214 [09:40<3:57:31,  0.35it/s, v_num=3071, train_loss_step=0.162]batch: 204 => Loss: 0.22917237877845764\n",
      "Epoch 0:   4%|███▋                                                                                          | 205/5214 [09:43<3:57:29,  0.35it/s, v_num=3071, train_loss_step=0.229]batch: 205 => Loss: 0.1369781345129013\n",
      "Epoch 0:   4%|███▋                                                                                          | 206/5214 [09:46<3:57:26,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 206 => Loss: 0.24665383994579315\n",
      "Epoch 0:   4%|███▋                                                                                          | 207/5214 [09:48<3:57:24,  0.35it/s, v_num=3071, train_loss_step=0.247]batch: 207 => Loss: 0.0960196852684021\n",
      "Epoch 0:   4%|███▋                                                                                          | 208/5214 [09:51<3:57:21,  0.35it/s, v_num=3071, train_loss_step=0.096]batch: 208 => Loss: 0.24747057259082794\n",
      "Epoch 0:   4%|███▊                                                                                          | 209/5214 [09:54<3:57:19,  0.35it/s, v_num=3071, train_loss_step=0.247]batch: 209 => Loss: 0.17768457531929016\n",
      "Epoch 0:   4%|███▊                                                                                          | 210/5214 [09:57<3:57:17,  0.35it/s, v_num=3071, train_loss_step=0.178]batch: 210 => Loss: 0.18361437320709229\n",
      "Epoch 0:   4%|███▊                                                                                          | 211/5214 [10:00<3:57:15,  0.35it/s, v_num=3071, train_loss_step=0.184]batch: 211 => Loss: 0.11522294580936432\n",
      "Epoch 0:   4%|███▊                                                                                          | 212/5214 [10:03<3:57:12,  0.35it/s, v_num=3071, train_loss_step=0.115]batch: 212 => Loss: 0.29686006903648376\n",
      "Epoch 0:   4%|███▊                                                                                          | 213/5214 [10:06<3:57:10,  0.35it/s, v_num=3071, train_loss_step=0.297]batch: 213 => Loss: 0.22245101630687714\n",
      "Epoch 0:   4%|███▊                                                                                          | 214/5214 [10:08<3:57:07,  0.35it/s, v_num=3071, train_loss_step=0.222]batch: 214 => Loss: 0.14622782170772552\n",
      "Epoch 0:   4%|███▉                                                                                          | 215/5214 [10:11<3:57:05,  0.35it/s, v_num=3071, train_loss_step=0.146]batch: 215 => Loss: 0.1361941397190094\n",
      "Epoch 0:   4%|███▉                                                                                          | 216/5214 [10:14<3:57:02,  0.35it/s, v_num=3071, train_loss_step=0.136]batch: 216 => Loss: 0.1791248917579651\n",
      "Epoch 0:   4%|███▉                                                                                          | 217/5214 [10:17<3:57:00,  0.35it/s, v_num=3071, train_loss_step=0.179]batch: 217 => Loss: 0.17226159572601318\n",
      "Epoch 0:   4%|███▉                                                                                          | 218/5214 [10:20<3:56:58,  0.35it/s, v_num=3071, train_loss_step=0.172]batch: 218 => Loss: 0.17796258628368378\n",
      "Epoch 0:   4%|███▉                                                                                          | 219/5214 [10:23<3:56:56,  0.35it/s, v_num=3071, train_loss_step=0.178]batch: 219 => Loss: 0.15957598388195038\n",
      "Epoch 0:   4%|███▉                                                                                          | 220/5214 [10:26<3:56:53,  0.35it/s, v_num=3071, train_loss_step=0.160]batch: 220 => Loss: 0.17814306914806366\n",
      "Epoch 0:   4%|███▉                                                                                          | 221/5214 [10:29<3:56:50,  0.35it/s, v_num=3071, train_loss_step=0.178]batch: 221 => Loss: 0.12649908661842346\n",
      "Epoch 0:   4%|████                                                                                          | 222/5214 [10:31<3:56:48,  0.35it/s, v_num=3071, train_loss_step=0.126]batch: 222 => Loss: 0.21559929847717285\n",
      "Epoch 0:   4%|████                                                                                          | 223/5214 [10:34<3:56:46,  0.35it/s, v_num=3071, train_loss_step=0.216]batch: 223 => Loss: 0.10181548446416855\n",
      "Epoch 0:   4%|████                                                                                          | 224/5214 [10:37<3:56:43,  0.35it/s, v_num=3071, train_loss_step=0.102]batch: 224 => Loss: 0.2466314285993576\n",
      "Epoch 0:   4%|████                                                                                          | 225/5214 [10:40<3:56:40,  0.35it/s, v_num=3071, train_loss_step=0.247]batch: 225 => Loss: 0.2691109776496887\n",
      "Epoch 0:   4%|████                                                                                          | 226/5214 [10:43<3:56:38,  0.35it/s, v_num=3071, train_loss_step=0.269]batch: 226 => Loss: 0.15066681802272797\n",
      "Epoch 0:   4%|████                                                                                          | 227/5214 [10:46<3:56:35,  0.35it/s, v_num=3071, train_loss_step=0.151]batch: 227 => Loss: 0.20793269574642181\n",
      "Epoch 0:   4%|████                                                                                          | 228/5214 [10:49<3:56:32,  0.35it/s, v_num=3071, train_loss_step=0.208]batch: 228 => Loss: 0.13164477050304413\n",
      "Epoch 0:   4%|████▏                                                                                         | 229/5214 [10:51<3:56:30,  0.35it/s, v_num=3071, train_loss_step=0.132]batch: 229 => Loss: 0.2050655633211136\n",
      "Epoch 0:   4%|████▏                                                                                         | 230/5214 [10:54<3:56:27,  0.35it/s, v_num=3071, train_loss_step=0.205]batch: 230 => Loss: 0.2395678013563156\n",
      "Epoch 0:   4%|████▏                                                                                         | 231/5214 [10:57<3:56:25,  0.35it/s, v_num=3071, train_loss_step=0.240]batch: 231 => Loss: 0.13163664937019348\n",
      "Epoch 0:   4%|████▏                                                                                         | 232/5214 [11:00<3:56:22,  0.35it/s, v_num=3071, train_loss_step=0.132]batch: 232 => Loss: 0.24811315536499023\n",
      "Epoch 0:   4%|████▏                                                                                         | 233/5214 [11:03<3:56:19,  0.35it/s, v_num=3071, train_loss_step=0.248]batch: 233 => Loss: 0.22428858280181885\n",
      "Epoch 0:   4%|████▏                                                                                         | 234/5214 [11:06<3:56:16,  0.35it/s, v_num=3071, train_loss_step=0.224]batch: 234 => Loss: 0.09791997820138931\n",
      "Epoch 0:   5%|████▏                                                                                        | 235/5214 [11:09<3:56:14,  0.35it/s, v_num=3071, train_loss_step=0.0979]batch: 235 => Loss: 0.10133905708789825\n",
      "Epoch 0:   5%|████▎                                                                                         | 236/5214 [11:11<3:56:12,  0.35it/s, v_num=3071, train_loss_step=0.101]batch: 236 => Loss: 0.1364240199327469\n",
      "Epoch 0:   5%|████▎                                                                                         | 237/5214 [11:14<3:56:10,  0.35it/s, v_num=3071, train_loss_step=0.136]batch: 237 => Loss: 0.1366770714521408\n",
      "Epoch 0:   5%|████▎                                                                                         | 238/5214 [11:17<3:56:07,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 238 => Loss: 0.29757627844810486\n",
      "Epoch 0:   5%|████▎                                                                                         | 239/5214 [11:20<3:56:05,  0.35it/s, v_num=3071, train_loss_step=0.298]batch: 239 => Loss: 0.19565998017787933\n",
      "Epoch 0:   5%|████▎                                                                                         | 240/5214 [11:23<3:56:03,  0.35it/s, v_num=3071, train_loss_step=0.196]batch: 240 => Loss: 0.1369994729757309\n",
      "Epoch 0:   5%|████▎                                                                                         | 241/5214 [11:26<3:56:00,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 241 => Loss: 0.13269393146038055\n",
      "Epoch 0:   5%|████▎                                                                                         | 242/5214 [11:29<3:55:57,  0.35it/s, v_num=3071, train_loss_step=0.133]batch: 242 => Loss: 0.16520576179027557\n",
      "Epoch 0:   5%|████▍                                                                                         | 243/5214 [11:31<3:55:55,  0.35it/s, v_num=3071, train_loss_step=0.165]batch: 243 => Loss: 0.12698747217655182\n",
      "Epoch 0:   5%|████▍                                                                                         | 244/5214 [11:34<3:55:52,  0.35it/s, v_num=3071, train_loss_step=0.127]batch: 244 => Loss: 0.1289306879043579\n",
      "Epoch 0:   5%|████▍                                                                                         | 245/5214 [11:37<3:55:49,  0.35it/s, v_num=3071, train_loss_step=0.129]batch: 245 => Loss: 0.19578318297863007\n",
      "Epoch 0:   5%|████▍                                                                                         | 246/5214 [11:40<3:55:47,  0.35it/s, v_num=3071, train_loss_step=0.196]batch: 246 => Loss: 0.1657724678516388\n",
      "Epoch 0:   5%|████▍                                                                                         | 247/5214 [11:43<3:55:44,  0.35it/s, v_num=3071, train_loss_step=0.166]batch: 247 => Loss: 0.21289734542369843\n",
      "Epoch 0:   5%|████▍                                                                                         | 248/5214 [11:46<3:55:42,  0.35it/s, v_num=3071, train_loss_step=0.213]batch: 248 => Loss: 0.13407687842845917\n",
      "Epoch 0:   5%|████▍                                                                                         | 249/5214 [11:49<3:55:39,  0.35it/s, v_num=3071, train_loss_step=0.134]batch: 249 => Loss: 0.1873476505279541\n",
      "Epoch 0:   5%|████▌                                                                                         | 250/5214 [11:51<3:55:36,  0.35it/s, v_num=3071, train_loss_step=0.187]batch: 250 => Loss: 0.12338373810052872\n",
      "Epoch 0:   5%|████▌                                                                                         | 251/5214 [11:54<3:55:34,  0.35it/s, v_num=3071, train_loss_step=0.123]batch: 251 => Loss: 0.14454780519008636\n",
      "Epoch 0:   5%|████▌                                                                                         | 252/5214 [11:57<3:55:32,  0.35it/s, v_num=3071, train_loss_step=0.145]batch: 252 => Loss: 0.12197097390890121\n",
      "Epoch 0:   5%|████▌                                                                                         | 253/5214 [12:00<3:55:29,  0.35it/s, v_num=3071, train_loss_step=0.122]batch: 253 => Loss: 0.198433980345726\n",
      "Epoch 0:   5%|████▌                                                                                         | 254/5214 [12:03<3:55:27,  0.35it/s, v_num=3071, train_loss_step=0.198]batch: 254 => Loss: 0.14756636321544647\n",
      "Epoch 0:   5%|████▌                                                                                         | 255/5214 [12:06<3:55:24,  0.35it/s, v_num=3071, train_loss_step=0.148]batch: 255 => Loss: 0.09394761174917221\n",
      "Epoch 0:   5%|████▌                                                                                        | 256/5214 [12:09<3:55:21,  0.35it/s, v_num=3071, train_loss_step=0.0939]batch: 256 => Loss: 0.07695409655570984\n",
      "Epoch 0:   5%|████▋                                                                                         | 257/5214 [12:12<3:55:19,  0.35it/s, v_num=3071, train_loss_step=0.077]batch: 257 => Loss: 0.1779579222202301\n",
      "Epoch 0:   5%|████▋                                                                                         | 258/5214 [12:14<3:55:16,  0.35it/s, v_num=3071, train_loss_step=0.178]batch: 258 => Loss: 0.15892022848129272\n",
      "Epoch 0:   5%|████▋                                                                                         | 259/5214 [12:17<3:55:13,  0.35it/s, v_num=3071, train_loss_step=0.159]batch: 259 => Loss: 0.269737184047699\n",
      "Epoch 0:   5%|████▋                                                                                         | 260/5214 [12:20<3:55:11,  0.35it/s, v_num=3071, train_loss_step=0.270]batch: 260 => Loss: 0.16473309695720673\n",
      "Epoch 0:   5%|████▋                                                                                         | 261/5214 [12:23<3:55:08,  0.35it/s, v_num=3071, train_loss_step=0.165]batch: 261 => Loss: 0.16191695630550385\n",
      "Epoch 0:   5%|████▋                                                                                         | 262/5214 [12:26<3:55:06,  0.35it/s, v_num=3071, train_loss_step=0.162]batch: 262 => Loss: 0.14632868766784668\n",
      "Epoch 0:   5%|████▋                                                                                         | 263/5214 [12:29<3:55:03,  0.35it/s, v_num=3071, train_loss_step=0.146]batch: 263 => Loss: 0.1028825044631958\n",
      "Epoch 0:   5%|████▊                                                                                         | 264/5214 [12:32<3:55:01,  0.35it/s, v_num=3071, train_loss_step=0.103]batch: 264 => Loss: 0.22631262242794037\n",
      "Epoch 0:   5%|████▊                                                                                         | 265/5214 [12:34<3:54:58,  0.35it/s, v_num=3071, train_loss_step=0.226]batch: 265 => Loss: 0.11300169676542282\n",
      "Epoch 0:   5%|████▊                                                                                         | 266/5214 [12:37<3:54:55,  0.35it/s, v_num=3071, train_loss_step=0.113]batch: 266 => Loss: 0.23686395585536957\n",
      "Epoch 0:   5%|████▊                                                                                         | 267/5214 [12:40<3:54:53,  0.35it/s, v_num=3071, train_loss_step=0.237]batch: 267 => Loss: 0.17066851258277893\n",
      "Epoch 0:   5%|████▊                                                                                         | 268/5214 [12:43<3:54:50,  0.35it/s, v_num=3071, train_loss_step=0.171]batch: 268 => Loss: 0.13204620778560638\n",
      "Epoch 0:   5%|████▊                                                                                         | 269/5214 [12:46<3:54:47,  0.35it/s, v_num=3071, train_loss_step=0.132]batch: 269 => Loss: 0.1504371166229248\n",
      "Epoch 0:   5%|████▊                                                                                         | 270/5214 [12:49<3:54:44,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 270 => Loss: 0.12668351829051971\n",
      "Epoch 0:   5%|████▉                                                                                         | 271/5214 [12:52<3:54:42,  0.35it/s, v_num=3071, train_loss_step=0.127]batch: 271 => Loss: 0.13776688277721405\n",
      "Epoch 0:   5%|████▉                                                                                         | 272/5214 [12:54<3:54:39,  0.35it/s, v_num=3071, train_loss_step=0.138]batch: 272 => Loss: 0.15983299911022186\n",
      "Epoch 0:   5%|████▉                                                                                         | 273/5214 [12:57<3:54:36,  0.35it/s, v_num=3071, train_loss_step=0.160]batch: 273 => Loss: 0.1607370227575302\n",
      "Epoch 0:   5%|████▉                                                                                         | 274/5214 [13:00<3:54:33,  0.35it/s, v_num=3071, train_loss_step=0.161]batch: 274 => Loss: 0.18141056597232819\n",
      "Epoch 0:   5%|████▉                                                                                         | 275/5214 [13:03<3:54:31,  0.35it/s, v_num=3071, train_loss_step=0.181]batch: 275 => Loss: 0.10943140834569931\n",
      "Epoch 0:   5%|████▉                                                                                         | 276/5214 [13:06<3:54:28,  0.35it/s, v_num=3071, train_loss_step=0.109]batch: 276 => Loss: 0.13502518832683563\n",
      "Epoch 0:   5%|████▉                                                                                         | 277/5214 [13:09<3:54:25,  0.35it/s, v_num=3071, train_loss_step=0.135]batch: 277 => Loss: 0.1545218974351883\n",
      "Epoch 0:   5%|█████                                                                                         | 278/5214 [13:12<3:54:22,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 278 => Loss: 0.14229322969913483\n",
      "Epoch 0:   5%|█████                                                                                         | 279/5214 [13:14<3:54:19,  0.35it/s, v_num=3071, train_loss_step=0.142]batch: 279 => Loss: 0.22327835857868195\n",
      "Epoch 0:   5%|█████                                                                                         | 280/5214 [13:17<3:54:17,  0.35it/s, v_num=3071, train_loss_step=0.223]batch: 280 => Loss: 0.14560368657112122\n",
      "Epoch 0:   5%|█████                                                                                         | 281/5214 [13:20<3:54:14,  0.35it/s, v_num=3071, train_loss_step=0.146]batch: 281 => Loss: 0.11579791456460953\n",
      "Epoch 0:   5%|█████                                                                                         | 282/5214 [13:23<3:54:12,  0.35it/s, v_num=3071, train_loss_step=0.116]batch: 282 => Loss: 0.11786317080259323\n",
      "Epoch 0:   5%|█████                                                                                         | 283/5214 [13:26<3:54:09,  0.35it/s, v_num=3071, train_loss_step=0.118]batch: 283 => Loss: 0.13777799904346466\n",
      "Epoch 0:   5%|█████                                                                                         | 284/5214 [13:29<3:54:07,  0.35it/s, v_num=3071, train_loss_step=0.138]batch: 284 => Loss: 0.1711856573820114\n",
      "Epoch 0:   5%|█████▏                                                                                        | 285/5214 [13:32<3:54:04,  0.35it/s, v_num=3071, train_loss_step=0.171]batch: 285 => Loss: 0.17859701812267303\n",
      "Epoch 0:   5%|█████▏                                                                                        | 286/5214 [13:34<3:54:01,  0.35it/s, v_num=3071, train_loss_step=0.179]batch: 286 => Loss: 0.18491147458553314\n",
      "Epoch 0:   6%|█████▏                                                                                        | 287/5214 [13:37<3:53:59,  0.35it/s, v_num=3071, train_loss_step=0.185]batch: 287 => Loss: 0.12201278656721115\n",
      "Epoch 0:   6%|█████▏                                                                                        | 288/5214 [13:40<3:53:56,  0.35it/s, v_num=3071, train_loss_step=0.122]batch: 288 => Loss: 0.10672339051961899\n",
      "Epoch 0:   6%|█████▏                                                                                        | 289/5214 [13:43<3:53:53,  0.35it/s, v_num=3071, train_loss_step=0.107]batch: 289 => Loss: 0.10673309862613678\n",
      "Epoch 0:   6%|█████▏                                                                                        | 290/5214 [13:46<3:53:50,  0.35it/s, v_num=3071, train_loss_step=0.107]batch: 290 => Loss: 0.16767875850200653\n",
      "Epoch 0:   6%|█████▏                                                                                        | 291/5214 [13:49<3:53:47,  0.35it/s, v_num=3071, train_loss_step=0.168]batch: 291 => Loss: 0.1907532513141632\n",
      "Epoch 0:   6%|█████▎                                                                                        | 292/5214 [13:52<3:53:44,  0.35it/s, v_num=3071, train_loss_step=0.191]batch: 292 => Loss: 0.13228647410869598\n",
      "Epoch 0:   6%|█████▎                                                                                        | 293/5214 [13:54<3:53:42,  0.35it/s, v_num=3071, train_loss_step=0.132]batch: 293 => Loss: 0.12081809341907501\n",
      "Epoch 0:   6%|█████▎                                                                                        | 294/5214 [13:57<3:53:39,  0.35it/s, v_num=3071, train_loss_step=0.121]batch: 294 => Loss: 0.17107807099819183\n",
      "Epoch 0:   6%|█████▎                                                                                        | 295/5214 [14:00<3:53:36,  0.35it/s, v_num=3071, train_loss_step=0.171]batch: 295 => Loss: 0.11273043602705002\n",
      "Epoch 0:   6%|█████▎                                                                                        | 296/5214 [14:03<3:53:34,  0.35it/s, v_num=3071, train_loss_step=0.113]batch: 296 => Loss: 0.2067364901304245\n",
      "Epoch 0:   6%|█████▎                                                                                        | 297/5214 [14:06<3:53:31,  0.35it/s, v_num=3071, train_loss_step=0.207]batch: 297 => Loss: 0.16287992894649506\n",
      "Epoch 0:   6%|█████▎                                                                                        | 298/5214 [14:09<3:53:28,  0.35it/s, v_num=3071, train_loss_step=0.163]batch: 298 => Loss: 0.261379212141037\n",
      "Epoch 0:   6%|█████▍                                                                                        | 299/5214 [14:12<3:53:25,  0.35it/s, v_num=3071, train_loss_step=0.261]batch: 299 => Loss: 0.16992034018039703\n",
      "Epoch 0:   6%|█████▍                                                                                        | 300/5214 [14:14<3:53:22,  0.35it/s, v_num=3071, train_loss_step=0.170]batch: 300 => Loss: 0.13630811870098114\n",
      "Epoch 0:   6%|█████▍                                                                                        | 301/5214 [14:17<3:53:20,  0.35it/s, v_num=3071, train_loss_step=0.136]batch: 301 => Loss: 0.16061387956142426\n",
      "Epoch 0:   6%|█████▍                                                                                        | 302/5214 [14:20<3:53:17,  0.35it/s, v_num=3071, train_loss_step=0.161]batch: 302 => Loss: 0.2101181596517563\n",
      "Epoch 0:   6%|█████▍                                                                                        | 303/5214 [14:23<3:53:14,  0.35it/s, v_num=3071, train_loss_step=0.210]batch: 303 => Loss: 0.3061486780643463\n",
      "Epoch 0:   6%|█████▍                                                                                        | 304/5214 [14:26<3:53:12,  0.35it/s, v_num=3071, train_loss_step=0.306]batch: 304 => Loss: 0.17731483280658722\n",
      "Epoch 0:   6%|█████▍                                                                                        | 305/5214 [14:29<3:53:09,  0.35it/s, v_num=3071, train_loss_step=0.177]batch: 305 => Loss: 0.10426586866378784\n",
      "Epoch 0:   6%|█████▌                                                                                        | 306/5214 [14:32<3:53:06,  0.35it/s, v_num=3071, train_loss_step=0.104]batch: 306 => Loss: 0.15237148106098175\n",
      "Epoch 0:   6%|█████▌                                                                                        | 307/5214 [14:34<3:53:04,  0.35it/s, v_num=3071, train_loss_step=0.152]batch: 307 => Loss: 0.19242605566978455\n",
      "Epoch 0:   6%|█████▌                                                                                        | 308/5214 [14:37<3:53:01,  0.35it/s, v_num=3071, train_loss_step=0.192]batch: 308 => Loss: 0.17878778278827667\n",
      "Epoch 0:   6%|█████▌                                                                                        | 309/5214 [14:40<3:52:58,  0.35it/s, v_num=3071, train_loss_step=0.179]batch: 309 => Loss: 0.2086019068956375\n",
      "Epoch 0:   6%|█████▌                                                                                        | 310/5214 [14:43<3:52:56,  0.35it/s, v_num=3071, train_loss_step=0.209]batch: 310 => Loss: 0.23313914239406586\n",
      "Epoch 0:   6%|█████▌                                                                                        | 311/5214 [14:46<3:52:53,  0.35it/s, v_num=3071, train_loss_step=0.233]batch: 311 => Loss: 0.17790232598781586\n",
      "Epoch 0:   6%|█████▌                                                                                        | 312/5214 [14:49<3:52:50,  0.35it/s, v_num=3071, train_loss_step=0.178]batch: 312 => Loss: 0.19423529505729675\n",
      "Epoch 0:   6%|█████▋                                                                                        | 313/5214 [14:52<3:52:47,  0.35it/s, v_num=3071, train_loss_step=0.194]batch: 313 => Loss: 0.1539103239774704\n",
      "Epoch 0:   6%|█████▋                                                                                        | 314/5214 [14:54<3:52:44,  0.35it/s, v_num=3071, train_loss_step=0.154]batch: 314 => Loss: 0.2798795700073242\n",
      "Epoch 0:   6%|█████▋                                                                                        | 315/5214 [14:57<3:52:41,  0.35it/s, v_num=3071, train_loss_step=0.280]batch: 315 => Loss: 0.0936068743467331\n",
      "Epoch 0:   6%|█████▋                                                                                       | 316/5214 [15:00<3:52:38,  0.35it/s, v_num=3071, train_loss_step=0.0936]batch: 316 => Loss: 0.1466267853975296\n",
      "Epoch 0:   6%|█████▋                                                                                        | 317/5214 [15:03<3:52:36,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 317 => Loss: 0.14342103898525238\n",
      "Epoch 0:   6%|█████▋                                                                                        | 318/5214 [15:06<3:52:33,  0.35it/s, v_num=3071, train_loss_step=0.143]batch: 318 => Loss: 0.2598957121372223\n",
      "Epoch 0:   6%|█████▊                                                                                        | 319/5214 [15:09<3:52:30,  0.35it/s, v_num=3071, train_loss_step=0.260]batch: 319 => Loss: 0.22683556377887726\n",
      "Epoch 0:   6%|█████▊                                                                                        | 320/5214 [15:11<3:52:27,  0.35it/s, v_num=3071, train_loss_step=0.227]batch: 320 => Loss: 0.15639829635620117\n",
      "Epoch 0:   6%|█████▊                                                                                        | 321/5214 [15:14<3:52:24,  0.35it/s, v_num=3071, train_loss_step=0.156]batch: 321 => Loss: 0.21648965775966644\n",
      "Epoch 0:   6%|█████▊                                                                                        | 322/5214 [15:17<3:52:21,  0.35it/s, v_num=3071, train_loss_step=0.216]batch: 322 => Loss: 0.0941801518201828\n",
      "Epoch 0:   6%|█████▊                                                                                       | 323/5214 [15:20<3:52:19,  0.35it/s, v_num=3071, train_loss_step=0.0942]batch: 323 => Loss: 0.2117094099521637\n",
      "Epoch 0:   6%|█████▊                                                                                        | 324/5214 [15:23<3:52:16,  0.35it/s, v_num=3071, train_loss_step=0.212]batch: 324 => Loss: 0.2198449820280075\n",
      "Epoch 0:   6%|█████▊                                                                                        | 325/5214 [15:26<3:52:13,  0.35it/s, v_num=3071, train_loss_step=0.220]batch: 325 => Loss: 0.1591830998659134\n",
      "Epoch 0:   6%|█████▉                                                                                        | 326/5214 [15:29<3:52:10,  0.35it/s, v_num=3071, train_loss_step=0.159]batch: 326 => Loss: 0.22756071388721466\n",
      "Epoch 0:   6%|█████▉                                                                                        | 327/5214 [15:31<3:52:08,  0.35it/s, v_num=3071, train_loss_step=0.228]batch: 327 => Loss: 0.125874325633049\n",
      "Epoch 0:   6%|█████▉                                                                                        | 328/5214 [15:34<3:52:05,  0.35it/s, v_num=3071, train_loss_step=0.126]batch: 328 => Loss: 0.24146521091461182\n",
      "Epoch 0:   6%|█████▉                                                                                        | 329/5214 [15:37<3:52:03,  0.35it/s, v_num=3071, train_loss_step=0.241]batch: 329 => Loss: 0.24209356307983398\n",
      "Epoch 0:   6%|█████▉                                                                                        | 330/5214 [15:40<3:52:00,  0.35it/s, v_num=3071, train_loss_step=0.242]batch: 330 => Loss: 0.18294163048267365\n",
      "Epoch 0:   6%|█████▉                                                                                        | 331/5214 [15:43<3:51:57,  0.35it/s, v_num=3071, train_loss_step=0.183]batch: 331 => Loss: 0.1393417865037918\n",
      "Epoch 0:   6%|█████▉                                                                                        | 332/5214 [15:46<3:51:55,  0.35it/s, v_num=3071, train_loss_step=0.139]batch: 332 => Loss: 0.1842527687549591\n",
      "Epoch 0:   6%|██████                                                                                        | 333/5214 [15:49<3:51:52,  0.35it/s, v_num=3071, train_loss_step=0.184]batch: 333 => Loss: 0.19371850788593292\n",
      "Epoch 0:   6%|██████                                                                                        | 334/5214 [15:52<3:51:49,  0.35it/s, v_num=3071, train_loss_step=0.194]batch: 334 => Loss: 0.1405208259820938\n",
      "Epoch 0:   6%|██████                                                                                        | 335/5214 [15:54<3:51:46,  0.35it/s, v_num=3071, train_loss_step=0.141]batch: 335 => Loss: 0.14725904166698456\n",
      "Epoch 0:   6%|██████                                                                                        | 336/5214 [15:57<3:51:44,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 336 => Loss: 0.23867249488830566\n",
      "Epoch 0:   6%|██████                                                                                        | 337/5214 [16:00<3:51:41,  0.35it/s, v_num=3071, train_loss_step=0.239]batch: 337 => Loss: 0.13528333604335785\n",
      "Epoch 0:   6%|██████                                                                                        | 338/5214 [16:03<3:51:38,  0.35it/s, v_num=3071, train_loss_step=0.135]batch: 338 => Loss: 0.10499653965234756\n",
      "Epoch 0:   7%|██████                                                                                        | 339/5214 [16:06<3:51:36,  0.35it/s, v_num=3071, train_loss_step=0.105]batch: 339 => Loss: 0.288755863904953\n",
      "Epoch 0:   7%|██████▏                                                                                       | 340/5214 [16:09<3:51:33,  0.35it/s, v_num=3071, train_loss_step=0.289]batch: 340 => Loss: 0.18631070852279663\n",
      "Epoch 0:   7%|██████▏                                                                                       | 341/5214 [16:12<3:51:30,  0.35it/s, v_num=3071, train_loss_step=0.186]batch: 341 => Loss: 0.350398987531662\n",
      "Epoch 0:   7%|██████▏                                                                                       | 342/5214 [16:14<3:51:27,  0.35it/s, v_num=3071, train_loss_step=0.350]batch: 342 => Loss: 0.12449329346418381\n",
      "Epoch 0:   7%|██████▏                                                                                       | 343/5214 [16:17<3:51:25,  0.35it/s, v_num=3071, train_loss_step=0.124]batch: 343 => Loss: 0.14644086360931396\n",
      "Epoch 0:   7%|██████▏                                                                                       | 344/5214 [16:20<3:51:22,  0.35it/s, v_num=3071, train_loss_step=0.146]batch: 344 => Loss: 0.2549874782562256\n",
      "Epoch 0:   7%|██████▏                                                                                       | 345/5214 [16:23<3:51:19,  0.35it/s, v_num=3071, train_loss_step=0.255]batch: 345 => Loss: 0.18931317329406738\n",
      "Epoch 0:   7%|██████▏                                                                                       | 346/5214 [16:26<3:51:16,  0.35it/s, v_num=3071, train_loss_step=0.189]batch: 346 => Loss: 0.18453092873096466\n",
      "Epoch 0:   7%|██████▎                                                                                       | 347/5214 [16:29<3:51:13,  0.35it/s, v_num=3071, train_loss_step=0.185]batch: 347 => Loss: 0.14825883507728577\n",
      "Epoch 0:   7%|██████▎                                                                                       | 348/5214 [16:32<3:51:10,  0.35it/s, v_num=3071, train_loss_step=0.148]batch: 348 => Loss: 0.1445472687482834\n",
      "Epoch 0:   7%|██████▎                                                                                       | 349/5214 [16:34<3:51:08,  0.35it/s, v_num=3071, train_loss_step=0.145]batch: 349 => Loss: 0.13070516288280487\n",
      "Epoch 0:   7%|██████▎                                                                                       | 350/5214 [16:37<3:51:05,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 350 => Loss: 0.17811857163906097\n",
      "Epoch 0:   7%|██████▎                                                                                       | 351/5214 [16:40<3:51:03,  0.35it/s, v_num=3071, train_loss_step=0.178]batch: 351 => Loss: 0.2512885332107544\n",
      "Epoch 0:   7%|██████▎                                                                                       | 352/5214 [16:43<3:51:00,  0.35it/s, v_num=3071, train_loss_step=0.251]batch: 352 => Loss: 0.12142734974622726\n",
      "Epoch 0:   7%|██████▎                                                                                       | 353/5214 [16:46<3:50:57,  0.35it/s, v_num=3071, train_loss_step=0.121]batch: 353 => Loss: 0.1387258619070053\n",
      "Epoch 0:   7%|██████▍                                                                                       | 354/5214 [16:49<3:50:54,  0.35it/s, v_num=3071, train_loss_step=0.139]batch: 354 => Loss: 0.16671617329120636\n",
      "Epoch 0:   7%|██████▍                                                                                       | 355/5214 [16:52<3:50:51,  0.35it/s, v_num=3071, train_loss_step=0.167]batch: 355 => Loss: 0.1240585595369339\n",
      "Epoch 0:   7%|██████▍                                                                                       | 356/5214 [16:54<3:50:49,  0.35it/s, v_num=3071, train_loss_step=0.124]batch: 356 => Loss: 0.22377701103687286\n",
      "Epoch 0:   7%|██████▍                                                                                       | 357/5214 [16:57<3:50:46,  0.35it/s, v_num=3071, train_loss_step=0.224]batch: 357 => Loss: 0.17832665145397186\n",
      "Epoch 0:   7%|██████▍                                                                                       | 358/5214 [17:00<3:50:43,  0.35it/s, v_num=3071, train_loss_step=0.178]batch: 358 => Loss: 0.1980695128440857\n",
      "Epoch 0:   7%|██████▍                                                                                       | 359/5214 [17:03<3:50:41,  0.35it/s, v_num=3071, train_loss_step=0.198]batch: 359 => Loss: 0.18170355260372162\n",
      "Epoch 0:   7%|██████▍                                                                                       | 360/5214 [17:06<3:50:38,  0.35it/s, v_num=3071, train_loss_step=0.182]batch: 360 => Loss: 0.14445233345031738\n",
      "Epoch 0:   7%|██████▌                                                                                       | 361/5214 [17:09<3:50:35,  0.35it/s, v_num=3071, train_loss_step=0.144]batch: 361 => Loss: 0.17999643087387085\n",
      "Epoch 0:   7%|██████▌                                                                                       | 362/5214 [17:12<3:50:32,  0.35it/s, v_num=3071, train_loss_step=0.180]batch: 362 => Loss: 0.12734268605709076\n",
      "Epoch 0:   7%|██████▌                                                                                       | 363/5214 [17:14<3:50:29,  0.35it/s, v_num=3071, train_loss_step=0.127]batch: 363 => Loss: 0.1874023973941803\n",
      "Epoch 0:   7%|██████▌                                                                                       | 364/5214 [17:17<3:50:27,  0.35it/s, v_num=3071, train_loss_step=0.187]batch: 364 => Loss: 0.08273705095052719\n",
      "Epoch 0:   7%|██████▌                                                                                      | 365/5214 [17:20<3:50:24,  0.35it/s, v_num=3071, train_loss_step=0.0827]batch: 365 => Loss: 0.16311906278133392\n",
      "Epoch 0:   7%|██████▌                                                                                       | 366/5214 [17:23<3:50:21,  0.35it/s, v_num=3071, train_loss_step=0.163]batch: 366 => Loss: 0.2246684581041336\n",
      "Epoch 0:   7%|██████▌                                                                                       | 367/5214 [17:26<3:50:19,  0.35it/s, v_num=3071, train_loss_step=0.225]batch: 367 => Loss: 0.17386837303638458\n",
      "Epoch 0:   7%|██████▋                                                                                       | 368/5214 [17:29<3:50:16,  0.35it/s, v_num=3071, train_loss_step=0.174]batch: 368 => Loss: 0.1201598048210144\n",
      "Epoch 0:   7%|██████▋                                                                                       | 369/5214 [17:32<3:50:13,  0.35it/s, v_num=3071, train_loss_step=0.120]batch: 369 => Loss: 0.21482360363006592\n",
      "Epoch 0:   7%|██████▋                                                                                       | 370/5214 [17:34<3:50:10,  0.35it/s, v_num=3071, train_loss_step=0.215]batch: 370 => Loss: 0.16871492564678192\n",
      "Epoch 0:   7%|██████▋                                                                                       | 371/5214 [17:37<3:50:08,  0.35it/s, v_num=3071, train_loss_step=0.169]batch: 371 => Loss: 0.12922070920467377\n",
      "Epoch 0:   7%|██████▋                                                                                       | 372/5214 [17:40<3:50:05,  0.35it/s, v_num=3071, train_loss_step=0.129]batch: 372 => Loss: 0.27200591564178467\n",
      "Epoch 0:   7%|██████▋                                                                                       | 373/5214 [17:43<3:50:02,  0.35it/s, v_num=3071, train_loss_step=0.272]batch: 373 => Loss: 0.12386555969715118\n",
      "Epoch 0:   7%|██████▋                                                                                       | 374/5214 [17:46<3:49:59,  0.35it/s, v_num=3071, train_loss_step=0.124]batch: 374 => Loss: 0.11277680844068527\n",
      "Epoch 0:   7%|██████▊                                                                                       | 375/5214 [17:49<3:49:56,  0.35it/s, v_num=3071, train_loss_step=0.113]batch: 375 => Loss: 0.12307103723287582\n",
      "Epoch 0:   7%|██████▊                                                                                       | 376/5214 [17:52<3:49:54,  0.35it/s, v_num=3071, train_loss_step=0.123]batch: 376 => Loss: 0.2137593775987625\n",
      "Epoch 0:   7%|██████▊                                                                                       | 377/5214 [17:54<3:49:51,  0.35it/s, v_num=3071, train_loss_step=0.214]batch: 377 => Loss: 0.13947370648384094\n",
      "Epoch 0:   7%|██████▊                                                                                       | 378/5214 [17:57<3:49:48,  0.35it/s, v_num=3071, train_loss_step=0.139]batch: 378 => Loss: 0.12165693193674088\n",
      "Epoch 0:   7%|██████▊                                                                                       | 379/5214 [18:00<3:49:46,  0.35it/s, v_num=3071, train_loss_step=0.122]batch: 379 => Loss: 0.18937844038009644\n",
      "Epoch 0:   7%|██████▊                                                                                       | 380/5214 [18:03<3:49:43,  0.35it/s, v_num=3071, train_loss_step=0.189]batch: 380 => Loss: 0.11941909790039062\n",
      "Epoch 0:   7%|██████▊                                                                                       | 381/5214 [18:06<3:49:40,  0.35it/s, v_num=3071, train_loss_step=0.119]batch: 381 => Loss: 0.16742001473903656\n",
      "Epoch 0:   7%|██████▉                                                                                       | 382/5214 [18:09<3:49:38,  0.35it/s, v_num=3071, train_loss_step=0.167]batch: 382 => Loss: 0.12394461780786514\n",
      "Epoch 0:   7%|██████▉                                                                                       | 383/5214 [18:12<3:49:35,  0.35it/s, v_num=3071, train_loss_step=0.124]batch: 383 => Loss: 0.260384738445282\n",
      "Epoch 0:   7%|██████▉                                                                                       | 384/5214 [18:14<3:49:32,  0.35it/s, v_num=3071, train_loss_step=0.260]batch: 384 => Loss: 0.14959852397441864\n",
      "Epoch 0:   7%|██████▉                                                                                       | 385/5214 [18:17<3:49:29,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 385 => Loss: 0.20391777157783508\n",
      "Epoch 0:   7%|██████▉                                                                                       | 386/5214 [18:20<3:49:26,  0.35it/s, v_num=3071, train_loss_step=0.204]batch: 386 => Loss: 0.13280652463436127\n",
      "Epoch 0:   7%|██████▉                                                                                       | 387/5214 [18:23<3:49:23,  0.35it/s, v_num=3071, train_loss_step=0.133]batch: 387 => Loss: 0.1747066080570221\n",
      "Epoch 0:   7%|██████▉                                                                                       | 388/5214 [18:26<3:49:21,  0.35it/s, v_num=3071, train_loss_step=0.175]batch: 388 => Loss: 0.31447407603263855\n",
      "Epoch 0:   7%|███████                                                                                       | 389/5214 [18:29<3:49:18,  0.35it/s, v_num=3071, train_loss_step=0.314]batch: 389 => Loss: 0.15703463554382324\n",
      "Epoch 0:   7%|███████                                                                                       | 390/5214 [18:32<3:49:15,  0.35it/s, v_num=3071, train_loss_step=0.157]batch: 390 => Loss: 0.22348825633525848\n",
      "Epoch 0:   7%|███████                                                                                       | 391/5214 [18:34<3:49:13,  0.35it/s, v_num=3071, train_loss_step=0.223]batch: 391 => Loss: 0.26463186740875244\n",
      "Epoch 0:   8%|███████                                                                                       | 392/5214 [18:37<3:49:10,  0.35it/s, v_num=3071, train_loss_step=0.265]batch: 392 => Loss: 0.1464916169643402\n",
      "Epoch 0:   8%|███████                                                                                       | 393/5214 [18:40<3:49:07,  0.35it/s, v_num=3071, train_loss_step=0.146]batch: 393 => Loss: 0.20489223301410675\n",
      "Epoch 0:   8%|███████                                                                                       | 394/5214 [18:43<3:49:04,  0.35it/s, v_num=3071, train_loss_step=0.205]batch: 394 => Loss: 0.19694213569164276\n",
      "Epoch 0:   8%|███████                                                                                       | 395/5214 [18:46<3:49:01,  0.35it/s, v_num=3071, train_loss_step=0.197]batch: 395 => Loss: 0.09753447026014328\n",
      "Epoch 0:   8%|███████                                                                                      | 396/5214 [18:49<3:48:59,  0.35it/s, v_num=3071, train_loss_step=0.0975]batch: 396 => Loss: 0.10907202214002609\n",
      "Epoch 0:   8%|███████▏                                                                                      | 397/5214 [18:52<3:48:56,  0.35it/s, v_num=3071, train_loss_step=0.109]batch: 397 => Loss: 0.16352646052837372\n",
      "Epoch 0:   8%|███████▏                                                                                      | 398/5214 [18:54<3:48:53,  0.35it/s, v_num=3071, train_loss_step=0.164]batch: 398 => Loss: 0.11198259890079498\n",
      "Epoch 0:   8%|███████▏                                                                                      | 399/5214 [18:57<3:48:50,  0.35it/s, v_num=3071, train_loss_step=0.112]batch: 399 => Loss: 0.18862108886241913\n",
      "Epoch 0:   8%|███████▏                                                                                      | 400/5214 [19:00<3:48:47,  0.35it/s, v_num=3071, train_loss_step=0.189]batch: 400 => Loss: 0.17830374836921692\n",
      "Epoch 0:   8%|███████▏                                                                                      | 401/5214 [19:03<3:48:45,  0.35it/s, v_num=3071, train_loss_step=0.178]batch: 401 => Loss: 0.1597033590078354\n",
      "Epoch 0:   8%|███████▏                                                                                      | 402/5214 [19:06<3:48:42,  0.35it/s, v_num=3071, train_loss_step=0.160]batch: 402 => Loss: 0.24037747085094452\n",
      "Epoch 0:   8%|███████▎                                                                                      | 403/5214 [19:09<3:48:39,  0.35it/s, v_num=3071, train_loss_step=0.240]batch: 403 => Loss: 0.23936961591243744\n",
      "Epoch 0:   8%|███████▎                                                                                      | 404/5214 [19:12<3:48:36,  0.35it/s, v_num=3071, train_loss_step=0.239]batch: 404 => Loss: 0.1313304454088211\n",
      "Epoch 0:   8%|███████▎                                                                                      | 405/5214 [19:14<3:48:34,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 405 => Loss: 0.23387113213539124\n",
      "Epoch 0:   8%|███████▎                                                                                      | 406/5214 [19:17<3:48:31,  0.35it/s, v_num=3071, train_loss_step=0.234]batch: 406 => Loss: 0.21110720932483673\n",
      "Epoch 0:   8%|███████▎                                                                                      | 407/5214 [19:20<3:48:28,  0.35it/s, v_num=3071, train_loss_step=0.211]batch: 407 => Loss: 0.16750681400299072\n",
      "Epoch 0:   8%|███████▎                                                                                      | 408/5214 [19:23<3:48:25,  0.35it/s, v_num=3071, train_loss_step=0.168]batch: 408 => Loss: 0.18839071691036224\n",
      "Epoch 0:   8%|███████▎                                                                                      | 409/5214 [19:26<3:48:22,  0.35it/s, v_num=3071, train_loss_step=0.188]batch: 409 => Loss: 0.2165820598602295\n",
      "Epoch 0:   8%|███████▍                                                                                      | 410/5214 [19:29<3:48:19,  0.35it/s, v_num=3071, train_loss_step=0.217]batch: 410 => Loss: 0.1889452040195465\n",
      "Epoch 0:   8%|███████▍                                                                                      | 411/5214 [19:32<3:48:16,  0.35it/s, v_num=3071, train_loss_step=0.189]batch: 411 => Loss: 0.1262778788805008\n",
      "Epoch 0:   8%|███████▍                                                                                      | 412/5214 [19:34<3:48:14,  0.35it/s, v_num=3071, train_loss_step=0.126]batch: 412 => Loss: 0.1898145079612732\n",
      "Epoch 0:   8%|███████▍                                                                                      | 413/5214 [19:37<3:48:11,  0.35it/s, v_num=3071, train_loss_step=0.190]batch: 413 => Loss: 0.2595556378364563\n",
      "Epoch 0:   8%|███████▍                                                                                      | 414/5214 [19:40<3:48:08,  0.35it/s, v_num=3071, train_loss_step=0.260]batch: 414 => Loss: 0.1303578019142151\n",
      "Epoch 0:   8%|███████▍                                                                                      | 415/5214 [19:43<3:48:05,  0.35it/s, v_num=3071, train_loss_step=0.130]batch: 415 => Loss: 0.19684521853923798\n",
      "Epoch 0:   8%|███████▍                                                                                      | 416/5214 [19:46<3:48:02,  0.35it/s, v_num=3071, train_loss_step=0.197]batch: 416 => Loss: 0.09971445053815842\n",
      "Epoch 0:   8%|███████▍                                                                                     | 417/5214 [19:49<3:48:00,  0.35it/s, v_num=3071, train_loss_step=0.0997]batch: 417 => Loss: 0.2013774961233139\n",
      "Epoch 0:   8%|███████▌                                                                                      | 418/5214 [19:52<3:47:57,  0.35it/s, v_num=3071, train_loss_step=0.201]batch: 418 => Loss: 0.15231506526470184\n",
      "Epoch 0:   8%|███████▌                                                                                      | 419/5214 [19:54<3:47:54,  0.35it/s, v_num=3071, train_loss_step=0.152]batch: 419 => Loss: 0.12343137711286545\n",
      "Epoch 0:   8%|███████▌                                                                                      | 420/5214 [19:57<3:47:51,  0.35it/s, v_num=3071, train_loss_step=0.123]batch: 420 => Loss: 0.2792784869670868\n",
      "Epoch 0:   8%|███████▌                                                                                      | 421/5214 [20:00<3:47:48,  0.35it/s, v_num=3071, train_loss_step=0.279]batch: 421 => Loss: 0.13086369633674622\n",
      "Epoch 0:   8%|███████▌                                                                                      | 422/5214 [20:03<3:47:46,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 422 => Loss: 0.1597602814435959\n",
      "Epoch 0:   8%|███████▋                                                                                      | 423/5214 [20:06<3:47:43,  0.35it/s, v_num=3071, train_loss_step=0.160]batch: 423 => Loss: 0.21066541969776154\n",
      "Epoch 0:   8%|███████▋                                                                                      | 424/5214 [20:09<3:47:40,  0.35it/s, v_num=3071, train_loss_step=0.211]batch: 424 => Loss: 0.12716208398342133\n",
      "Epoch 0:   8%|███████▋                                                                                      | 425/5214 [20:12<3:47:37,  0.35it/s, v_num=3071, train_loss_step=0.127]batch: 425 => Loss: 0.27572521567344666\n",
      "Epoch 0:   8%|███████▋                                                                                      | 426/5214 [20:14<3:47:34,  0.35it/s, v_num=3071, train_loss_step=0.276]batch: 426 => Loss: 0.13059678673744202\n",
      "Epoch 0:   8%|███████▋                                                                                      | 427/5214 [20:17<3:47:31,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 427 => Loss: 0.19076108932495117\n",
      "Epoch 0:   8%|███████▋                                                                                      | 428/5214 [20:20<3:47:29,  0.35it/s, v_num=3071, train_loss_step=0.191]batch: 428 => Loss: 0.13588117063045502\n",
      "Epoch 0:   8%|███████▋                                                                                      | 429/5214 [20:23<3:47:26,  0.35it/s, v_num=3071, train_loss_step=0.136]batch: 429 => Loss: 0.16688869893550873\n",
      "Epoch 0:   8%|███████▊                                                                                      | 430/5214 [20:26<3:47:23,  0.35it/s, v_num=3071, train_loss_step=0.167]batch: 430 => Loss: 0.1410314291715622\n",
      "Epoch 0:   8%|███████▊                                                                                      | 431/5214 [20:29<3:47:20,  0.35it/s, v_num=3071, train_loss_step=0.141]batch: 431 => Loss: 0.15708865225315094\n",
      "Epoch 0:   8%|███████▊                                                                                      | 432/5214 [20:32<3:47:17,  0.35it/s, v_num=3071, train_loss_step=0.157]batch: 432 => Loss: 0.15128085017204285\n",
      "Epoch 0:   8%|███████▊                                                                                      | 433/5214 [20:34<3:47:14,  0.35it/s, v_num=3071, train_loss_step=0.151]batch: 433 => Loss: 0.2286142110824585\n",
      "Epoch 0:   8%|███████▊                                                                                      | 434/5214 [20:37<3:47:12,  0.35it/s, v_num=3071, train_loss_step=0.229]batch: 434 => Loss: 0.15026824176311493\n",
      "Epoch 0:   8%|███████▊                                                                                      | 435/5214 [20:40<3:47:09,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 435 => Loss: 0.13307414948940277\n",
      "Epoch 0:   8%|███████▊                                                                                      | 436/5214 [20:43<3:47:06,  0.35it/s, v_num=3071, train_loss_step=0.133]batch: 436 => Loss: 0.16740864515304565\n",
      "Epoch 0:   8%|███████▉                                                                                      | 437/5214 [20:46<3:47:03,  0.35it/s, v_num=3071, train_loss_step=0.167]batch: 437 => Loss: 0.12770268321037292\n",
      "Epoch 0:   8%|███████▉                                                                                      | 438/5214 [20:49<3:47:00,  0.35it/s, v_num=3071, train_loss_step=0.128]batch: 438 => Loss: 0.15172205865383148\n",
      "Epoch 0:   8%|███████▉                                                                                      | 439/5214 [20:52<3:46:58,  0.35it/s, v_num=3071, train_loss_step=0.152]batch: 439 => Loss: 0.15906785428524017\n",
      "Epoch 0:   8%|███████▉                                                                                      | 440/5214 [20:54<3:46:55,  0.35it/s, v_num=3071, train_loss_step=0.159]batch: 440 => Loss: 0.11732053011655807\n",
      "Epoch 0:   8%|███████▉                                                                                      | 441/5214 [20:57<3:46:52,  0.35it/s, v_num=3071, train_loss_step=0.117]batch: 441 => Loss: 0.18794406950473785\n",
      "Epoch 0:   8%|███████▉                                                                                      | 442/5214 [21:00<3:46:49,  0.35it/s, v_num=3071, train_loss_step=0.188]batch: 442 => Loss: 0.15574394166469574\n",
      "Epoch 0:   8%|███████▉                                                                                      | 443/5214 [21:03<3:46:47,  0.35it/s, v_num=3071, train_loss_step=0.156]batch: 443 => Loss: 0.10955389589071274\n",
      "Epoch 0:   9%|████████                                                                                      | 444/5214 [21:06<3:46:44,  0.35it/s, v_num=3071, train_loss_step=0.110]batch: 444 => Loss: 0.20882056653499603\n",
      "Epoch 0:   9%|████████                                                                                      | 445/5214 [21:09<3:46:42,  0.35it/s, v_num=3071, train_loss_step=0.209]batch: 445 => Loss: 0.2301207333803177\n",
      "Epoch 0:   9%|████████                                                                                      | 446/5214 [21:12<3:46:39,  0.35it/s, v_num=3071, train_loss_step=0.230]batch: 446 => Loss: 0.12845173478126526\n",
      "Epoch 0:   9%|████████                                                                                      | 447/5214 [21:14<3:46:36,  0.35it/s, v_num=3071, train_loss_step=0.128]batch: 447 => Loss: 0.26338091492652893\n",
      "Epoch 0:   9%|████████                                                                                      | 448/5214 [21:17<3:46:33,  0.35it/s, v_num=3071, train_loss_step=0.263]batch: 448 => Loss: 0.1515781283378601\n",
      "Epoch 0:   9%|████████                                                                                      | 449/5214 [21:20<3:46:30,  0.35it/s, v_num=3071, train_loss_step=0.152]batch: 449 => Loss: 0.29644718766212463\n",
      "Epoch 0:   9%|████████                                                                                      | 450/5214 [21:23<3:46:28,  0.35it/s, v_num=3071, train_loss_step=0.296]batch: 450 => Loss: 0.1623557060956955\n",
      "Epoch 0:   9%|████████▏                                                                                     | 451/5214 [21:26<3:46:25,  0.35it/s, v_num=3071, train_loss_step=0.162]batch: 451 => Loss: 0.15106070041656494\n",
      "Epoch 0:   9%|████████▏                                                                                     | 452/5214 [21:29<3:46:22,  0.35it/s, v_num=3071, train_loss_step=0.151]batch: 452 => Loss: 0.1675114929676056\n",
      "Epoch 0:   9%|████████▏                                                                                     | 453/5214 [21:32<3:46:20,  0.35it/s, v_num=3071, train_loss_step=0.168]batch: 453 => Loss: 0.15604057908058167\n",
      "Epoch 0:   9%|████████▏                                                                                     | 454/5214 [21:34<3:46:17,  0.35it/s, v_num=3071, train_loss_step=0.156]batch: 454 => Loss: 0.1501527577638626\n",
      "Epoch 0:   9%|████████▏                                                                                     | 455/5214 [21:37<3:46:14,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 455 => Loss: 0.14620231091976166\n",
      "Epoch 0:   9%|████████▏                                                                                     | 456/5214 [21:40<3:46:11,  0.35it/s, v_num=3071, train_loss_step=0.146]batch: 456 => Loss: 0.11809158325195312\n",
      "Epoch 0:   9%|████████▏                                                                                     | 457/5214 [21:43<3:46:08,  0.35it/s, v_num=3071, train_loss_step=0.118]batch: 457 => Loss: 0.22049163281917572\n",
      "Epoch 0:   9%|████████▎                                                                                     | 458/5214 [21:46<3:46:06,  0.35it/s, v_num=3071, train_loss_step=0.220]batch: 458 => Loss: 0.17176203429698944\n",
      "Epoch 0:   9%|████████▎                                                                                     | 459/5214 [21:49<3:46:03,  0.35it/s, v_num=3071, train_loss_step=0.172]batch: 459 => Loss: 0.14424142241477966\n",
      "Epoch 0:   9%|████████▎                                                                                     | 460/5214 [21:52<3:46:00,  0.35it/s, v_num=3071, train_loss_step=0.144]batch: 460 => Loss: 0.16488701105117798\n",
      "Epoch 0:   9%|████████▎                                                                                     | 461/5214 [21:54<3:45:57,  0.35it/s, v_num=3071, train_loss_step=0.165]batch: 461 => Loss: 0.21517756581306458\n",
      "Epoch 0:   9%|████████▎                                                                                     | 462/5214 [21:57<3:45:54,  0.35it/s, v_num=3071, train_loss_step=0.215]batch: 462 => Loss: 0.12229396402835846\n",
      "Epoch 0:   9%|████████▎                                                                                     | 463/5214 [22:00<3:45:51,  0.35it/s, v_num=3071, train_loss_step=0.122]batch: 463 => Loss: 0.2007528394460678\n",
      "Epoch 0:   9%|████████▎                                                                                     | 464/5214 [22:03<3:45:49,  0.35it/s, v_num=3071, train_loss_step=0.201]batch: 464 => Loss: 0.1785934716463089\n",
      "Epoch 0:   9%|████████▍                                                                                     | 465/5214 [22:06<3:45:46,  0.35it/s, v_num=3071, train_loss_step=0.179]batch: 465 => Loss: 0.1862502098083496\n",
      "Epoch 0:   9%|████████▍                                                                                     | 466/5214 [22:09<3:45:43,  0.35it/s, v_num=3071, train_loss_step=0.186]batch: 466 => Loss: 0.12235765904188156\n",
      "Epoch 0:   9%|████████▍                                                                                     | 467/5214 [22:12<3:45:40,  0.35it/s, v_num=3071, train_loss_step=0.122]batch: 467 => Loss: 0.2346935272216797\n",
      "Epoch 0:   9%|████████▍                                                                                     | 468/5214 [22:14<3:45:38,  0.35it/s, v_num=3071, train_loss_step=0.235]batch: 468 => Loss: 0.24777284264564514\n",
      "Epoch 0:   9%|████████▍                                                                                     | 469/5214 [22:17<3:45:35,  0.35it/s, v_num=3071, train_loss_step=0.248]batch: 469 => Loss: 0.13592734932899475\n",
      "Epoch 0:   9%|████████▍                                                                                     | 470/5214 [22:20<3:45:32,  0.35it/s, v_num=3071, train_loss_step=0.136]batch: 470 => Loss: 0.17355157434940338\n",
      "Epoch 0:   9%|████████▍                                                                                     | 471/5214 [22:23<3:45:29,  0.35it/s, v_num=3071, train_loss_step=0.174]batch: 471 => Loss: 0.19560156762599945\n",
      "Epoch 0:   9%|████████▌                                                                                     | 472/5214 [22:26<3:45:26,  0.35it/s, v_num=3071, train_loss_step=0.196]batch: 472 => Loss: 0.16711848974227905\n",
      "Epoch 0:   9%|████████▌                                                                                     | 473/5214 [22:29<3:45:23,  0.35it/s, v_num=3071, train_loss_step=0.167]batch: 473 => Loss: 0.1632031947374344\n",
      "Epoch 0:   9%|████████▌                                                                                     | 474/5214 [22:32<3:45:21,  0.35it/s, v_num=3071, train_loss_step=0.163]batch: 474 => Loss: 0.1641812026500702\n",
      "Epoch 0:   9%|████████▌                                                                                     | 475/5214 [22:34<3:45:18,  0.35it/s, v_num=3071, train_loss_step=0.164]batch: 475 => Loss: 0.13491252064704895\n",
      "Epoch 0:   9%|████████▌                                                                                     | 476/5214 [22:37<3:45:15,  0.35it/s, v_num=3071, train_loss_step=0.135]batch: 476 => Loss: 0.19381853938102722\n",
      "Epoch 0:   9%|████████▌                                                                                     | 477/5214 [22:40<3:45:12,  0.35it/s, v_num=3071, train_loss_step=0.194]batch: 477 => Loss: 0.1087765023112297\n",
      "Epoch 0:   9%|████████▌                                                                                     | 478/5214 [22:43<3:45:09,  0.35it/s, v_num=3071, train_loss_step=0.109]batch: 478 => Loss: 0.18453410267829895\n",
      "Epoch 0:   9%|████████▋                                                                                     | 479/5214 [22:46<3:45:06,  0.35it/s, v_num=3071, train_loss_step=0.185]batch: 479 => Loss: 0.21018271148204803\n",
      "Epoch 0:   9%|████████▋                                                                                     | 480/5214 [22:49<3:45:03,  0.35it/s, v_num=3071, train_loss_step=0.210]batch: 480 => Loss: 0.13044171035289764\n",
      "Epoch 0:   9%|████████▋                                                                                     | 481/5214 [22:52<3:45:00,  0.35it/s, v_num=3071, train_loss_step=0.130]batch: 481 => Loss: 0.23172597587108612\n",
      "Epoch 0:   9%|████████▋                                                                                     | 482/5214 [22:54<3:44:58,  0.35it/s, v_num=3071, train_loss_step=0.232]batch: 482 => Loss: 0.22746257483959198\n",
      "Epoch 0:   9%|████████▋                                                                                     | 483/5214 [22:57<3:44:55,  0.35it/s, v_num=3071, train_loss_step=0.227]batch: 483 => Loss: 0.2216864377260208\n",
      "Epoch 0:   9%|████████▋                                                                                     | 484/5214 [23:00<3:44:52,  0.35it/s, v_num=3071, train_loss_step=0.222]batch: 484 => Loss: 0.15255364775657654\n",
      "Epoch 0:   9%|████████▋                                                                                     | 485/5214 [23:03<3:44:49,  0.35it/s, v_num=3071, train_loss_step=0.153]batch: 485 => Loss: 0.190969318151474\n",
      "Epoch 0:   9%|████████▊                                                                                     | 486/5214 [23:06<3:44:47,  0.35it/s, v_num=3071, train_loss_step=0.191]batch: 486 => Loss: 0.28646236658096313\n",
      "Epoch 0:   9%|████████▊                                                                                     | 487/5214 [23:09<3:44:44,  0.35it/s, v_num=3071, train_loss_step=0.286]batch: 487 => Loss: 0.18250258266925812\n",
      "Epoch 0:   9%|████████▊                                                                                     | 488/5214 [23:12<3:44:41,  0.35it/s, v_num=3071, train_loss_step=0.183]batch: 488 => Loss: 0.27616360783576965\n",
      "Epoch 0:   9%|████████▊                                                                                     | 489/5214 [23:14<3:44:38,  0.35it/s, v_num=3071, train_loss_step=0.276]batch: 489 => Loss: 0.20759502053260803\n",
      "Epoch 0:   9%|████████▊                                                                                     | 490/5214 [23:17<3:44:35,  0.35it/s, v_num=3071, train_loss_step=0.208]batch: 490 => Loss: 0.1543283313512802\n",
      "Epoch 0:   9%|████████▊                                                                                     | 491/5214 [23:20<3:44:33,  0.35it/s, v_num=3071, train_loss_step=0.154]batch: 491 => Loss: 0.1434803307056427\n",
      "Epoch 0:   9%|████████▊                                                                                     | 492/5214 [23:23<3:44:30,  0.35it/s, v_num=3071, train_loss_step=0.143]batch: 492 => Loss: 0.27804601192474365\n",
      "Epoch 0:   9%|████████▉                                                                                     | 493/5214 [23:26<3:44:27,  0.35it/s, v_num=3071, train_loss_step=0.278]batch: 493 => Loss: 0.14196494221687317\n",
      "Epoch 0:   9%|████████▉                                                                                     | 494/5214 [23:29<3:44:24,  0.35it/s, v_num=3071, train_loss_step=0.142]batch: 494 => Loss: 0.10632260143756866\n",
      "Epoch 0:   9%|████████▉                                                                                     | 495/5214 [23:32<3:44:21,  0.35it/s, v_num=3071, train_loss_step=0.106]batch: 495 => Loss: 0.20474545657634735\n",
      "Epoch 0:  10%|████████▉                                                                                     | 496/5214 [23:34<3:44:18,  0.35it/s, v_num=3071, train_loss_step=0.205]batch: 496 => Loss: 0.12696929275989532\n",
      "Epoch 0:  10%|████████▉                                                                                     | 497/5214 [23:37<3:44:15,  0.35it/s, v_num=3071, train_loss_step=0.127]batch: 497 => Loss: 0.10534880310297012\n",
      "Epoch 0:  10%|████████▉                                                                                     | 498/5214 [23:40<3:44:13,  0.35it/s, v_num=3071, train_loss_step=0.105]batch: 498 => Loss: 0.18430151045322418\n",
      "Epoch 0:  10%|████████▉                                                                                     | 499/5214 [23:43<3:44:10,  0.35it/s, v_num=3071, train_loss_step=0.184]batch: 499 => Loss: 0.16576211154460907\n",
      "Epoch 0:  10%|█████████                                                                                     | 500/5214 [23:46<3:44:07,  0.35it/s, v_num=3071, train_loss_step=0.166]batch: 500 => Loss: 0.13149209320545197\n",
      "Epoch 0:  10%|█████████                                                                                     | 501/5214 [23:49<3:44:04,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 501 => Loss: 0.14666642248630524\n",
      "Epoch 0:  10%|█████████                                                                                     | 502/5214 [23:52<3:44:01,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 502 => Loss: 0.19212959706783295\n",
      "Epoch 0:  10%|█████████                                                                                     | 503/5214 [23:54<3:43:59,  0.35it/s, v_num=3071, train_loss_step=0.192]batch: 503 => Loss: 0.15216709673404694\n",
      "Epoch 0:  10%|█████████                                                                                     | 504/5214 [23:57<3:43:56,  0.35it/s, v_num=3071, train_loss_step=0.152]batch: 504 => Loss: 0.15015943348407745\n",
      "Epoch 0:  10%|█████████                                                                                     | 505/5214 [24:00<3:43:53,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 505 => Loss: 0.21293552219867706\n",
      "Epoch 0:  10%|█████████                                                                                     | 506/5214 [24:03<3:43:50,  0.35it/s, v_num=3071, train_loss_step=0.213]batch: 506 => Loss: 0.20841436088085175\n",
      "Epoch 0:  10%|█████████▏                                                                                    | 507/5214 [24:06<3:43:47,  0.35it/s, v_num=3071, train_loss_step=0.208]batch: 507 => Loss: 0.20986001193523407\n",
      "Epoch 0:  10%|█████████▏                                                                                    | 508/5214 [24:09<3:43:45,  0.35it/s, v_num=3071, train_loss_step=0.210]batch: 508 => Loss: 0.20487342774868011\n",
      "Epoch 0:  10%|█████████▏                                                                                    | 509/5214 [24:12<3:43:42,  0.35it/s, v_num=3071, train_loss_step=0.205]batch: 509 => Loss: 0.1791970282793045\n",
      "Epoch 0:  10%|█████████▏                                                                                    | 510/5214 [24:14<3:43:39,  0.35it/s, v_num=3071, train_loss_step=0.179]batch: 510 => Loss: 0.25038570165634155\n",
      "Epoch 0:  10%|█████████▏                                                                                    | 511/5214 [24:17<3:43:37,  0.35it/s, v_num=3071, train_loss_step=0.250]batch: 511 => Loss: 0.2536885440349579\n",
      "Epoch 0:  10%|█████████▏                                                                                    | 512/5214 [24:20<3:43:34,  0.35it/s, v_num=3071, train_loss_step=0.254]batch: 512 => Loss: 0.1557912677526474\n",
      "Epoch 0:  10%|█████████▏                                                                                    | 513/5214 [24:23<3:43:31,  0.35it/s, v_num=3071, train_loss_step=0.156]batch: 513 => Loss: 0.21224401891231537\n",
      "Epoch 0:  10%|█████████▎                                                                                    | 514/5214 [24:26<3:43:28,  0.35it/s, v_num=3071, train_loss_step=0.212]batch: 514 => Loss: 0.1861187070608139\n",
      "Epoch 0:  10%|█████████▎                                                                                    | 515/5214 [24:29<3:43:25,  0.35it/s, v_num=3071, train_loss_step=0.186]batch: 515 => Loss: 0.24277643859386444\n",
      "Epoch 0:  10%|█████████▎                                                                                    | 516/5214 [24:32<3:43:22,  0.35it/s, v_num=3071, train_loss_step=0.243]batch: 516 => Loss: 0.11603605002164841\n",
      "Epoch 0:  10%|█████████▎                                                                                    | 517/5214 [24:34<3:43:19,  0.35it/s, v_num=3071, train_loss_step=0.116]batch: 517 => Loss: 0.13572192192077637\n",
      "Epoch 0:  10%|█████████▎                                                                                    | 518/5214 [24:37<3:43:16,  0.35it/s, v_num=3071, train_loss_step=0.136]batch: 518 => Loss: 0.1394483894109726\n",
      "Epoch 0:  10%|█████████▎                                                                                    | 519/5214 [24:40<3:43:13,  0.35it/s, v_num=3071, train_loss_step=0.139]batch: 519 => Loss: 0.09214567393064499\n",
      "Epoch 0:  10%|█████████▎                                                                                   | 520/5214 [24:43<3:43:10,  0.35it/s, v_num=3071, train_loss_step=0.0921]batch: 520 => Loss: 0.11704862117767334\n",
      "Epoch 0:  10%|█████████▍                                                                                    | 521/5214 [24:46<3:43:07,  0.35it/s, v_num=3071, train_loss_step=0.117]batch: 521 => Loss: 0.18024292588233948\n",
      "Epoch 0:  10%|█████████▍                                                                                    | 522/5214 [24:49<3:43:05,  0.35it/s, v_num=3071, train_loss_step=0.180]batch: 522 => Loss: 0.12196371704339981\n",
      "Epoch 0:  10%|█████████▍                                                                                    | 523/5214 [24:51<3:43:02,  0.35it/s, v_num=3071, train_loss_step=0.122]batch: 523 => Loss: 0.2199702262878418\n",
      "Epoch 0:  10%|█████████▍                                                                                    | 524/5214 [24:54<3:42:59,  0.35it/s, v_num=3071, train_loss_step=0.220]batch: 524 => Loss: 0.15732145309448242\n",
      "Epoch 0:  10%|█████████▍                                                                                    | 525/5214 [24:57<3:42:56,  0.35it/s, v_num=3071, train_loss_step=0.157]batch: 525 => Loss: 0.11835308372974396\n",
      "Epoch 0:  10%|█████████▍                                                                                    | 526/5214 [25:00<3:42:54,  0.35it/s, v_num=3071, train_loss_step=0.118]batch: 526 => Loss: 0.14362286031246185\n",
      "Epoch 0:  10%|█████████▌                                                                                    | 527/5214 [25:03<3:42:51,  0.35it/s, v_num=3071, train_loss_step=0.144]batch: 527 => Loss: 0.2844293713569641\n",
      "Epoch 0:  10%|█████████▌                                                                                    | 528/5214 [25:06<3:42:48,  0.35it/s, v_num=3071, train_loss_step=0.284]batch: 528 => Loss: 0.2401064932346344\n",
      "Epoch 0:  10%|█████████▌                                                                                    | 529/5214 [25:09<3:42:45,  0.35it/s, v_num=3071, train_loss_step=0.240]batch: 529 => Loss: 0.13913366198539734\n",
      "Epoch 0:  10%|█████████▌                                                                                    | 530/5214 [25:12<3:42:43,  0.35it/s, v_num=3071, train_loss_step=0.139]batch: 530 => Loss: 0.13310812413692474\n",
      "Epoch 0:  10%|█████████▌                                                                                    | 531/5214 [25:14<3:42:40,  0.35it/s, v_num=3071, train_loss_step=0.133]batch: 531 => Loss: 0.20086021721363068\n",
      "Epoch 0:  10%|█████████▌                                                                                    | 532/5214 [25:17<3:42:37,  0.35it/s, v_num=3071, train_loss_step=0.201]batch: 532 => Loss: 0.15325649082660675\n",
      "Epoch 0:  10%|█████████▌                                                                                    | 533/5214 [25:20<3:42:34,  0.35it/s, v_num=3071, train_loss_step=0.153]batch: 533 => Loss: 0.12406732887029648\n",
      "Epoch 0:  10%|█████████▋                                                                                    | 534/5214 [25:23<3:42:31,  0.35it/s, v_num=3071, train_loss_step=0.124]batch: 534 => Loss: 0.1835571974515915\n",
      "Epoch 0:  10%|█████████▋                                                                                    | 535/5214 [25:26<3:42:28,  0.35it/s, v_num=3071, train_loss_step=0.184]batch: 535 => Loss: 0.194121316075325\n",
      "Epoch 0:  10%|█████████▋                                                                                    | 536/5214 [25:29<3:42:26,  0.35it/s, v_num=3071, train_loss_step=0.194]batch: 536 => Loss: 0.10131354629993439\n",
      "Epoch 0:  10%|█████████▋                                                                                    | 537/5214 [25:32<3:42:23,  0.35it/s, v_num=3071, train_loss_step=0.101]batch: 537 => Loss: 0.17968125641345978\n",
      "Epoch 0:  10%|█████████▋                                                                                    | 538/5214 [25:34<3:42:20,  0.35it/s, v_num=3071, train_loss_step=0.180]batch: 538 => Loss: 0.25337767601013184\n",
      "Epoch 0:  10%|█████████▋                                                                                    | 539/5214 [25:37<3:42:17,  0.35it/s, v_num=3071, train_loss_step=0.253]batch: 539 => Loss: 0.08968886733055115\n",
      "Epoch 0:  10%|█████████▋                                                                                   | 540/5214 [25:40<3:42:15,  0.35it/s, v_num=3071, train_loss_step=0.0897]batch: 540 => Loss: 0.14492976665496826\n",
      "Epoch 0:  10%|█████████▊                                                                                    | 541/5214 [25:43<3:42:12,  0.35it/s, v_num=3071, train_loss_step=0.145]batch: 541 => Loss: 0.19434073567390442\n",
      "Epoch 0:  10%|█████████▊                                                                                    | 542/5214 [25:46<3:42:09,  0.35it/s, v_num=3071, train_loss_step=0.194]batch: 542 => Loss: 0.12809188663959503\n",
      "Epoch 0:  10%|█████████▊                                                                                    | 543/5214 [25:49<3:42:06,  0.35it/s, v_num=3071, train_loss_step=0.128]batch: 543 => Loss: 0.16677497327327728\n",
      "Epoch 0:  10%|█████████▊                                                                                    | 544/5214 [25:52<3:42:03,  0.35it/s, v_num=3071, train_loss_step=0.167]batch: 544 => Loss: 0.09968677908182144\n",
      "Epoch 0:  10%|█████████▋                                                                                   | 545/5214 [25:54<3:42:00,  0.35it/s, v_num=3071, train_loss_step=0.0997]batch: 545 => Loss: 0.23698551952838898\n",
      "Epoch 0:  10%|█████████▊                                                                                    | 546/5214 [25:57<3:41:58,  0.35it/s, v_num=3071, train_loss_step=0.237]batch: 546 => Loss: 0.183921217918396\n",
      "Epoch 0:  10%|█████████▊                                                                                    | 547/5214 [26:00<3:41:55,  0.35it/s, v_num=3071, train_loss_step=0.184]batch: 547 => Loss: 0.12019913643598557\n",
      "Epoch 0:  11%|█████████▉                                                                                    | 548/5214 [26:03<3:41:52,  0.35it/s, v_num=3071, train_loss_step=0.120]batch: 548 => Loss: 0.17508994042873383\n",
      "Epoch 0:  11%|█████████▉                                                                                    | 549/5214 [26:06<3:41:49,  0.35it/s, v_num=3071, train_loss_step=0.175]batch: 549 => Loss: 0.19269707798957825\n",
      "Epoch 0:  11%|█████████▉                                                                                    | 550/5214 [26:09<3:41:47,  0.35it/s, v_num=3071, train_loss_step=0.193]batch: 550 => Loss: 0.12924841046333313\n",
      "Epoch 0:  11%|█████████▉                                                                                    | 551/5214 [26:12<3:41:44,  0.35it/s, v_num=3071, train_loss_step=0.129]batch: 551 => Loss: 0.2175261527299881\n",
      "Epoch 0:  11%|█████████▉                                                                                    | 552/5214 [26:14<3:41:41,  0.35it/s, v_num=3071, train_loss_step=0.218]batch: 552 => Loss: 0.1155228391289711\n",
      "Epoch 0:  11%|█████████▉                                                                                    | 553/5214 [26:17<3:41:38,  0.35it/s, v_num=3071, train_loss_step=0.116]batch: 553 => Loss: 0.10236399620771408\n",
      "Epoch 0:  11%|█████████▉                                                                                    | 554/5214 [26:20<3:41:35,  0.35it/s, v_num=3071, train_loss_step=0.102]batch: 554 => Loss: 0.13614743947982788\n",
      "Epoch 0:  11%|██████████                                                                                    | 555/5214 [26:23<3:41:32,  0.35it/s, v_num=3071, train_loss_step=0.136]batch: 555 => Loss: 0.2325764149427414\n",
      "Epoch 0:  11%|██████████                                                                                    | 556/5214 [26:26<3:41:30,  0.35it/s, v_num=3071, train_loss_step=0.233]batch: 556 => Loss: 0.1394183486700058\n",
      "Epoch 0:  11%|██████████                                                                                    | 557/5214 [26:29<3:41:27,  0.35it/s, v_num=3071, train_loss_step=0.139]batch: 557 => Loss: 0.13913653790950775\n",
      "Epoch 0:  11%|██████████                                                                                    | 558/5214 [26:32<3:41:24,  0.35it/s, v_num=3071, train_loss_step=0.139]batch: 558 => Loss: 0.3058761656284332\n",
      "Epoch 0:  11%|██████████                                                                                    | 559/5214 [26:34<3:41:21,  0.35it/s, v_num=3071, train_loss_step=0.306]batch: 559 => Loss: 0.28140291571617126\n",
      "Epoch 0:  11%|██████████                                                                                    | 560/5214 [26:37<3:41:18,  0.35it/s, v_num=3071, train_loss_step=0.281]batch: 560 => Loss: 0.1947491466999054\n",
      "Epoch 0:  11%|██████████                                                                                    | 561/5214 [26:40<3:41:16,  0.35it/s, v_num=3071, train_loss_step=0.195]batch: 561 => Loss: 0.12315372377634048\n",
      "Epoch 0:  11%|██████████▏                                                                                   | 562/5214 [26:43<3:41:13,  0.35it/s, v_num=3071, train_loss_step=0.123]batch: 562 => Loss: 0.14821821451187134\n",
      "Epoch 0:  11%|██████████▏                                                                                   | 563/5214 [26:46<3:41:10,  0.35it/s, v_num=3071, train_loss_step=0.148]batch: 563 => Loss: 0.12387275695800781\n",
      "Epoch 0:  11%|██████████▏                                                                                   | 564/5214 [26:49<3:41:07,  0.35it/s, v_num=3071, train_loss_step=0.124]batch: 564 => Loss: 0.2074650079011917\n",
      "Epoch 0:  11%|██████████▏                                                                                   | 565/5214 [26:52<3:41:04,  0.35it/s, v_num=3071, train_loss_step=0.207]batch: 565 => Loss: 0.16740472614765167\n",
      "Epoch 0:  11%|██████████▏                                                                                   | 566/5214 [26:54<3:41:02,  0.35it/s, v_num=3071, train_loss_step=0.167]batch: 566 => Loss: 0.14721563458442688\n",
      "Epoch 0:  11%|██████████▏                                                                                   | 567/5214 [26:57<3:40:59,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 567 => Loss: 0.18899822235107422\n",
      "Epoch 0:  11%|██████████▏                                                                                   | 568/5214 [27:00<3:40:56,  0.35it/s, v_num=3071, train_loss_step=0.189]batch: 568 => Loss: 0.12910065054893494\n",
      "Epoch 0:  11%|██████████▎                                                                                   | 569/5214 [27:03<3:40:53,  0.35it/s, v_num=3071, train_loss_step=0.129]batch: 569 => Loss: 0.1529940366744995\n",
      "Epoch 0:  11%|██████████▎                                                                                   | 570/5214 [27:06<3:40:50,  0.35it/s, v_num=3071, train_loss_step=0.153]batch: 570 => Loss: 0.18470677733421326\n",
      "Epoch 0:  11%|██████████▎                                                                                   | 571/5214 [27:09<3:40:48,  0.35it/s, v_num=3071, train_loss_step=0.185]batch: 571 => Loss: 0.14248071610927582\n",
      "Epoch 0:  11%|██████████▎                                                                                   | 572/5214 [27:12<3:40:45,  0.35it/s, v_num=3071, train_loss_step=0.142]batch: 572 => Loss: 0.09992845356464386\n",
      "Epoch 0:  11%|██████████▏                                                                                  | 573/5214 [27:14<3:40:42,  0.35it/s, v_num=3071, train_loss_step=0.0999]batch: 573 => Loss: 0.10789592564105988\n",
      "Epoch 0:  11%|██████████▎                                                                                   | 574/5214 [27:17<3:40:39,  0.35it/s, v_num=3071, train_loss_step=0.108]batch: 574 => Loss: 0.14420285820960999\n",
      "Epoch 0:  11%|██████████▎                                                                                   | 575/5214 [27:20<3:40:36,  0.35it/s, v_num=3071, train_loss_step=0.144]batch: 575 => Loss: 0.20243366062641144\n",
      "Epoch 0:  11%|██████████▍                                                                                   | 576/5214 [27:23<3:40:33,  0.35it/s, v_num=3071, train_loss_step=0.202]batch: 576 => Loss: 0.26624926924705505\n",
      "Epoch 0:  11%|██████████▍                                                                                   | 577/5214 [27:26<3:40:31,  0.35it/s, v_num=3071, train_loss_step=0.266]batch: 577 => Loss: 0.2122570276260376\n",
      "Epoch 0:  11%|██████████▍                                                                                   | 578/5214 [27:29<3:40:28,  0.35it/s, v_num=3071, train_loss_step=0.212]batch: 578 => Loss: 0.16046682000160217\n",
      "Epoch 0:  11%|██████████▍                                                                                   | 579/5214 [27:32<3:40:25,  0.35it/s, v_num=3071, train_loss_step=0.160]batch: 579 => Loss: 0.3521268367767334\n",
      "Epoch 0:  11%|██████████▍                                                                                   | 580/5214 [27:34<3:40:22,  0.35it/s, v_num=3071, train_loss_step=0.352]batch: 580 => Loss: 0.17292727530002594\n",
      "Epoch 0:  11%|██████████▍                                                                                   | 581/5214 [27:37<3:40:19,  0.35it/s, v_num=3071, train_loss_step=0.173]batch: 581 => Loss: 0.17395856976509094\n",
      "Epoch 0:  11%|██████████▍                                                                                   | 582/5214 [27:40<3:40:16,  0.35it/s, v_num=3071, train_loss_step=0.174]batch: 582 => Loss: 0.16653335094451904\n",
      "Epoch 0:  11%|██████████▌                                                                                   | 583/5214 [27:43<3:40:13,  0.35it/s, v_num=3071, train_loss_step=0.167]batch: 583 => Loss: 0.29137954115867615\n",
      "Epoch 0:  11%|██████████▌                                                                                   | 584/5214 [27:46<3:40:11,  0.35it/s, v_num=3071, train_loss_step=0.291]batch: 584 => Loss: 0.17181146144866943\n",
      "Epoch 0:  11%|██████████▌                                                                                   | 585/5214 [27:49<3:40:08,  0.35it/s, v_num=3071, train_loss_step=0.172]batch: 585 => Loss: 0.16688291728496552\n",
      "Epoch 0:  11%|██████████▌                                                                                   | 586/5214 [27:52<3:40:05,  0.35it/s, v_num=3071, train_loss_step=0.167]batch: 586 => Loss: 0.17007936537265778\n",
      "Epoch 0:  11%|██████████▌                                                                                   | 587/5214 [27:54<3:40:02,  0.35it/s, v_num=3071, train_loss_step=0.170]batch: 587 => Loss: 0.16885370016098022\n",
      "Epoch 0:  11%|██████████▌                                                                                   | 588/5214 [27:57<3:39:59,  0.35it/s, v_num=3071, train_loss_step=0.169]batch: 588 => Loss: 0.12487240135669708\n",
      "Epoch 0:  11%|██████████▌                                                                                   | 589/5214 [28:00<3:39:56,  0.35it/s, v_num=3071, train_loss_step=0.125]batch: 589 => Loss: 0.11449276655912399\n",
      "Epoch 0:  11%|██████████▋                                                                                   | 590/5214 [28:03<3:39:53,  0.35it/s, v_num=3071, train_loss_step=0.114]batch: 590 => Loss: 0.12851698696613312\n",
      "Epoch 0:  11%|██████████▋                                                                                   | 591/5214 [28:06<3:39:50,  0.35it/s, v_num=3071, train_loss_step=0.129]batch: 591 => Loss: 0.18335016071796417\n",
      "Epoch 0:  11%|██████████▋                                                                                   | 592/5214 [28:09<3:39:48,  0.35it/s, v_num=3071, train_loss_step=0.183]batch: 592 => Loss: 0.15021434426307678\n",
      "Epoch 0:  11%|██████████▋                                                                                   | 593/5214 [28:12<3:39:45,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 593 => Loss: 0.17239801585674286\n",
      "Epoch 0:  11%|██████████▋                                                                                   | 594/5214 [28:14<3:39:42,  0.35it/s, v_num=3071, train_loss_step=0.172]batch: 594 => Loss: 0.18334849178791046\n",
      "Epoch 0:  11%|██████████▋                                                                                   | 595/5214 [28:17<3:39:39,  0.35it/s, v_num=3071, train_loss_step=0.183]batch: 595 => Loss: 0.19987520575523376\n",
      "Epoch 0:  11%|██████████▋                                                                                   | 596/5214 [28:20<3:39:36,  0.35it/s, v_num=3071, train_loss_step=0.200]batch: 596 => Loss: 0.12387311458587646\n",
      "Epoch 0:  11%|██████████▊                                                                                   | 597/5214 [28:23<3:39:33,  0.35it/s, v_num=3071, train_loss_step=0.124]batch: 597 => Loss: 0.15883998572826385\n",
      "Epoch 0:  11%|██████████▊                                                                                   | 598/5214 [28:26<3:39:31,  0.35it/s, v_num=3071, train_loss_step=0.159]batch: 598 => Loss: 0.1671876460313797\n",
      "Epoch 0:  11%|██████████▊                                                                                   | 599/5214 [28:29<3:39:28,  0.35it/s, v_num=3071, train_loss_step=0.167]batch: 599 => Loss: 0.13986419141292572\n",
      "Epoch 0:  12%|██████████▊                                                                                   | 600/5214 [28:32<3:39:25,  0.35it/s, v_num=3071, train_loss_step=0.140]batch: 600 => Loss: 0.13879351317882538\n",
      "Epoch 0:  12%|██████████▊                                                                                   | 601/5214 [28:34<3:39:22,  0.35it/s, v_num=3071, train_loss_step=0.139]batch: 601 => Loss: 0.216165229678154\n",
      "Epoch 0:  12%|██████████▊                                                                                   | 602/5214 [28:37<3:39:19,  0.35it/s, v_num=3071, train_loss_step=0.216]batch: 602 => Loss: 0.1476309448480606\n",
      "Epoch 0:  12%|██████████▊                                                                                   | 603/5214 [28:40<3:39:16,  0.35it/s, v_num=3071, train_loss_step=0.148]batch: 603 => Loss: 0.1465139240026474\n",
      "Epoch 0:  12%|██████████▉                                                                                   | 604/5214 [28:43<3:39:13,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 604 => Loss: 0.16133201122283936\n",
      "Epoch 0:  12%|██████████▉                                                                                   | 605/5214 [28:46<3:39:11,  0.35it/s, v_num=3071, train_loss_step=0.161]batch: 605 => Loss: 0.17245768010616302\n",
      "Epoch 0:  12%|██████████▉                                                                                   | 606/5214 [28:49<3:39:08,  0.35it/s, v_num=3071, train_loss_step=0.172]batch: 606 => Loss: 0.16897307336330414\n",
      "Epoch 0:  12%|██████████▉                                                                                   | 607/5214 [28:51<3:39:05,  0.35it/s, v_num=3071, train_loss_step=0.169]batch: 607 => Loss: 0.13691715896129608\n",
      "Epoch 0:  12%|██████████▉                                                                                   | 608/5214 [28:54<3:39:02,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 608 => Loss: 0.18258832395076752\n",
      "Epoch 0:  12%|██████████▉                                                                                   | 609/5214 [28:57<3:38:59,  0.35it/s, v_num=3071, train_loss_step=0.183]batch: 609 => Loss: 0.13869285583496094\n",
      "Epoch 0:  12%|██████████▉                                                                                   | 610/5214 [29:00<3:38:56,  0.35it/s, v_num=3071, train_loss_step=0.139]batch: 610 => Loss: 0.1511375606060028\n",
      "Epoch 0:  12%|███████████                                                                                   | 611/5214 [29:03<3:38:53,  0.35it/s, v_num=3071, train_loss_step=0.151]batch: 611 => Loss: 0.17254990339279175\n",
      "Epoch 0:  12%|███████████                                                                                   | 612/5214 [29:06<3:38:51,  0.35it/s, v_num=3071, train_loss_step=0.173]batch: 612 => Loss: 0.2086472064256668\n",
      "Epoch 0:  12%|███████████                                                                                   | 613/5214 [29:09<3:38:48,  0.35it/s, v_num=3071, train_loss_step=0.209]batch: 613 => Loss: 0.16969846189022064\n",
      "Epoch 0:  12%|███████████                                                                                   | 614/5214 [29:11<3:38:45,  0.35it/s, v_num=3071, train_loss_step=0.170]batch: 614 => Loss: 0.25028952956199646\n",
      "Epoch 0:  12%|███████████                                                                                   | 615/5214 [29:14<3:38:42,  0.35it/s, v_num=3071, train_loss_step=0.250]batch: 615 => Loss: 0.22000575065612793\n",
      "Epoch 0:  12%|███████████                                                                                   | 616/5214 [29:17<3:38:39,  0.35it/s, v_num=3071, train_loss_step=0.220]batch: 616 => Loss: 0.12007005512714386\n",
      "Epoch 0:  12%|███████████                                                                                   | 617/5214 [29:20<3:38:37,  0.35it/s, v_num=3071, train_loss_step=0.120]batch: 617 => Loss: 0.2174696922302246\n",
      "Epoch 0:  12%|███████████▏                                                                                  | 618/5214 [29:23<3:38:34,  0.35it/s, v_num=3071, train_loss_step=0.217]batch: 618 => Loss: 0.2786470055580139\n",
      "Epoch 0:  12%|███████████▏                                                                                  | 619/5214 [29:26<3:38:31,  0.35it/s, v_num=3071, train_loss_step=0.279]batch: 619 => Loss: 0.19855311512947083\n",
      "Epoch 0:  12%|███████████▏                                                                                  | 620/5214 [29:29<3:38:28,  0.35it/s, v_num=3071, train_loss_step=0.199]batch: 620 => Loss: 0.1274872124195099\n",
      "Epoch 0:  12%|███████████▏                                                                                  | 621/5214 [29:31<3:38:25,  0.35it/s, v_num=3071, train_loss_step=0.127]batch: 621 => Loss: 0.18876232206821442\n",
      "Epoch 0:  12%|███████████▏                                                                                  | 622/5214 [29:34<3:38:22,  0.35it/s, v_num=3071, train_loss_step=0.189]batch: 622 => Loss: 0.18155495822429657\n",
      "Epoch 0:  12%|███████████▏                                                                                  | 623/5214 [29:37<3:38:20,  0.35it/s, v_num=3071, train_loss_step=0.182]batch: 623 => Loss: 0.18215754628181458\n",
      "Epoch 0:  12%|███████████▏                                                                                  | 624/5214 [29:40<3:38:17,  0.35it/s, v_num=3071, train_loss_step=0.182]batch: 624 => Loss: 0.14399759471416473\n",
      "Epoch 0:  12%|███████████▎                                                                                  | 625/5214 [29:43<3:38:14,  0.35it/s, v_num=3071, train_loss_step=0.144]batch: 625 => Loss: 0.17949770390987396\n",
      "Epoch 0:  12%|███████████▎                                                                                  | 626/5214 [29:46<3:38:11,  0.35it/s, v_num=3071, train_loss_step=0.179]batch: 626 => Loss: 0.16057366132736206\n",
      "Epoch 0:  12%|███████████▎                                                                                  | 627/5214 [29:49<3:38:08,  0.35it/s, v_num=3071, train_loss_step=0.161]batch: 627 => Loss: 0.08917476236820221\n",
      "Epoch 0:  12%|███████████▏                                                                                 | 628/5214 [29:51<3:38:06,  0.35it/s, v_num=3071, train_loss_step=0.0892]batch: 628 => Loss: 0.15744267404079437\n",
      "Epoch 0:  12%|███████████▎                                                                                  | 629/5214 [29:54<3:38:03,  0.35it/s, v_num=3071, train_loss_step=0.157]batch: 629 => Loss: 0.12235050648450851\n",
      "Epoch 0:  12%|███████████▎                                                                                  | 630/5214 [29:57<3:38:00,  0.35it/s, v_num=3071, train_loss_step=0.122]batch: 630 => Loss: 0.19797949492931366\n",
      "Epoch 0:  12%|███████████▍                                                                                  | 631/5214 [30:00<3:37:57,  0.35it/s, v_num=3071, train_loss_step=0.198]batch: 631 => Loss: 0.12339110672473907\n",
      "Epoch 0:  12%|███████████▍                                                                                  | 632/5214 [30:03<3:37:54,  0.35it/s, v_num=3071, train_loss_step=0.123]batch: 632 => Loss: 0.1073639765381813\n",
      "Epoch 0:  12%|███████████▍                                                                                  | 633/5214 [30:06<3:37:52,  0.35it/s, v_num=3071, train_loss_step=0.107]batch: 633 => Loss: 0.19378598034381866\n",
      "Epoch 0:  12%|███████████▍                                                                                  | 634/5214 [30:09<3:37:49,  0.35it/s, v_num=3071, train_loss_step=0.194]batch: 634 => Loss: 0.1525358259677887\n",
      "Epoch 0:  12%|███████████▍                                                                                  | 635/5214 [30:12<3:37:46,  0.35it/s, v_num=3071, train_loss_step=0.153]batch: 635 => Loss: 0.1094251498579979\n",
      "Epoch 0:  12%|███████████▍                                                                                  | 636/5214 [30:14<3:37:43,  0.35it/s, v_num=3071, train_loss_step=0.109]batch: 636 => Loss: 0.18009935319423676\n",
      "Epoch 0:  12%|███████████▍                                                                                  | 637/5214 [30:17<3:37:41,  0.35it/s, v_num=3071, train_loss_step=0.180]batch: 637 => Loss: 0.17434686422348022\n",
      "Epoch 0:  12%|███████████▌                                                                                  | 638/5214 [30:20<3:37:38,  0.35it/s, v_num=3071, train_loss_step=0.174]batch: 638 => Loss: 0.15503591299057007\n",
      "Epoch 0:  12%|███████████▌                                                                                  | 639/5214 [30:23<3:37:35,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 639 => Loss: 0.14094270765781403\n",
      "Epoch 0:  12%|███████████▌                                                                                  | 640/5214 [30:26<3:37:32,  0.35it/s, v_num=3071, train_loss_step=0.141]batch: 640 => Loss: 0.15005896985530853\n",
      "Epoch 0:  12%|███████████▌                                                                                  | 641/5214 [30:29<3:37:29,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 641 => Loss: 0.19097284972667694\n",
      "Epoch 0:  12%|███████████▌                                                                                  | 642/5214 [30:32<3:37:26,  0.35it/s, v_num=3071, train_loss_step=0.191]batch: 642 => Loss: 0.10807635635137558\n",
      "Epoch 0:  12%|███████████▌                                                                                  | 643/5214 [30:34<3:37:23,  0.35it/s, v_num=3071, train_loss_step=0.108]batch: 643 => Loss: 0.3310411274433136\n",
      "Epoch 0:  12%|███████████▌                                                                                  | 644/5214 [30:37<3:37:20,  0.35it/s, v_num=3071, train_loss_step=0.331]batch: 644 => Loss: 0.13242149353027344\n",
      "Epoch 0:  12%|███████████▋                                                                                  | 645/5214 [30:40<3:37:18,  0.35it/s, v_num=3071, train_loss_step=0.132]batch: 645 => Loss: 0.2241886407136917\n",
      "Epoch 0:  12%|███████████▋                                                                                  | 646/5214 [30:43<3:37:15,  0.35it/s, v_num=3071, train_loss_step=0.224]batch: 646 => Loss: 0.17017535865306854\n",
      "Epoch 0:  12%|███████████▋                                                                                  | 647/5214 [30:46<3:37:12,  0.35it/s, v_num=3071, train_loss_step=0.170]batch: 647 => Loss: 0.14726150035858154\n",
      "Epoch 0:  12%|███████████▋                                                                                  | 648/5214 [30:49<3:37:09,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 648 => Loss: 0.13091842830181122\n",
      "Epoch 0:  12%|███████████▋                                                                                  | 649/5214 [30:52<3:37:06,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 649 => Loss: 0.18343690037727356\n",
      "Epoch 0:  12%|███████████▋                                                                                  | 650/5214 [30:54<3:37:04,  0.35it/s, v_num=3071, train_loss_step=0.183]batch: 650 => Loss: 0.17551259696483612\n",
      "Epoch 0:  12%|███████████▋                                                                                  | 651/5214 [30:57<3:37:01,  0.35it/s, v_num=3071, train_loss_step=0.176]batch: 651 => Loss: 0.2117566168308258\n",
      "Epoch 0:  13%|███████████▊                                                                                  | 652/5214 [31:00<3:36:58,  0.35it/s, v_num=3071, train_loss_step=0.212]batch: 652 => Loss: 0.10669224709272385\n",
      "Epoch 0:  13%|███████████▊                                                                                  | 653/5214 [31:03<3:36:55,  0.35it/s, v_num=3071, train_loss_step=0.107]batch: 653 => Loss: 0.17646874487400055\n",
      "Epoch 0:  13%|███████████▊                                                                                  | 654/5214 [31:06<3:36:52,  0.35it/s, v_num=3071, train_loss_step=0.176]batch: 654 => Loss: 0.19124308228492737\n",
      "Epoch 0:  13%|███████████▊                                                                                  | 655/5214 [31:09<3:36:49,  0.35it/s, v_num=3071, train_loss_step=0.191]batch: 655 => Loss: 0.14729532599449158\n",
      "Epoch 0:  13%|███████████▊                                                                                  | 656/5214 [31:12<3:36:47,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 656 => Loss: 0.14932410418987274\n",
      "Epoch 0:  13%|███████████▊                                                                                  | 657/5214 [31:14<3:36:44,  0.35it/s, v_num=3071, train_loss_step=0.149]batch: 657 => Loss: 0.11192947626113892\n",
      "Epoch 0:  13%|███████████▊                                                                                  | 658/5214 [31:17<3:36:41,  0.35it/s, v_num=3071, train_loss_step=0.112]batch: 658 => Loss: 0.18035821616649628\n",
      "Epoch 0:  13%|███████████▉                                                                                  | 659/5214 [31:20<3:36:38,  0.35it/s, v_num=3071, train_loss_step=0.180]batch: 659 => Loss: 0.10797403007745743\n",
      "Epoch 0:  13%|███████████▉                                                                                  | 660/5214 [31:23<3:36:35,  0.35it/s, v_num=3071, train_loss_step=0.108]batch: 660 => Loss: 0.18539635837078094\n",
      "Epoch 0:  13%|███████████▉                                                                                  | 661/5214 [31:26<3:36:33,  0.35it/s, v_num=3071, train_loss_step=0.185]batch: 661 => Loss: 0.2523307502269745\n",
      "Epoch 0:  13%|███████████▉                                                                                  | 662/5214 [31:29<3:36:30,  0.35it/s, v_num=3071, train_loss_step=0.252]batch: 662 => Loss: 0.16240982711315155\n",
      "Epoch 0:  13%|███████████▉                                                                                  | 663/5214 [31:32<3:36:27,  0.35it/s, v_num=3071, train_loss_step=0.162]batch: 663 => Loss: 0.13965144753456116\n",
      "Epoch 0:  13%|███████████▉                                                                                  | 664/5214 [31:34<3:36:24,  0.35it/s, v_num=3071, train_loss_step=0.140]batch: 664 => Loss: 0.2144371122121811\n",
      "Epoch 0:  13%|███████████▉                                                                                  | 665/5214 [31:37<3:36:21,  0.35it/s, v_num=3071, train_loss_step=0.214]batch: 665 => Loss: 0.1519637405872345\n",
      "Epoch 0:  13%|████████████                                                                                  | 666/5214 [31:40<3:36:19,  0.35it/s, v_num=3071, train_loss_step=0.152]batch: 666 => Loss: 0.2357577085494995\n",
      "Epoch 0:  13%|████████████                                                                                  | 667/5214 [31:43<3:36:16,  0.35it/s, v_num=3071, train_loss_step=0.236]batch: 667 => Loss: 0.14990368485450745\n",
      "Epoch 0:  13%|████████████                                                                                  | 668/5214 [31:46<3:36:14,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 668 => Loss: 0.18248538672924042\n",
      "Epoch 0:  13%|████████████                                                                                  | 669/5214 [31:49<3:36:11,  0.35it/s, v_num=3071, train_loss_step=0.182]batch: 669 => Loss: 0.17423315346240997\n",
      "Epoch 0:  13%|████████████                                                                                  | 670/5214 [31:52<3:36:08,  0.35it/s, v_num=3071, train_loss_step=0.174]batch: 670 => Loss: 0.16367562115192413\n",
      "Epoch 0:  13%|████████████                                                                                  | 671/5214 [31:55<3:36:05,  0.35it/s, v_num=3071, train_loss_step=0.164]batch: 671 => Loss: 0.1333743929862976\n",
      "Epoch 0:  13%|████████████                                                                                  | 672/5214 [31:57<3:36:03,  0.35it/s, v_num=3071, train_loss_step=0.133]batch: 672 => Loss: 0.12102396786212921\n",
      "Epoch 0:  13%|████████████▏                                                                                 | 673/5214 [32:00<3:36:00,  0.35it/s, v_num=3071, train_loss_step=0.121]batch: 673 => Loss: 0.1382315307855606\n",
      "Epoch 0:  13%|████████████▏                                                                                 | 674/5214 [32:03<3:35:57,  0.35it/s, v_num=3071, train_loss_step=0.138]batch: 674 => Loss: 0.16977918148040771\n",
      "Epoch 0:  13%|████████████▏                                                                                 | 675/5214 [32:06<3:35:54,  0.35it/s, v_num=3071, train_loss_step=0.170]batch: 675 => Loss: 0.1392083764076233\n",
      "Epoch 0:  13%|████████████▏                                                                                 | 676/5214 [32:09<3:35:51,  0.35it/s, v_num=3071, train_loss_step=0.139]batch: 676 => Loss: 0.16002725064754486\n",
      "Epoch 0:  13%|████████████▏                                                                                 | 677/5214 [32:12<3:35:48,  0.35it/s, v_num=3071, train_loss_step=0.160]batch: 677 => Loss: 0.1168542131781578\n",
      "Epoch 0:  13%|████████████▏                                                                                 | 678/5214 [32:15<3:35:45,  0.35it/s, v_num=3071, train_loss_step=0.117]batch: 678 => Loss: 0.19197070598602295\n",
      "Epoch 0:  13%|████████████▏                                                                                 | 679/5214 [32:17<3:35:42,  0.35it/s, v_num=3071, train_loss_step=0.192]batch: 679 => Loss: 0.13131029903888702\n",
      "Epoch 0:  13%|████████████▎                                                                                 | 680/5214 [32:20<3:35:39,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 680 => Loss: 0.11517196893692017\n",
      "Epoch 0:  13%|████████████▎                                                                                 | 681/5214 [32:23<3:35:37,  0.35it/s, v_num=3071, train_loss_step=0.115]batch: 681 => Loss: 0.08929946273565292\n",
      "Epoch 0:  13%|████████████▏                                                                                | 682/5214 [32:26<3:35:34,  0.35it/s, v_num=3071, train_loss_step=0.0893]batch: 682 => Loss: 0.14317820966243744\n",
      "Epoch 0:  13%|████████████▎                                                                                 | 683/5214 [32:29<3:35:31,  0.35it/s, v_num=3071, train_loss_step=0.143]batch: 683 => Loss: 0.10461073368787766\n",
      "Epoch 0:  13%|████████████▎                                                                                 | 684/5214 [32:32<3:35:28,  0.35it/s, v_num=3071, train_loss_step=0.105]batch: 684 => Loss: 0.0807972401380539\n",
      "Epoch 0:  13%|████████████▏                                                                                | 685/5214 [32:35<3:35:26,  0.35it/s, v_num=3071, train_loss_step=0.0808]batch: 685 => Loss: 0.15402093529701233\n",
      "Epoch 0:  13%|████████████▎                                                                                 | 686/5214 [32:37<3:35:23,  0.35it/s, v_num=3071, train_loss_step=0.154]batch: 686 => Loss: 0.11920955032110214\n",
      "Epoch 0:  13%|████████████▍                                                                                 | 687/5214 [32:40<3:35:20,  0.35it/s, v_num=3071, train_loss_step=0.119]batch: 687 => Loss: 0.11791817098855972\n",
      "Epoch 0:  13%|████████████▍                                                                                 | 688/5214 [32:43<3:35:17,  0.35it/s, v_num=3071, train_loss_step=0.118]batch: 688 => Loss: 0.11426388472318649\n",
      "Epoch 0:  13%|████████████▍                                                                                 | 689/5214 [32:46<3:35:14,  0.35it/s, v_num=3071, train_loss_step=0.114]batch: 689 => Loss: 0.11687061935663223\n",
      "Epoch 0:  13%|████████████▍                                                                                 | 690/5214 [32:49<3:35:11,  0.35it/s, v_num=3071, train_loss_step=0.117]batch: 690 => Loss: 0.173364520072937\n",
      "Epoch 0:  13%|████████████▍                                                                                 | 691/5214 [32:52<3:35:08,  0.35it/s, v_num=3071, train_loss_step=0.173]batch: 691 => Loss: 0.2125617116689682\n",
      "Epoch 0:  13%|████████████▍                                                                                 | 692/5214 [32:54<3:35:05,  0.35it/s, v_num=3071, train_loss_step=0.213]batch: 692 => Loss: 0.16879715025424957\n",
      "Epoch 0:  13%|████████████▍                                                                                 | 693/5214 [32:57<3:35:02,  0.35it/s, v_num=3071, train_loss_step=0.169]batch: 693 => Loss: 0.17130468785762787\n",
      "Epoch 0:  13%|████████████▌                                                                                 | 694/5214 [33:00<3:35:00,  0.35it/s, v_num=3071, train_loss_step=0.171]batch: 694 => Loss: 0.1587425023317337\n",
      "Epoch 0:  13%|████████████▌                                                                                 | 695/5214 [33:03<3:34:57,  0.35it/s, v_num=3071, train_loss_step=0.159]batch: 695 => Loss: 0.15515437722206116\n",
      "Epoch 0:  13%|████████████▌                                                                                 | 696/5214 [33:06<3:34:54,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 696 => Loss: 0.15970556437969208\n",
      "Epoch 0:  13%|████████████▌                                                                                 | 697/5214 [33:09<3:34:51,  0.35it/s, v_num=3071, train_loss_step=0.160]batch: 697 => Loss: 0.17137734591960907\n",
      "Epoch 0:  13%|████████████▌                                                                                 | 698/5214 [33:12<3:34:48,  0.35it/s, v_num=3071, train_loss_step=0.171]batch: 698 => Loss: 0.1323026567697525\n",
      "Epoch 0:  13%|████████████▌                                                                                 | 699/5214 [33:14<3:34:45,  0.35it/s, v_num=3071, train_loss_step=0.132]batch: 699 => Loss: 0.13706396520137787\n",
      "Epoch 0:  13%|████████████▌                                                                                 | 700/5214 [33:17<3:34:42,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 700 => Loss: 0.19032153487205505\n",
      "Epoch 0:  13%|████████████▋                                                                                 | 701/5214 [33:20<3:34:39,  0.35it/s, v_num=3071, train_loss_step=0.190]batch: 701 => Loss: 0.17260603606700897\n",
      "Epoch 0:  13%|████████████▋                                                                                 | 702/5214 [33:23<3:34:36,  0.35it/s, v_num=3071, train_loss_step=0.173]batch: 702 => Loss: 0.16119126975536346\n",
      "Epoch 0:  13%|████████████▋                                                                                 | 703/5214 [33:26<3:34:34,  0.35it/s, v_num=3071, train_loss_step=0.161]batch: 703 => Loss: 0.3329653739929199\n",
      "Epoch 0:  14%|████████████▋                                                                                 | 704/5214 [33:29<3:34:31,  0.35it/s, v_num=3071, train_loss_step=0.333]batch: 704 => Loss: 0.2097032070159912\n",
      "Epoch 0:  14%|████████████▋                                                                                 | 705/5214 [33:32<3:34:28,  0.35it/s, v_num=3071, train_loss_step=0.210]batch: 705 => Loss: 0.14111477136611938\n",
      "Epoch 0:  14%|████████████▋                                                                                 | 706/5214 [33:34<3:34:25,  0.35it/s, v_num=3071, train_loss_step=0.141]batch: 706 => Loss: 0.17353276908397675\n",
      "Epoch 0:  14%|████████████▋                                                                                 | 707/5214 [33:37<3:34:23,  0.35it/s, v_num=3071, train_loss_step=0.174]batch: 707 => Loss: 0.13341867923736572\n",
      "Epoch 0:  14%|████████████▊                                                                                 | 708/5214 [33:40<3:34:20,  0.35it/s, v_num=3071, train_loss_step=0.133]batch: 708 => Loss: 0.22035737335681915\n",
      "Epoch 0:  14%|████████████▊                                                                                 | 709/5214 [33:43<3:34:17,  0.35it/s, v_num=3071, train_loss_step=0.220]batch: 709 => Loss: 0.14697317779064178\n",
      "Epoch 0:  14%|████████████▊                                                                                 | 710/5214 [33:46<3:34:14,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 710 => Loss: 0.15179362893104553\n",
      "Epoch 0:  14%|████████████▊                                                                                 | 711/5214 [33:49<3:34:11,  0.35it/s, v_num=3071, train_loss_step=0.152]batch: 711 => Loss: 0.2084035873413086\n",
      "Epoch 0:  14%|████████████▊                                                                                 | 712/5214 [33:52<3:34:09,  0.35it/s, v_num=3071, train_loss_step=0.208]batch: 712 => Loss: 0.18281958997249603\n",
      "Epoch 0:  14%|████████████▊                                                                                 | 713/5214 [33:54<3:34:06,  0.35it/s, v_num=3071, train_loss_step=0.183]batch: 713 => Loss: 0.30315032601356506\n",
      "Epoch 0:  14%|████████████▊                                                                                 | 714/5214 [33:57<3:34:03,  0.35it/s, v_num=3071, train_loss_step=0.303]batch: 714 => Loss: 0.16933928430080414\n",
      "Epoch 0:  14%|████████████▉                                                                                 | 715/5214 [34:00<3:34:00,  0.35it/s, v_num=3071, train_loss_step=0.169]batch: 715 => Loss: 0.13706786930561066\n",
      "Epoch 0:  14%|████████████▉                                                                                 | 716/5214 [34:03<3:33:57,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 716 => Loss: 0.30638012290000916\n",
      "Epoch 0:  14%|████████████▉                                                                                 | 717/5214 [34:06<3:33:54,  0.35it/s, v_num=3071, train_loss_step=0.306]batch: 717 => Loss: 0.12952224910259247\n",
      "Epoch 0:  14%|████████████▉                                                                                 | 718/5214 [34:09<3:33:52,  0.35it/s, v_num=3071, train_loss_step=0.130]batch: 718 => Loss: 0.15691423416137695\n",
      "Epoch 0:  14%|████████████▉                                                                                 | 719/5214 [34:12<3:33:49,  0.35it/s, v_num=3071, train_loss_step=0.157]batch: 719 => Loss: 0.11159797012805939\n",
      "Epoch 0:  14%|████████████▉                                                                                 | 720/5214 [34:14<3:33:46,  0.35it/s, v_num=3071, train_loss_step=0.112]batch: 720 => Loss: 0.21333633363246918\n",
      "Epoch 0:  14%|████████████▉                                                                                 | 721/5214 [34:17<3:33:43,  0.35it/s, v_num=3071, train_loss_step=0.213]batch: 721 => Loss: 0.13954906165599823\n",
      "Epoch 0:  14%|█████████████                                                                                 | 722/5214 [34:20<3:33:40,  0.35it/s, v_num=3071, train_loss_step=0.140]batch: 722 => Loss: 0.27996665239334106\n",
      "Epoch 0:  14%|█████████████                                                                                 | 723/5214 [34:23<3:33:37,  0.35it/s, v_num=3071, train_loss_step=0.280]batch: 723 => Loss: 0.15485158562660217\n",
      "Epoch 0:  14%|█████████████                                                                                 | 724/5214 [34:26<3:33:35,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 724 => Loss: 0.11554361879825592\n",
      "Epoch 0:  14%|█████████████                                                                                 | 725/5214 [34:29<3:33:32,  0.35it/s, v_num=3071, train_loss_step=0.116]batch: 725 => Loss: 0.2696356475353241\n",
      "Epoch 0:  14%|█████████████                                                                                 | 726/5214 [34:32<3:33:29,  0.35it/s, v_num=3071, train_loss_step=0.270]batch: 726 => Loss: 0.17242677509784698\n",
      "Epoch 0:  14%|█████████████                                                                                 | 727/5214 [34:34<3:33:26,  0.35it/s, v_num=3071, train_loss_step=0.172]batch: 727 => Loss: 0.12020862102508545batch: 764 => Loss: 0.21028466522693634\n",
      "Epoch 0:  15%|█████████████▊                                                                                | 765/5214 [36:23<3:31:38,  0.35it/s, v_num=3071, train_loss_step=0.210]batch: 765 => Loss: 0.16535203158855438\n",
      "Epoch 0:  15%|█████████████▊                                                                                | 766/5214 [36:26<3:31:35,  0.35it/s, v_num=3071, train_loss_step=0.165]batch: 766 => Loss: 0.1764470785856247\n",
      "Epoch 0:  15%|█████████████▊                                                                                | 767/5214 [36:29<3:31:32,  0.35it/s, v_num=3071, train_loss_step=0.176]batch: 767 => Loss: 0.15110436081886292\n",
      "Epoch 0:  15%|█████████████▊                                                                                | 768/5214 [36:31<3:31:29,  0.35it/s, v_num=3071, train_loss_step=0.151]batch: 768 => Loss: 0.22609487175941467\n",
      "Epoch 0:  15%|█████████████▊                                                                                | 769/5214 [36:34<3:31:26,  0.35it/s, v_num=3071, train_loss_step=0.226]batch: 769 => Loss: 0.12960951030254364\n",
      "Epoch 0:  15%|█████████████▉                                                                                | 770/5214 [36:37<3:31:23,  0.35it/s, v_num=3071, train_loss_step=0.130]batch: 770 => Loss: 0.15095210075378418\n",
      "Epoch 0:  15%|█████████████▉                                                                                | 771/5214 [36:40<3:31:20,  0.35it/s, v_num=3071, train_loss_step=0.151]batch: 771 => Loss: 0.13194110989570618\n",
      "Epoch 0:  15%|█████████████▉                                                                                | 772/5214 [36:43<3:31:17,  0.35it/s, v_num=3071, train_loss_step=0.132]batch: 772 => Loss: 0.09937434643507004\n",
      "Epoch 0:  15%|█████████████▊                                                                               | 773/5214 [36:46<3:31:14,  0.35it/s, v_num=3071, train_loss_step=0.0994]batch: 773 => Loss: 0.17731289565563202\n",
      "Epoch 0:  15%|█████████████▉                                                                                | 774/5214 [36:49<3:31:12,  0.35it/s, v_num=3071, train_loss_step=0.177]batch: 774 => Loss: 0.13571707904338837\n",
      "Epoch 0:  15%|█████████████▉                                                                                | 775/5214 [36:51<3:31:09,  0.35it/s, v_num=3071, train_loss_step=0.136]batch: 775 => Loss: 0.16224044561386108\n",
      "Epoch 0:  15%|█████████████▉                                                                                | 776/5214 [36:54<3:31:06,  0.35it/s, v_num=3071, train_loss_step=0.162]batch: 776 => Loss: 0.1685451716184616\n",
      "Epoch 0:  15%|██████████████                                                                                | 777/5214 [36:57<3:31:03,  0.35it/s, v_num=3071, train_loss_step=0.169]batch: 777 => Loss: 0.1555328071117401\n",
      "Epoch 0:  15%|██████████████                                                                                | 778/5214 [37:00<3:31:00,  0.35it/s, v_num=3071, train_loss_step=0.156]batch: 778 => Loss: 0.14136965572834015\n",
      "Epoch 0:  15%|██████████████                                                                                | 779/5214 [37:03<3:30:57,  0.35it/s, v_num=3071, train_loss_step=0.141]batch: 779 => Loss: 0.14623582363128662\n",
      "Epoch 0:  15%|██████████████                                                                                | 780/5214 [37:06<3:30:54,  0.35it/s, v_num=3071, train_loss_step=0.146]batch: 780 => Loss: 0.2068255990743637\n",
      "Epoch 0:  15%|██████████████                                                                                | 781/5214 [37:09<3:30:51,  0.35it/s, v_num=3071, train_loss_step=0.207]batch: 781 => Loss: 0.1483774334192276\n",
      "Epoch 0:  15%|██████████████                                                                                | 782/5214 [37:11<3:30:49,  0.35it/s, v_num=3071, train_loss_step=0.148]batch: 782 => Loss: 0.147662952542305\n",
      "Epoch 0:  15%|██████████████                                                                                | 783/5214 [37:14<3:30:46,  0.35it/s, v_num=3071, train_loss_step=0.148]batch: 783 => Loss: 0.13413284718990326\n",
      "Epoch 0:  15%|██████████████▏                                                                               | 784/5214 [37:17<3:30:43,  0.35it/s, v_num=3071, train_loss_step=0.134]batch: 784 => Loss: 0.1101578027009964\n",
      "Epoch 0:  15%|██████████████▏                                                                               | 785/5214 [37:20<3:30:40,  0.35it/s, v_num=3071, train_loss_step=0.110]batch: 785 => Loss: 0.15741148591041565\n",
      "Epoch 0:  15%|██████████████▏                                                                               | 786/5214 [37:23<3:30:37,  0.35it/s, v_num=3071, train_loss_step=0.157]batch: 786 => Loss: 0.22582419216632843\n",
      "Epoch 0:  15%|██████████████▏                                                                               | 787/5214 [37:26<3:30:34,  0.35it/s, v_num=3071, train_loss_step=0.226]batch: 787 => Loss: 0.19113551080226898\n",
      "Epoch 0:  15%|██████████████▏                                                                               | 788/5214 [37:28<3:30:31,  0.35it/s, v_num=3071, train_loss_step=0.191]batch: 788 => Loss: 0.19039718806743622\n",
      "Epoch 0:  15%|██████████████▏                                                                               | 789/5214 [37:31<3:30:28,  0.35it/s, v_num=3071, train_loss_step=0.190]batch: 789 => Loss: 0.15538088977336884\n",
      "Epoch 0:  15%|██████████████▏                                                                               | 790/5214 [37:34<3:30:26,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 790 => Loss: 0.15820780396461487\n",
      "Epoch 0:  15%|██████████████▎                                                                               | 791/5214 [37:37<3:30:23,  0.35it/s, v_num=3071, train_loss_step=0.158]batch: 791 => Loss: 0.12443660944700241\n",
      "Epoch 0:  15%|██████████████▎                                                                               | 792/5214 [37:40<3:30:20,  0.35it/s, v_num=3071, train_loss_step=0.124]batch: 792 => Loss: 0.22224445641040802\n",
      "Epoch 0:  15%|██████████████▎                                                                               | 793/5214 [37:43<3:30:17,  0.35it/s, v_num=3071, train_loss_step=0.222]batch: 793 => Loss: 0.20491094887256622\n",
      "Epoch 0:  15%|██████████████▎                                                                               | 794/5214 [37:46<3:30:14,  0.35it/s, v_num=3071, train_loss_step=0.205]batch: 794 => Loss: 0.17231027781963348\n",
      "Epoch 0:  15%|██████████████▎                                                                               | 795/5214 [37:48<3:30:11,  0.35it/s, v_num=3071, train_loss_step=0.172]batch: 795 => Loss: 0.27047887444496155\n",
      "Epoch 0:  15%|██████████████▎                                                                               | 796/5214 [37:51<3:30:08,  0.35it/s, v_num=3071, train_loss_step=0.270]batch: 796 => Loss: 0.26147016882896423\n",
      "Epoch 0:  15%|██████████████▎                                                                               | 797/5214 [37:54<3:30:05,  0.35it/s, v_num=3071, train_loss_step=0.261]batch: 797 => Loss: 0.1690068244934082\n",
      "Epoch 0:  15%|██████████████▍                                                                               | 798/5214 [37:57<3:30:03,  0.35it/s, v_num=3071, train_loss_step=0.169]batch: 798 => Loss: 0.08825501054525375\n",
      "Epoch 0:  15%|██████████████▎                                                                              | 799/5214 [38:00<3:30:00,  0.35it/s, v_num=3071, train_loss_step=0.0883]batch: 799 => Loss: 0.12632182240486145\n",
      "Epoch 0:  15%|██████████████▍                                                                               | 800/5214 [38:03<3:29:57,  0.35it/s, v_num=3071, train_loss_step=0.126]batch: 800 => Loss: 0.10985101759433746\n",
      "Epoch 0:  15%|██████████████▍                                                                               | 801/5214 [38:06<3:29:54,  0.35it/s, v_num=3071, train_loss_step=0.110]batch: 801 => Loss: 0.1452653855085373\n",
      "Epoch 0:  15%|██████████████▍                                                                               | 802/5214 [38:08<3:29:51,  0.35it/s, v_num=3071, train_loss_step=0.145]batch: 802 => Loss: 0.17377875745296478\n",
      "Epoch 0:  15%|██████████████▍                                                                               | 803/5214 [38:11<3:29:48,  0.35it/s, v_num=3071, train_loss_step=0.174]batch: 803 => Loss: 0.14224949479103088\n",
      "Epoch 0:  15%|██████████████▍                                                                               | 804/5214 [38:14<3:29:45,  0.35it/s, v_num=3071, train_loss_step=0.142]batch: 804 => Loss: 0.11066430062055588\n",
      "Epoch 0:  15%|██████████████▌                                                                               | 805/5214 [38:17<3:29:42,  0.35it/s, v_num=3071, train_loss_step=0.111]batch: 805 => Loss: 0.12873227894306183\n",
      "Epoch 0:  15%|██████████████▌                                                                               | 806/5214 [38:20<3:29:40,  0.35it/s, v_num=3071, train_loss_step=0.129]batch: 806 => Loss: 0.1337748020887375\n",
      "Epoch 0:  15%|██████████████▌                                                                               | 807/5214 [38:23<3:29:37,  0.35it/s, v_num=3071, train_loss_step=0.134]batch: 807 => Loss: 0.13171568512916565\n",
      "Epoch 0:  15%|██████████████▌                                                                               | 808/5214 [38:25<3:29:34,  0.35it/s, v_num=3071, train_loss_step=0.132]batch: 808 => Loss: 0.14215965569019318\n",
      "Epoch 0:  16%|██████████████▌                                                                               | 809/5214 [38:28<3:29:31,  0.35it/s, v_num=3071, train_loss_step=0.142]batch: 809 => Loss: 0.11686062812805176\n",
      "Epoch 0:  16%|██████████████▌                                                                               | 810/5214 [38:31<3:29:28,  0.35it/s, v_num=3071, train_loss_step=0.117]batch: 810 => Loss: 0.2925059497356415\n",
      "Epoch 0:  16%|██████████████▌                                                                               | 811/5214 [38:34<3:29:25,  0.35it/s, v_num=3071, train_loss_step=0.293]batch: 811 => Loss: 0.3032425343990326\n",
      "Epoch 0:  16%|██████████████▋                                                                               | 812/5214 [38:37<3:29:22,  0.35it/s, v_num=3071, train_loss_step=0.303]batch: 812 => Loss: 0.18506446480751038\n",
      "Epoch 0:  16%|██████████████▋                                                                               | 813/5214 [38:40<3:29:19,  0.35it/s, v_num=3071, train_loss_step=0.185]batch: 813 => Loss: 0.09963119775056839\n",
      "Epoch 0:  16%|██████████████▌                                                                              | 814/5214 [38:43<3:29:16,  0.35it/s, v_num=3071, train_loss_step=0.0996]batch: 814 => Loss: 0.1357375830411911\n",
      "Epoch 0:  16%|██████████████▋                                                                               | 815/5214 [38:45<3:29:14,  0.35it/s, v_num=3071, train_loss_step=0.136]batch: 815 => Loss: 0.1381242424249649\n",
      "Epoch 0:  16%|██████████████▋                                                                               | 816/5214 [38:48<3:29:11,  0.35it/s, v_num=3071, train_loss_step=0.138]batch: 816 => Loss: 0.1306777447462082\n",
      "Epoch 0:  16%|██████████████▋                                                                               | 817/5214 [38:51<3:29:08,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 817 => Loss: 0.18936888873577118\n",
      "Epoch 0:  16%|██████████████▋                                                                               | 818/5214 [38:54<3:29:05,  0.35it/s, v_num=3071, train_loss_step=0.189]batch: 818 => Loss: 0.14009280502796173\n",
      "Epoch 0:  16%|██████████████▊                                                                               | 819/5214 [38:57<3:29:02,  0.35it/s, v_num=3071, train_loss_step=0.140]batch: 819 => Loss: 0.31619372963905334\n",
      "Epoch 0:  16%|██████████████▊                                                                               | 820/5214 [39:00<3:28:59,  0.35it/s, v_num=3071, train_loss_step=0.316]batch: 820 => Loss: 0.2420896589756012\n",
      "Epoch 0:  16%|██████████████▊                                                                               | 821/5214 [39:03<3:28:56,  0.35it/s, v_num=3071, train_loss_step=0.242]batch: 821 => Loss: 0.18396417796611786\n",
      "Epoch 0:  16%|██████████████▊                                                                               | 822/5214 [39:05<3:28:54,  0.35it/s, v_num=3071, train_loss_step=0.184]batch: 822 => Loss: 0.1768575757741928\n",
      "Epoch 0:  16%|██████████████▊                                                                               | 823/5214 [39:08<3:28:51,  0.35it/s, v_num=3071, train_loss_step=0.177]batch: 823 => Loss: 0.17509838938713074\n",
      "Epoch 0:  16%|██████████████▊                                                                               | 824/5214 [39:11<3:28:48,  0.35it/s, v_num=3071, train_loss_step=0.175]batch: 824 => Loss: 0.16741728782653809\n",
      "Epoch 0:  16%|██████████████▊                                                                               | 825/5214 [39:14<3:28:45,  0.35it/s, v_num=3071, train_loss_step=0.167]batch: 825 => Loss: 0.1255098283290863\n",
      "Epoch 0:  16%|██████████████▉                                                                               | 826/5214 [39:17<3:28:42,  0.35it/s, v_num=3071, train_loss_step=0.126]batch: 826 => Loss: 0.14653848111629486\n",
      "Epoch 0:  16%|██████████████▉                                                                               | 827/5214 [39:20<3:28:40,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 827 => Loss: 0.11135248094797134\n",
      "Epoch 0:  16%|██████████████▉                                                                               | 828/5214 [39:23<3:28:37,  0.35it/s, v_num=3071, train_loss_step=0.111]batch: 828 => Loss: 0.21496382355690002\n",
      "Epoch 0:  16%|██████████████▉                                                                               | 829/5214 [39:25<3:28:34,  0.35it/s, v_num=3071, train_loss_step=0.215]batch: 829 => Loss: 0.1006823182106018\n",
      "Epoch 0:  16%|██████████████▉                                                                               | 830/5214 [39:28<3:28:31,  0.35it/s, v_num=3071, train_loss_step=0.101]batch: 830 => Loss: 0.16285865008831024\n",
      "Epoch 0:  16%|██████████████▉                                                                               | 831/5214 [39:31<3:28:28,  0.35it/s, v_num=3071, train_loss_step=0.163]batch: 831 => Loss: 0.11064346134662628\n",
      "Epoch 0:  16%|██████████████▉                                                                               | 832/5214 [39:34<3:28:25,  0.35it/s, v_num=3071, train_loss_step=0.111]batch: 832 => Loss: 0.12641452252864838\n",
      "Epoch 0:  16%|███████████████                                                                               | 833/5214 [39:37<3:28:22,  0.35it/s, v_num=3071, train_loss_step=0.126]batch: 833 => Loss: 0.16262750327587128\n",
      "Epoch 0:  16%|███████████████                                                                               | 834/5214 [39:40<3:28:19,  0.35it/s, v_num=3071, train_loss_step=0.163]batch: 834 => Loss: 0.1170799732208252\n",
      "Epoch 0:  16%|███████████████                                                                               | 835/5214 [39:42<3:28:17,  0.35it/s, v_num=3071, train_loss_step=0.117]batch: 835 => Loss: 0.1951478272676468\n",
      "Epoch 0:  16%|███████████████                                                                               | 836/5214 [39:45<3:28:14,  0.35it/s, v_num=3071, train_loss_step=0.195]batch: 836 => Loss: 0.10997722297906876\n",
      "Epoch 0:  16%|███████████████                                                                               | 837/5214 [39:48<3:28:11,  0.35it/s, v_num=3071, train_loss_step=0.110]batch: 837 => Loss: 0.19675719738006592\n",
      "Epoch 0:  16%|███████████████                                                                               | 838/5214 [39:51<3:28:08,  0.35it/s, v_num=3071, train_loss_step=0.197]batch: 838 => Loss: 0.1019342765212059\n",
      "Epoch 0:  16%|███████████████▏                                                                              | 839/5214 [39:54<3:28:05,  0.35it/s, v_num=3071, train_loss_step=0.102]batch: 839 => Loss: 0.2192939817905426\n",
      "Epoch 0:  16%|███████████████▏                                                                              | 840/5214 [39:57<3:28:02,  0.35it/s, v_num=3071, train_loss_step=0.219]batch: 840 => Loss: 0.21478834748268127\n",
      "Epoch 0:  16%|███████████████▏                                                                              | 841/5214 [40:00<3:28:00,  0.35it/s, v_num=3071, train_loss_step=0.215]batch: 841 => Loss: 0.19866573810577393\n",
      "Epoch 0:  16%|███████████████▏                                                                              | 842/5214 [40:02<3:27:57,  0.35it/s, v_num=3071, train_loss_step=0.199]batch: 842 => Loss: 0.10708780586719513\n",
      "Epoch 0:  16%|███████████████▏                                                                              | 843/5214 [40:05<3:27:54,  0.35it/s, v_num=3071, train_loss_step=0.107]batch: 843 => Loss: 0.24467921257019043\n",
      "Epoch 0:  16%|███████████████▏                                                                              | 844/5214 [40:08<3:27:51,  0.35it/s, v_num=3071, train_loss_step=0.245]batch: 844 => Loss: 0.14076641201972961\n",
      "Epoch 0:  16%|███████████████▏                                                                              | 845/5214 [40:11<3:27:48,  0.35it/s, v_num=3071, train_loss_step=0.141]batch: 845 => Loss: 0.14434637129306793\n",
      "Epoch 0:  16%|███████████████▎                                                                              | 846/5214 [40:14<3:27:45,  0.35it/s, v_num=3071, train_loss_step=0.144]batch: 846 => Loss: 0.19859588146209717\n",
      "Epoch 0:  16%|███████████████▎                                                                              | 847/5214 [40:17<3:27:42,  0.35it/s, v_num=3071, train_loss_step=0.199]batch: 847 => Loss: 0.16568180918693542\n",
      "Epoch 0:  16%|███████████████▎                                                                              | 848/5214 [40:20<3:27:40,  0.35it/s, v_num=3071, train_loss_step=0.166]batch: 848 => Loss: 0.1132778748869896\n",
      "Epoch 0:  16%|███████████████▎                                                                              | 849/5214 [40:22<3:27:37,  0.35it/s, v_num=3071, train_loss_step=0.113]batch: 849 => Loss: 0.19194194674491882\n",
      "Epoch 0:  16%|███████████████▎                                                                              | 850/5214 [40:25<3:27:34,  0.35it/s, v_num=3071, train_loss_step=0.192]batch: 850 => Loss: 0.12052420526742935\n",
      "Epoch 0:  16%|███████████████▎                                                                              | 851/5214 [40:28<3:27:31,  0.35it/s, v_num=3071, train_loss_step=0.121]batch: 851 => Loss: 0.21301423013210297\n",
      "Epoch 0:  16%|███████████████▎                                                                              | 852/5214 [40:31<3:27:28,  0.35it/s, v_num=3071, train_loss_step=0.213]batch: 852 => Loss: 0.19526389241218567\n",
      "Epoch 0:  16%|███████████████▍                                                                              | 853/5214 [40:34<3:27:25,  0.35it/s, v_num=3071, train_loss_step=0.195]batch: 853 => Loss: 0.11064648628234863\n",
      "Epoch 0:  16%|███████████████▍                                                                              | 854/5214 [40:37<3:27:23,  0.35it/s, v_num=3071, train_loss_step=0.111]batch: 854 => Loss: 0.18971727788448334\n",
      "Epoch 0:  16%|███████████████▍                                                                              | 855/5214 [40:40<3:27:20,  0.35it/s, v_num=3071, train_loss_step=0.190]batch: 855 => Loss: 0.24472050368785858\n",
      "Epoch 0:  16%|███████████████▍                                                                              | 856/5214 [40:42<3:27:17,  0.35it/s, v_num=3071, train_loss_step=0.245]batch: 856 => Loss: 0.12614469230175018\n",
      "Epoch 0:  16%|███████████████▍                                                                              | 857/5214 [40:45<3:27:14,  0.35it/s, v_num=3071, train_loss_step=0.126]batch: 857 => Loss: 0.12138543277978897\n",
      "Epoch 0:  16%|███████████████▍                                                                              | 858/5214 [40:48<3:27:11,  0.35it/s, v_num=3071, train_loss_step=0.121]batch: 858 => Loss: 0.12206842005252838\n",
      "Epoch 0:  16%|███████████████▍                                                                              | 859/5214 [40:51<3:27:08,  0.35it/s, v_num=3071, train_loss_step=0.122]batch: 859 => Loss: 0.17636100947856903\n",
      "Epoch 0:  16%|███████████████▌                                                                              | 860/5214 [40:54<3:27:06,  0.35it/s, v_num=3071, train_loss_step=0.176]batch: 860 => Loss: 0.1856498122215271\n",
      "Epoch 0:  17%|███████████████▌                                                                              | 861/5214 [40:57<3:27:03,  0.35it/s, v_num=3071, train_loss_step=0.186]batch: 861 => Loss: 0.19468583166599274\n",
      "Epoch 0:  17%|███████████████▌                                                                              | 862/5214 [41:00<3:27:00,  0.35it/s, v_num=3071, train_loss_step=0.195]batch: 862 => Loss: 0.09108708798885345\n",
      "Epoch 0:  17%|███████████████▍                                                                             | 863/5214 [41:02<3:26:57,  0.35it/s, v_num=3071, train_loss_step=0.0911]batch: 863 => Loss: 0.15159323811531067\n",
      "Epoch 0:  17%|███████████████▌                                                                              | 864/5214 [41:05<3:26:54,  0.35it/s, v_num=3071, train_loss_step=0.152]batch: 864 => Loss: 0.144504114985466\n",
      "Epoch 0:  17%|███████████████▌                                                                              | 865/5214 [41:08<3:26:51,  0.35it/s, v_num=3071, train_loss_step=0.145]batch: 865 => Loss: 0.2072901725769043\n",
      "Epoch 0:  17%|███████████████▌                                                                              | 866/5214 [41:11<3:26:49,  0.35it/s, v_num=3071, train_loss_step=0.207]batch: 866 => Loss: 0.1584121733903885\n",
      "Epoch 0:  17%|███████████████▋                                                                              | 867/5214 [41:14<3:26:46,  0.35it/s, v_num=3071, train_loss_step=0.158]batch: 867 => Loss: 0.11504963785409927\n",
      "Epoch 0:  17%|███████████████▋                                                                              | 868/5214 [41:17<3:26:43,  0.35it/s, v_num=3071, train_loss_step=0.115]batch: 868 => Loss: 0.20895853638648987\n",
      "Epoch 0:  17%|███████████████▋                                                                              | 869/5214 [41:20<3:26:40,  0.35it/s, v_num=3071, train_loss_step=0.209]batch: 869 => Loss: 0.20844998955726624\n",
      "Epoch 0:  17%|███████████████▋                                                                              | 870/5214 [41:22<3:26:37,  0.35it/s, v_num=3071, train_loss_step=0.208]batch: 870 => Loss: 0.19550634920597076\n",
      "Epoch 0:  17%|███████████████▋                                                                              | 871/5214 [41:25<3:26:34,  0.35it/s, v_num=3071, train_loss_step=0.196]batch: 871 => Loss: 0.20637087523937225\n",
      "Epoch 0:  17%|███████████████▋                                                                              | 872/5214 [41:28<3:26:31,  0.35it/s, v_num=3071, train_loss_step=0.206]batch: 872 => Loss: 0.1525851935148239\n",
      "Epoch 0:  17%|███████████████▋                                                                              | 873/5214 [41:31<3:26:29,  0.35it/s, v_num=3071, train_loss_step=0.153]batch: 873 => Loss: 0.09879245609045029\n",
      "Epoch 0:  17%|███████████████▌                                                                             | 874/5214 [41:34<3:26:26,  0.35it/s, v_num=3071, train_loss_step=0.0988]batch: 874 => Loss: 0.15511217713356018\n",
      "Epoch 0:  17%|███████████████▊                                                                              | 875/5214 [41:37<3:26:23,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 875 => Loss: 0.16976331174373627\n",
      "Epoch 0:  17%|███████████████▊                                                                              | 876/5214 [41:40<3:26:20,  0.35it/s, v_num=3071, train_loss_step=0.170]batch: 876 => Loss: 0.1259247213602066\n",
      "Epoch 0:  17%|███████████████▊                                                                              | 877/5214 [41:42<3:26:17,  0.35it/s, v_num=3071, train_loss_step=0.126]batch: 877 => Loss: 0.16651777923107147\n",
      "Epoch 0:  17%|███████████████▊                                                                              | 878/5214 [41:45<3:26:14,  0.35it/s, v_num=3071, train_loss_step=0.167]batch: 878 => Loss: 0.19963601231575012\n",
      "Epoch 0:  17%|███████████████▊                                                                              | 879/5214 [41:48<3:26:12,  0.35it/s, v_num=3071, train_loss_step=0.200]batch: 879 => Loss: 0.14154241979122162\n",
      "Epoch 0:  17%|███████████████▊                                                                              | 880/5214 [41:51<3:26:09,  0.35it/s, v_num=3071, train_loss_step=0.142]batch: 880 => Loss: 0.30961212515830994\n",
      "Epoch 0:  17%|███████████████▉                                                                              | 881/5214 [41:54<3:26:06,  0.35it/s, v_num=3071, train_loss_step=0.310]batch: 881 => Loss: 0.16603469848632812\n",
      "Epoch 0:  17%|███████████████▉                                                                              | 882/5214 [41:57<3:26:03,  0.35it/s, v_num=3071, train_loss_step=0.166]batch: 882 => Loss: 0.10129734128713608\n",
      "Epoch 0:  17%|███████████████▉                                                                              | 883/5214 [42:00<3:26:00,  0.35it/s, v_num=3071, train_loss_step=0.101]batch: 883 => Loss: 0.18776218593120575\n",
      "Epoch 0:  17%|███████████████▉                                                                              | 884/5214 [42:02<3:25:57,  0.35it/s, v_num=3071, train_loss_step=0.188]batch: 884 => Loss: 0.12100800126791\n",
      "Epoch 0:  17%|███████████████▉                                                                              | 885/5214 [42:05<3:25:55,  0.35it/s, v_num=3071, train_loss_step=0.121]batch: 885 => Loss: 0.2385082244873047\n",
      "Epoch 0:  17%|███████████████▉                                                                              | 886/5214 [42:08<3:25:52,  0.35it/s, v_num=3071, train_loss_step=0.239]batch: 886 => Loss: 0.1642298549413681\n",
      "Epoch 0:  17%|███████████████▉                                                                              | 887/5214 [42:11<3:25:49,  0.35it/s, v_num=3071, train_loss_step=0.164]batch: 887 => Loss: 0.13518230617046356\n",
      "Epoch 0:  17%|████████████████                                                                              | 888/5214 [42:14<3:25:46,  0.35it/s, v_num=3071, train_loss_step=0.135]batch: 888 => Loss: 0.1297421008348465\n",
      "Epoch 0:  17%|████████████████                                                                              | 889/5214 [42:17<3:25:43,  0.35it/s, v_num=3071, train_loss_step=0.130]batch: 889 => Loss: 0.17377281188964844\n",
      "Epoch 0:  17%|████████████████                                                                              | 890/5214 [42:20<3:25:40,  0.35it/s, v_num=3071, train_loss_step=0.174]batch: 890 => Loss: 0.2554568946361542\n",
      "Epoch 0:  17%|████████████████                                                                              | 891/5214 [42:22<3:25:37,  0.35it/s, v_num=3071, train_loss_step=0.255]batch: 891 => Loss: 0.24479369819164276\n",
      "Epoch 0:  17%|████████████████                                                                              | 892/5214 [42:25<3:25:34,  0.35it/s, v_num=3071, train_loss_step=0.245]batch: 892 => Loss: 0.13590803742408752\n",
      "Epoch 0:  17%|████████████████                                                                              | 893/5214 [42:28<3:25:32,  0.35it/s, v_num=3071, train_loss_step=0.136]batch: 893 => Loss: 0.09959960728883743\n",
      "Epoch 0:  17%|███████████████▉                                                                             | 894/5214 [42:31<3:25:29,  0.35it/s, v_num=3071, train_loss_step=0.0996]batch: 894 => Loss: 0.131418377161026\n",
      "Epoch 0:  17%|████████████████▏                                                                             | 895/5214 [42:34<3:25:26,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 895 => Loss: 0.12920649349689484\n",
      "Epoch 0:  17%|████████████████▏                                                                             | 896/5214 [42:37<3:25:23,  0.35it/s, v_num=3071, train_loss_step=0.129]batch: 896 => Loss: 0.1842181235551834\n",
      "Epoch 0:  17%|████████████████▏                                                                             | 897/5214 [42:40<3:25:20,  0.35it/s, v_num=3071, train_loss_step=0.184]batch: 897 => Loss: 0.1665097326040268\n",
      "Epoch 0:  17%|████████████████▏                                                                             | 898/5214 [42:42<3:25:17,  0.35it/s, v_num=3071, train_loss_step=0.167]batch: 898 => Loss: 0.12713228166103363\n",
      "Epoch 0:  17%|████████████████▏                                                                             | 899/5214 [42:45<3:25:14,  0.35it/s, v_num=3071, train_loss_step=0.127]batch: 899 => Loss: 0.14203964173793793\n",
      "Epoch 0:  17%|████████████████▏                                                                             | 900/5214 [42:48<3:25:12,  0.35it/s, v_num=3071, train_loss_step=0.142]batch: 900 => Loss: 0.19142253696918488\n",
      "Epoch 0:  17%|████████████████▏                                                                             | 901/5214 [42:51<3:25:09,  0.35it/s, v_num=3071, train_loss_step=0.191]batch: 901 => Loss: 0.16379855573177338\n",
      "Epoch 0:  17%|████████████████▎                                                                             | 902/5214 [42:54<3:25:06,  0.35it/s, v_num=3071, train_loss_step=0.164]batch: 902 => Loss: 0.16312798857688904\n",
      "Epoch 0:  17%|████████████████▎                                                                             | 903/5214 [42:57<3:25:03,  0.35it/s, v_num=3071, train_loss_step=0.163]batch: 903 => Loss: 0.17124192416667938\n",
      "Epoch 0:  17%|████████████████▎                                                                             | 904/5214 [43:00<3:25:00,  0.35it/s, v_num=3071, train_loss_step=0.171]batch: 904 => Loss: 0.12910132110118866\n",
      "Epoch 0:  17%|████████████████▎                                                                             | 905/5214 [43:02<3:24:57,  0.35it/s, v_num=3071, train_loss_step=0.129]batch: 905 => Loss: 0.311286598443985\n",
      "Epoch 0:  17%|████████████████▎                                                                             | 906/5214 [43:05<3:24:54,  0.35it/s, v_num=3071, train_loss_step=0.311]batch: 906 => Loss: 0.20707999169826508\n",
      "Epoch 0:  17%|████████████████▎                                                                             | 907/5214 [43:08<3:24:52,  0.35it/s, v_num=3071, train_loss_step=0.207]batch: 907 => Loss: 0.13138069212436676\n",
      "Epoch 0:  17%|████████████████▎                                                                             | 908/5214 [43:11<3:24:49,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 908 => Loss: 0.12977808713912964\n",
      "Epoch 0:  17%|████████████████▍                                                                             | 909/5214 [43:14<3:24:46,  0.35it/s, v_num=3071, train_loss_step=0.130]batch: 909 => Loss: 0.1380154937505722\n",
      "Epoch 0:  17%|████████████████▍                                                                             | 910/5214 [43:17<3:24:43,  0.35it/s, v_num=3071, train_loss_step=0.138]batch: 910 => Loss: 0.1953628659248352\n",
      "Epoch 0:  17%|████████████████▍                                                                             | 911/5214 [43:19<3:24:40,  0.35it/s, v_num=3071, train_loss_step=0.195]batch: 911 => Loss: 0.15446269512176514\n",
      "Epoch 0:  17%|████████████████▍                                                                             | 912/5214 [43:22<3:24:37,  0.35it/s, v_num=3071, train_loss_step=0.154]batch: 912 => Loss: 0.13406319916248322\n",
      "Epoch 0:  18%|████████████████▍                                                                             | 913/5214 [43:25<3:24:34,  0.35it/s, v_num=3071, train_loss_step=0.134]batch: 913 => Loss: 0.1295468509197235\n",
      "Epoch 0:  18%|████████████████▍                                                                             | 914/5214 [43:28<3:24:32,  0.35it/s, v_num=3071, train_loss_step=0.130]batch: 914 => Loss: 0.2255941927433014\n",
      "Epoch 0:  18%|████████████████▍                                                                             | 915/5214 [43:31<3:24:29,  0.35it/s, v_num=3071, train_loss_step=0.226]batch: 915 => Loss: 0.18116532266139984\n",
      "Epoch 0:  18%|████████████████▌                                                                             | 916/5214 [43:34<3:24:26,  0.35it/s, v_num=3071, train_loss_step=0.181]batch: 916 => Loss: 0.14650142192840576\n",
      "Epoch 0:  18%|████████████████▌                                                                             | 917/5214 [43:37<3:24:23,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 917 => Loss: 0.13124322891235352\n",
      "Epoch 0:  18%|████████████████▌                                                                             | 918/5214 [43:39<3:24:20,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 918 => Loss: 0.23948097229003906\n",
      "Epoch 0:  18%|████████████████▌                                                                             | 919/5214 [43:42<3:24:17,  0.35it/s, v_num=3071, train_loss_step=0.239]batch: 919 => Loss: 0.23500601947307587\n",
      "Epoch 0:  18%|████████████████▌                                                                             | 920/5214 [43:45<3:24:15,  0.35it/s, v_num=3071, train_loss_step=0.235]batch: 920 => Loss: 0.20955966413021088\n",
      "Epoch 0:  18%|████████████████▌                                                                             | 921/5214 [43:48<3:24:12,  0.35it/s, v_num=3071, train_loss_step=0.210]batch: 921 => Loss: 0.16900299489498138\n",
      "Epoch 0:  18%|████████████████▌                                                                             | 922/5214 [43:51<3:24:09,  0.35it/s, v_num=3071, train_loss_step=0.169]batch: 922 => Loss: 0.10589732229709625\n",
      "Epoch 0:  18%|████████████████▋                                                                             | 923/5214 [43:54<3:24:06,  0.35it/s, v_num=3071, train_loss_step=0.106]batch: 923 => Loss: 0.1177087053656578\n",
      "Epoch 0:  18%|████████████████▋                                                                             | 924/5214 [43:57<3:24:03,  0.35it/s, v_num=3071, train_loss_step=0.118]batch: 924 => Loss: 0.1950286626815796\n",
      "Epoch 0:  18%|████████████████▋                                                                             | 925/5214 [43:59<3:24:00,  0.35it/s, v_num=3071, train_loss_step=0.195]batch: 925 => Loss: 0.10885012149810791\n",
      "Epoch 0:  18%|████████████████▋                                                                             | 926/5214 [44:02<3:23:57,  0.35it/s, v_num=3071, train_loss_step=0.109]batch: 926 => Loss: 0.08679521828889847\n",
      "Epoch 0:  18%|████████████████▌                                                                            | 927/5214 [44:05<3:23:55,  0.35it/s, v_num=3071, train_loss_step=0.0868]batch: 927 => Loss: 0.18655648827552795\n",
      "Epoch 0:  18%|████████████████▋                                                                             | 928/5214 [44:08<3:23:52,  0.35it/s, v_num=3071, train_loss_step=0.187]batch: 928 => Loss: 0.17275553941726685\n",
      "Epoch 0:  18%|████████████████▋                                                                             | 929/5214 [44:11<3:23:49,  0.35it/s, v_num=3071, train_loss_step=0.173]batch: 929 => Loss: 0.15510194003582\n",
      "Epoch 0:  18%|████████████████▊                                                                             | 930/5214 [44:14<3:23:46,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 930 => Loss: 0.1952543705701828\n",
      "Epoch 0:  18%|████████████████▊                                                                             | 931/5214 [44:17<3:23:43,  0.35it/s, v_num=3071, train_loss_step=0.195]batch: 931 => Loss: 0.1607331782579422\n",
      "Epoch 0:  18%|████████████████▊                                                                             | 932/5214 [44:19<3:23:40,  0.35it/s, v_num=3071, train_loss_step=0.161]batch: 932 => Loss: 0.13142018020153046\n",
      "Epoch 0:  18%|████████████████▊                                                                             | 933/5214 [44:22<3:23:38,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 933 => Loss: 0.18537314236164093\n",
      "Epoch 0:  18%|████████████████▊                                                                             | 934/5214 [44:25<3:23:35,  0.35it/s, v_num=3071, train_loss_step=0.185]batch: 934 => Loss: 0.24140320718288422\n",
      "Epoch 0:  18%|████████████████▊                                                                             | 935/5214 [44:28<3:23:32,  0.35it/s, v_num=3071, train_loss_step=0.241]batch: 935 => Loss: 0.1592506766319275\n",
      "Epoch 0:  18%|████████████████▊                                                                             | 936/5214 [44:31<3:23:29,  0.35it/s, v_num=3071, train_loss_step=0.159]batch: 936 => Loss: 0.17804421484470367\n",
      "Epoch 0:  18%|████████████████▉                                                                             | 937/5214 [44:34<3:23:26,  0.35it/s, v_num=3071, train_loss_step=0.178]batch: 937 => Loss: 0.13298501074314117\n",
      "Epoch 0:  18%|████████████████▉                                                                             | 938/5214 [44:37<3:23:23,  0.35it/s, v_num=3071, train_loss_step=0.133]batch: 938 => Loss: 0.15181751549243927\n",
      "Epoch 0:  18%|████████████████▉                                                                             | 939/5214 [44:39<3:23:21,  0.35it/s, v_num=3071, train_loss_step=0.152]batch: 939 => Loss: 0.14121471345424652\n",
      "Epoch 0:  18%|████████████████▉                                                                             | 940/5214 [44:42<3:23:18,  0.35it/s, v_num=3071, train_loss_step=0.141]batch: 940 => Loss: 0.15489239990711212\n",
      "Epoch 0:  18%|████████████████▉                                                                             | 941/5214 [44:45<3:23:15,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 941 => Loss: 0.14224599301815033\n",
      "Epoch 0:  18%|████████████████▉                                                                             | 942/5214 [44:48<3:23:12,  0.35it/s, v_num=3071, train_loss_step=0.142]batch: 942 => Loss: 0.2688666880130768\n",
      "Epoch 0:  18%|█████████████████                                                                             | 943/5214 [44:51<3:23:09,  0.35it/s, v_num=3071, train_loss_step=0.269]batch: 943 => Loss: 0.17286452651023865\n",
      "Epoch 0:  18%|█████████████████                                                                             | 944/5214 [44:54<3:23:06,  0.35it/s, v_num=3071, train_loss_step=0.173]batch: 944 => Loss: 0.2183089256286621\n",
      "Epoch 0:  18%|█████████████████                                                                             | 945/5214 [44:57<3:23:03,  0.35it/s, v_num=3071, train_loss_step=0.218]batch: 945 => Loss: 0.1741275042295456\n",
      "Epoch 0:  18%|█████████████████                                                                             | 946/5214 [44:59<3:23:01,  0.35it/s, v_num=3071, train_loss_step=0.174]batch: 946 => Loss: 0.1838056743144989\n",
      "Epoch 0:  18%|█████████████████                                                                             | 947/5214 [45:02<3:22:58,  0.35it/s, v_num=3071, train_loss_step=0.184]batch: 947 => Loss: 0.2662508189678192\n",
      "Epoch 0:  18%|█████████████████                                                                             | 948/5214 [45:05<3:22:55,  0.35it/s, v_num=3071, train_loss_step=0.266]batch: 948 => Loss: 0.12924428284168243\n",
      "Epoch 0:  18%|█████████████████                                                                             | 949/5214 [45:08<3:22:52,  0.35it/s, v_num=3071, train_loss_step=0.129]batch: 949 => Loss: 0.15850749611854553\n",
      "Epoch 0:  18%|█████████████████▏                                                                            | 950/5214 [45:11<3:22:49,  0.35it/s, v_num=3071, train_loss_step=0.159]batch: 950 => Loss: 0.17448878288269043\n",
      "Epoch 0:  18%|█████████████████▏                                                                            | 951/5214 [45:14<3:22:46,  0.35it/s, v_num=3071, train_loss_step=0.174]batch: 951 => Loss: 0.15046873688697815\n",
      "Epoch 0:  18%|█████████████████▏                                                                            | 952/5214 [45:17<3:22:43,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 952 => Loss: 0.11900313198566437\n",
      "Epoch 0:  18%|█████████████████▏                                                                            | 953/5214 [45:19<3:22:40,  0.35it/s, v_num=3071, train_loss_step=0.119]batch: 953 => Loss: 0.20533457398414612\n",
      "Epoch 0:  18%|█████████████████▏                                                                            | 954/5214 [45:22<3:22:38,  0.35it/s, v_num=3071, train_loss_step=0.205]batch: 954 => Loss: 0.16628597676753998\n",
      "Epoch 0:  18%|█████████████████▏                                                                            | 955/5214 [45:25<3:22:35,  0.35it/s, v_num=3071, train_loss_step=0.166]batch: 955 => Loss: 0.1969277709722519\n",
      "Epoch 0:  18%|█████████████████▏                                                                            | 956/5214 [45:28<3:22:32,  0.35it/s, v_num=3071, train_loss_step=0.197]batch: 956 => Loss: 0.12073458731174469\n",
      "Epoch 0:  18%|█████████████████▎                                                                            | 957/5214 [45:31<3:22:29,  0.35it/s, v_num=3071, train_loss_step=0.121]batch: 957 => Loss: 0.1393832266330719\n",
      "Epoch 0:  18%|█████████████████▎                                                                            | 958/5214 [45:34<3:22:26,  0.35it/s, v_num=3071, train_loss_step=0.139]batch: 958 => Loss: 0.30902233719825745\n",
      "Epoch 0:  18%|█████████████████▎                                                                            | 959/5214 [45:36<3:22:23,  0.35it/s, v_num=3071, train_loss_step=0.309]batch: 959 => Loss: 0.2300703525543213\n",
      "Epoch 0:  18%|█████████████████▎                                                                            | 960/5214 [45:39<3:22:20,  0.35it/s, v_num=3071, train_loss_step=0.230]batch: 960 => Loss: 0.1772102564573288\n",
      "Epoch 0:  18%|█████████████████▎                                                                            | 961/5214 [45:42<3:22:18,  0.35it/s, v_num=3071, train_loss_step=0.177]batch: 961 => Loss: 0.20122957229614258\n",
      "Epoch 0:  18%|█████████████████▎                                                                            | 962/5214 [45:45<3:22:15,  0.35it/s, v_num=3071, train_loss_step=0.201]batch: 962 => Loss: 0.3165767788887024\n",
      "Epoch 0:  18%|█████████████████▎                                                                            | 963/5214 [45:48<3:22:12,  0.35it/s, v_num=3071, train_loss_step=0.317]batch: 963 => Loss: 0.11553957313299179\n",
      "Epoch 0:  18%|█████████████████▍                                                                            | 964/5214 [45:51<3:22:09,  0.35it/s, v_num=3071, train_loss_step=0.116]batch: 964 => Loss: 0.12547187507152557\n",
      "Epoch 0:  19%|█████████████████▍                                                                            | 965/5214 [45:54<3:22:06,  0.35it/s, v_num=3071, train_loss_step=0.125]batch: 965 => Loss: 0.2351016104221344\n",
      "Epoch 0:  19%|█████████████████▍                                                                            | 966/5214 [45:56<3:22:03,  0.35it/s, v_num=3071, train_loss_step=0.235]batch: 966 => Loss: 0.13575424253940582\n",
      "Epoch 0:  19%|█████████████████▍                                                                            | 967/5214 [45:59<3:22:00,  0.35it/s, v_num=3071, train_loss_step=0.136]batch: 967 => Loss: 0.2195703089237213\n",
      "Epoch 0:  19%|█████████████████▍                                                                            | 968/5214 [46:02<3:21:57,  0.35it/s, v_num=3071, train_loss_step=0.220]batch: 968 => Loss: 0.14031921327114105\n",
      "Epoch 0:  19%|█████████████████▍                                                                            | 969/5214 [46:05<3:21:55,  0.35it/s, v_num=3071, train_loss_step=0.140]batch: 969 => Loss: 0.14204968512058258\n",
      "Epoch 0:  19%|█████████████████▍                                                                            | 970/5214 [46:08<3:21:52,  0.35it/s, v_num=3071, train_loss_step=0.142]batch: 970 => Loss: 0.1066339984536171\n",
      "Epoch 0:  19%|█████████████████▌                                                                            | 971/5214 [46:11<3:21:49,  0.35it/s, v_num=3071, train_loss_step=0.107]batch: 971 => Loss: 0.28002187609672546\n",
      "Epoch 0:  19%|█████████████████▌                                                                            | 972/5214 [46:14<3:21:46,  0.35it/s, v_num=3071, train_loss_step=0.280]batch: 972 => Loss: 0.09494340419769287\n",
      "Epoch 0:  19%|█████████████████▎                                                                           | 973/5214 [46:16<3:21:43,  0.35it/s, v_num=3071, train_loss_step=0.0949]batch: 973 => Loss: 0.1329231709241867\n",
      "Epoch 0:  19%|█████████████████▌                                                                            | 974/5214 [46:19<3:21:40,  0.35it/s, v_num=3071, train_loss_step=0.133]batch: 974 => Loss: 0.17240910232067108\n",
      "Epoch 0:  19%|█████████████████▌                                                                            | 975/5214 [46:22<3:21:37,  0.35it/s, v_num=3071, train_loss_step=0.172]batch: 975 => Loss: 0.17777632176876068\n",
      "Epoch 0:  19%|█████████████████▌                                                                            | 976/5214 [46:25<3:21:35,  0.35it/s, v_num=3071, train_loss_step=0.178]batch: 976 => Loss: 0.14323356747627258\n",
      "Epoch 0:  19%|█████████████████▌                                                                            | 977/5214 [46:28<3:21:32,  0.35it/s, v_num=3071, train_loss_step=0.143]batch: 977 => Loss: 0.12978188693523407\n",
      "Epoch 0:  19%|█████████████████▋                                                                            | 978/5214 [46:31<3:21:29,  0.35it/s, v_num=3071, train_loss_step=0.130]batch: 978 => Loss: 0.11548259109258652\n",
      "Epoch 0:  19%|█████████████████▋                                                                            | 979/5214 [46:34<3:21:26,  0.35it/s, v_num=3071, train_loss_step=0.115]batch: 979 => Loss: 0.19196148216724396\n",
      "Epoch 0:  19%|█████████████████▋                                                                            | 980/5214 [46:36<3:21:23,  0.35it/s, v_num=3071, train_loss_step=0.192]batch: 980 => Loss: 0.1830339878797531\n",
      "Epoch 0:  19%|█████████████████▋                                                                            | 981/5214 [46:39<3:21:20,  0.35it/s, v_num=3071, train_loss_step=0.183]batch: 981 => Loss: 0.15174049139022827\n",
      "Epoch 0:  19%|█████████████████▋                                                                            | 982/5214 [46:42<3:21:17,  0.35it/s, v_num=3071, train_loss_step=0.152]batch: 982 => Loss: 0.14295680820941925\n",
      "Epoch 0:  19%|█████████████████▋                                                                            | 983/5214 [46:45<3:21:15,  0.35it/s, v_num=3071, train_loss_step=0.143]batch: 983 => Loss: 0.3413461744785309\n",
      "Epoch 0:  19%|█████████████████▋                                                                            | 984/5214 [46:48<3:21:12,  0.35it/s, v_num=3071, train_loss_step=0.341]batch: 984 => Loss: 0.13091814517974854\n",
      "Epoch 0:  19%|█████████████████▊                                                                            | 985/5214 [46:51<3:21:09,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 985 => Loss: 0.15503323078155518\n",
      "Epoch 0:  19%|█████████████████▊                                                                            | 986/5214 [46:53<3:21:06,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 986 => Loss: 0.12153533846139908\n",
      "Epoch 0:  19%|█████████████████▊                                                                            | 987/5214 [46:56<3:21:03,  0.35it/s, v_num=3071, train_loss_step=0.122]batch: 987 => Loss: 0.1209726557135582\n",
      "Epoch 0:  19%|█████████████████▊                                                                            | 988/5214 [46:59<3:21:00,  0.35it/s, v_num=3071, train_loss_step=0.121]batch: 988 => Loss: 0.1384245604276657\n",
      "Epoch 0:  19%|█████████████████▊                                                                            | 989/5214 [47:02<3:20:57,  0.35it/s, v_num=3071, train_loss_step=0.138]batch: 989 => Loss: 0.1371241807937622\n",
      "Epoch 0:  19%|█████████████████▊                                                                            | 990/5214 [47:05<3:20:54,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 990 => Loss: 0.17878596484661102\n",
      "Epoch 0:  19%|█████████████████▊                                                                            | 991/5214 [47:08<3:20:52,  0.35it/s, v_num=3071, train_loss_step=0.179]batch: 991 => Loss: 0.1933109611272812\n",
      "Epoch 0:  19%|█████████████████▉                                                                            | 992/5214 [47:11<3:20:49,  0.35it/s, v_num=3071, train_loss_step=0.193]batch: 992 => Loss: 0.11573281139135361\n",
      "Epoch 0:  19%|█████████████████▉                                                                            | 993/5214 [47:13<3:20:46,  0.35it/s, v_num=3071, train_loss_step=0.116]batch: 993 => Loss: 0.16861256957054138\n",
      "Epoch 0:  19%|█████████████████▉                                                                            | 994/5214 [47:16<3:20:43,  0.35it/s, v_num=3071, train_loss_step=0.169]batch: 994 => Loss: 0.15354393422603607\n",
      "Epoch 0:  19%|█████████████████▉                                                                            | 995/5214 [47:19<3:20:40,  0.35it/s, v_num=3071, train_loss_step=0.154]batch: 995 => Loss: 0.15341058373451233\n",
      "Epoch 0:  19%|█████████████████▉                                                                            | 996/5214 [47:22<3:20:37,  0.35it/s, v_num=3071, train_loss_step=0.153]batch: 996 => Loss: 0.18883121013641357\n",
      "Epoch 0:  19%|█████████████████▉                                                                            | 997/5214 [47:25<3:20:34,  0.35it/s, v_num=3071, train_loss_step=0.189]batch: 997 => Loss: 0.18233972787857056\n",
      "Epoch 0:  19%|█████████████████▉                                                                            | 998/5214 [47:28<3:20:32,  0.35it/s, v_num=3071, train_loss_step=0.182]batch: 998 => Loss: 0.14461643993854523\n",
      "Epoch 0:  19%|██████████████████                                                                            | 999/5214 [47:31<3:20:29,  0.35it/s, v_num=3071, train_loss_step=0.145]batch: 999 => Loss: 0.15592102706432343\n",
      "Epoch 0:  19%|█████████████████▊                                                                           | 1000/5214 [47:33<3:20:26,  0.35it/s, v_num=3071, train_loss_step=0.156]batch: 1000 => Loss: 0.1529126614332199\n",
      "Epoch 0:  19%|█████████████████▊                                                                           | 1001/5214 [47:36<3:20:23,  0.35it/s, v_num=3071, train_loss_step=0.153]batch: 1001 => Loss: 0.23464111983776093\n",
      "Epoch 0:  19%|█████████████████▊                                                                           | 1002/5214 [47:39<3:20:20,  0.35it/s, v_num=3071, train_loss_step=0.235]batch: 1002 => Loss: 0.1613539755344391\n",
      "Epoch 0:  19%|█████████████████▉                                                                           | 1003/5214 [47:42<3:20:17,  0.35it/s, v_num=3071, train_loss_step=0.161]batch: 1003 => Loss: 0.1988518089056015\n",
      "Epoch 0:  19%|█████████████████▉                                                                           | 1004/5214 [47:45<3:20:14,  0.35it/s, v_num=3071, train_loss_step=0.199]batch: 1004 => Loss: 0.11763638257980347\n",
      "Epoch 0:  19%|█████████████████▉                                                                           | 1005/5214 [47:48<3:20:12,  0.35it/s, v_num=3071, train_loss_step=0.118]batch: 1005 => Loss: 0.12985268235206604\n",
      "Epoch 0:  19%|█████████████████▉                                                                           | 1006/5214 [47:51<3:20:09,  0.35it/s, v_num=3071, train_loss_step=0.130]batch: 1006 => Loss: 0.13791966438293457\n",
      "Epoch 0:  19%|█████████████████▉                                                                           | 1007/5214 [47:53<3:20:06,  0.35it/s, v_num=3071, train_loss_step=0.138]batch: 1007 => Loss: 0.14402149617671967\n",
      "Epoch 0:  19%|█████████████████▉                                                                           | 1008/5214 [47:56<3:20:03,  0.35it/s, v_num=3071, train_loss_step=0.144]batch: 1008 => Loss: 0.22060275077819824\n",
      "Epoch 0:  19%|█████████████████▉                                                                           | 1009/5214 [47:59<3:20:00,  0.35it/s, v_num=3071, train_loss_step=0.221]batch: 1009 => Loss: 0.15973737835884094\n",
      "Epoch 0:  19%|██████████████████                                                                           | 1010/5214 [48:02<3:19:57,  0.35it/s, v_num=3071, train_loss_step=0.160]batch: 1010 => Loss: 0.1449594795703888\n",
      "Epoch 0:  19%|██████████████████                                                                           | 1011/5214 [48:05<3:19:54,  0.35it/s, v_num=3071, train_loss_step=0.145]batch: 1011 => Loss: 0.15556217730045319\n",
      "Epoch 0:  19%|██████████████████                                                                           | 1012/5214 [48:08<3:19:51,  0.35it/s, v_num=3071, train_loss_step=0.156]batch: 1012 => Loss: 0.0969904288649559\n",
      "Epoch 0:  19%|██████████████████                                                                           | 1013/5214 [48:10<3:19:48,  0.35it/s, v_num=3071, train_loss_step=0.097]batch: 1013 => Loss: 0.14279159903526306\n",
      "Epoch 0:  19%|██████████████████                                                                           | 1014/5214 [48:13<3:19:46,  0.35it/s, v_num=3071, train_loss_step=0.143]batch: 1014 => Loss: 0.213637113571167\n",
      "Epoch 0:  19%|██████████████████                                                                           | 1015/5214 [48:16<3:19:43,  0.35it/s, v_num=3071, train_loss_step=0.214]batch: 1015 => Loss: 0.23442593216896057\n",
      "Epoch 0:  19%|██████████████████                                                                           | 1016/5214 [48:19<3:19:40,  0.35it/s, v_num=3071, train_loss_step=0.234]batch: 1016 => Loss: 0.14994914829730988\n",
      "Epoch 0:  20%|██████████████████▏                                                                          | 1017/5214 [48:22<3:19:37,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 1017 => Loss: 0.15088827908039093\n",
      "Epoch 0:  20%|██████████████████▏                                                                          | 1018/5214 [48:25<3:19:34,  0.35it/s, v_num=3071, train_loss_step=0.151]batch: 1018 => Loss: 0.11334627121686935\n",
      "Epoch 0:  20%|██████████████████▏                                                                          | 1019/5214 [48:28<3:19:31,  0.35it/s, v_num=3071, train_loss_step=0.113]batch: 1019 => Loss: 0.11810772866010666\n",
      "Epoch 0:  20%|██████████████████▏                                                                          | 1020/5214 [48:30<3:19:28,  0.35it/s, v_num=3071, train_loss_step=0.118]batch: 1020 => Loss: 0.15253329277038574\n",
      "Epoch 0:  20%|██████████████████▏                                                                          | 1021/5214 [48:33<3:19:25,  0.35it/s, v_num=3071, train_loss_step=0.153]batch: 1021 => Loss: 0.13365252315998077\n",
      "Epoch 0:  20%|██████████████████▏                                                                          | 1022/5214 [48:36<3:19:23,  0.35it/s, v_num=3071, train_loss_step=0.134]batch: 1022 => Loss: 0.1744261533021927\n",
      "Epoch 0:  20%|██████████████████▏                                                                          | 1023/5214 [48:39<3:19:20,  0.35it/s, v_num=3071, train_loss_step=0.174]batch: 1023 => Loss: 0.14001666009426117\n",
      "Epoch 0:  20%|██████████████████▎                                                                          | 1024/5214 [48:42<3:19:17,  0.35it/s, v_num=3071, train_loss_step=0.140]batch: 1024 => Loss: 0.17112770676612854\n",
      "Epoch 0:  20%|██████████████████▎                                                                          | 1025/5214 [48:45<3:19:14,  0.35it/s, v_num=3071, train_loss_step=0.171]batch: 1025 => Loss: 0.15675334632396698\n",
      "Epoch 0:  20%|██████████████████▎                                                                          | 1026/5214 [48:48<3:19:11,  0.35it/s, v_num=3071, train_loss_step=0.157]batch: 1026 => Loss: 0.17978791892528534\n",
      "Epoch 0:  20%|██████████████████▎                                                                          | 1027/5214 [48:50<3:19:08,  0.35it/s, v_num=3071, train_loss_step=0.180]batch: 1027 => Loss: 0.16101352870464325\n",
      "Epoch 0:  20%|██████████████████▎                                                                          | 1028/5214 [48:53<3:19:06,  0.35it/s, v_num=3071, train_loss_step=0.161]batch: 1028 => Loss: 0.13111308217048645\n",
      "Epoch 0:  20%|██████████████████▎                                                                          | 1029/5214 [48:56<3:19:03,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 1029 => Loss: 0.10421866178512573\n",
      "Epoch 0:  20%|██████████████████▎                                                                          | 1030/5214 [48:59<3:19:00,  0.35it/s, v_num=3071, train_loss_step=0.104]batch: 1030 => Loss: 0.13677124679088593\n",
      "Epoch 0:  20%|██████████████████▍                                                                          | 1031/5214 [49:02<3:18:57,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 1031 => Loss: 0.10095896571874619\n",
      "Epoch 0:  20%|██████████████████▍                                                                          | 1032/5214 [49:05<3:18:54,  0.35it/s, v_num=3071, train_loss_step=0.101]batch: 1032 => Loss: 0.23858903348445892\n",
      "Epoch 0:  20%|██████████████████▍                                                                          | 1033/5214 [49:07<3:18:51,  0.35it/s, v_num=3071, train_loss_step=0.239]batch: 1033 => Loss: 0.1011795774102211\n",
      "Epoch 0:  20%|██████████████████▍                                                                          | 1034/5214 [49:10<3:18:48,  0.35it/s, v_num=3071, train_loss_step=0.101]batch: 1034 => Loss: 0.19719107449054718\n",
      "Epoch 0:  20%|██████████████████▍                                                                          | 1035/5214 [49:13<3:18:46,  0.35it/s, v_num=3071, train_loss_step=0.197]batch: 1035 => Loss: 0.15732555091381073\n",
      "Epoch 0:  20%|██████████████████▍                                                                          | 1036/5214 [49:16<3:18:43,  0.35it/s, v_num=3071, train_loss_step=0.157]batch: 1036 => Loss: 0.16096094250679016\n",
      "Epoch 0:  20%|██████████████████▍                                                                          | 1037/5214 [49:19<3:18:40,  0.35it/s, v_num=3071, train_loss_step=0.161]batch: 1037 => Loss: 0.14986184239387512\n",
      "Epoch 0:  20%|██████████████████▌                                                                          | 1038/5214 [49:22<3:18:37,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 1038 => Loss: 0.11080608516931534\n",
      "Epoch 0:  20%|██████████████████▌                                                                          | 1039/5214 [49:25<3:18:34,  0.35it/s, v_num=3071, train_loss_step=0.111]batch: 1039 => Loss: 0.24552646279335022\n",
      "Epoch 0:  20%|██████████████████▌                                                                          | 1040/5214 [49:27<3:18:31,  0.35it/s, v_num=3071, train_loss_step=0.246]batch: 1040 => Loss: 0.1085859090089798\n",
      "Epoch 0:  20%|██████████████████▌                                                                          | 1041/5214 [49:30<3:18:28,  0.35it/s, v_num=3071, train_loss_step=0.109]batch: 1041 => Loss: 0.1390720009803772\n",
      "Epoch 0:  20%|██████████████████▌                                                                          | 1042/5214 [49:33<3:18:26,  0.35it/s, v_num=3071, train_loss_step=0.139]batch: 1042 => Loss: 0.13328856229782104\n",
      "Epoch 0:  20%|██████████████████▌                                                                          | 1043/5214 [49:36<3:18:23,  0.35it/s, v_num=3071, train_loss_step=0.133]batch: 1043 => Loss: 0.22038426995277405\n",
      "Epoch 0:  20%|██████████████████▌                                                                          | 1044/5214 [49:39<3:18:20,  0.35it/s, v_num=3071, train_loss_step=0.220]batch: 1044 => Loss: 0.1394960582256317\n",
      "Epoch 0:  20%|██████████████████▋                                                                          | 1045/5214 [49:42<3:18:17,  0.35it/s, v_num=3071, train_loss_step=0.139]batch: 1045 => Loss: 0.16839252412319183\n",
      "Epoch 0:  20%|██████████████████▋                                                                          | 1046/5214 [49:45<3:18:14,  0.35it/s, v_num=3071, train_loss_step=0.168]batch: 1046 => Loss: 0.16014519333839417\n",
      "Epoch 0:  20%|██████████████████▋                                                                          | 1047/5214 [49:47<3:18:11,  0.35it/s, v_num=3071, train_loss_step=0.160]batch: 1047 => Loss: 0.1947433203458786\n",
      "Epoch 0:  20%|██████████████████▋                                                                          | 1048/5214 [49:50<3:18:08,  0.35it/s, v_num=3071, train_loss_step=0.195]batch: 1048 => Loss: 0.13506963849067688\n",
      "Epoch 0:  20%|██████████████████▋                                                                          | 1049/5214 [49:53<3:18:05,  0.35it/s, v_num=3071, train_loss_step=0.135]batch: 1049 => Loss: 0.24491217732429504\n",
      "Epoch 0:  20%|██████████████████▋                                                                          | 1050/5214 [49:56<3:18:03,  0.35it/s, v_num=3071, train_loss_step=0.245]batch: 1050 => Loss: 0.143217071890831\n",
      "Epoch 0:  20%|██████████████████▋                                                                          | 1051/5214 [49:59<3:18:00,  0.35it/s, v_num=3071, train_loss_step=0.143]batch: 1051 => Loss: 0.13697750866413116\n",
      "Epoch 0:  20%|██████████████████▊                                                                          | 1052/5214 [50:02<3:17:57,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 1052 => Loss: 0.15299706161022186\n",
      "Epoch 0:  20%|██████████████████▊                                                                          | 1053/5214 [50:04<3:17:54,  0.35it/s, v_num=3071, train_loss_step=0.153]batch: 1053 => Loss: 0.18795689940452576\n",
      "Epoch 0:  20%|██████████████████▊                                                                          | 1054/5214 [50:07<3:17:51,  0.35it/s, v_num=3071, train_loss_step=0.188]batch: 1054 => Loss: 0.14384940266609192\n",
      "Epoch 0:  20%|██████████████████▊                                                                          | 1055/5214 [50:10<3:17:48,  0.35it/s, v_num=3071, train_loss_step=0.144]batch: 1055 => Loss: 0.18753306567668915\n",
      "Epoch 0:  20%|██████████████████▊                                                                          | 1056/5214 [50:13<3:17:45,  0.35it/s, v_num=3071, train_loss_step=0.188]batch: 1056 => Loss: 0.09422043710947037\n",
      "Epoch 0:  20%|██████████████████▋                                                                         | 1057/5214 [50:16<3:17:42,  0.35it/s, v_num=3071, train_loss_step=0.0942]batch: 1057 => Loss: 0.11811047047376633\n",
      "Epoch 0:  20%|██████████████████▊                                                                          | 1058/5214 [50:19<3:17:40,  0.35it/s, v_num=3071, train_loss_step=0.118]batch: 1058 => Loss: 0.13874755799770355\n",
      "Epoch 0:  20%|██████████████████▉                                                                          | 1059/5214 [50:22<3:17:37,  0.35it/s, v_num=3071, train_loss_step=0.139]batch: 1059 => Loss: 0.1679341346025467\n",
      "Epoch 0:  20%|██████████████████▉                                                                          | 1060/5214 [50:24<3:17:34,  0.35it/s, v_num=3071, train_loss_step=0.168]batch: 1060 => Loss: 0.2069733589887619\n",
      "Epoch 0:  20%|██████████████████▉                                                                          | 1061/5214 [50:27<3:17:31,  0.35it/s, v_num=3071, train_loss_step=0.207]batch: 1061 => Loss: 0.20313286781311035\n",
      "Epoch 0:  20%|██████████████████▉                                                                          | 1062/5214 [50:30<3:17:28,  0.35it/s, v_num=3071, train_loss_step=0.203]batch: 1062 => Loss: 0.13763415813446045\n",
      "Epoch 0:  20%|██████████████████▉                                                                          | 1063/5214 [50:33<3:17:25,  0.35it/s, v_num=3071, train_loss_step=0.138]batch: 1063 => Loss: 0.14060331881046295\n",
      "Epoch 0:  20%|██████████████████▉                                                                          | 1064/5214 [50:36<3:17:22,  0.35it/s, v_num=3071, train_loss_step=0.141]batch: 1064 => Loss: 0.16622254252433777\n",
      "Epoch 0:  20%|██████████████████▉                                                                          | 1065/5214 [50:39<3:17:20,  0.35it/s, v_num=3071, train_loss_step=0.166]batch: 1065 => Loss: 0.1720375269651413\n",
      "Epoch 0:  20%|███████████████████                                                                          | 1066/5214 [50:42<3:17:17,  0.35it/s, v_num=3071, train_loss_step=0.172]batch: 1066 => Loss: 0.19620156288146973\n",
      "Epoch 0:  20%|███████████████████                                                                          | 1067/5214 [50:44<3:17:14,  0.35it/s, v_num=3071, train_loss_step=0.196]batch: 1067 => Loss: 0.14503028988838196\n",
      "Epoch 0:  20%|███████████████████                                                                          | 1068/5214 [50:47<3:17:11,  0.35it/s, v_num=3071, train_loss_step=0.145]batch: 1068 => Loss: 0.17601299285888672\n",
      "Epoch 0:  21%|███████████████████                                                                          | 1069/5214 [50:50<3:17:08,  0.35it/s, v_num=3071, train_loss_step=0.176]batch: 1069 => Loss: 0.18594962358474731\n",
      "Epoch 0:  21%|███████████████████                                                                          | 1070/5214 [50:53<3:17:06,  0.35it/s, v_num=3071, train_loss_step=0.186]batch: 1070 => Loss: 0.1595567762851715\n",
      "Epoch 0:  21%|███████████████████                                                                          | 1071/5214 [50:56<3:17:03,  0.35it/s, v_num=3071, train_loss_step=0.160]batch: 1071 => Loss: 0.22708463668823242\n",
      "Epoch 0:  21%|███████████████████                                                                          | 1072/5214 [50:59<3:17:00,  0.35it/s, v_num=3071, train_loss_step=0.227]batch: 1072 => Loss: 0.16042564809322357\n",
      "Epoch 0:  21%|███████████████████▏                                                                         | 1073/5214 [51:02<3:16:57,  0.35it/s, v_num=3071, train_loss_step=0.160]batch: 1073 => Loss: 0.13742759823799133\n",
      "Epoch 0:  21%|███████████████████▏                                                                         | 1074/5214 [51:04<3:16:54,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 1074 => Loss: 0.15353597700595856\n",
      "Epoch 0:  21%|███████████████████▏                                                                         | 1075/5214 [51:07<3:16:51,  0.35it/s, v_num=3071, train_loss_step=0.154]batch: 1075 => Loss: 0.15575315058231354\n",
      "Epoch 0:  21%|███████████████████▏                                                                         | 1076/5214 [51:10<3:16:48,  0.35it/s, v_num=3071, train_loss_step=0.156]batch: 1076 => Loss: 0.18115662038326263\n",
      "Epoch 0:  21%|███████████████████▏                                                                         | 1077/5214 [51:13<3:16:46,  0.35it/s, v_num=3071, train_loss_step=0.181]batch: 1077 => Loss: 0.1590917557477951\n",
      "Epoch 0:  21%|███████████████████▏                                                                         | 1078/5214 [51:16<3:16:43,  0.35it/s, v_num=3071, train_loss_step=0.159]batch: 1078 => Loss: 0.15060792863368988\n",
      "Epoch 0:  21%|███████████████████▏                                                                         | 1079/5214 [51:19<3:16:40,  0.35it/s, v_num=3071, train_loss_step=0.151]batch: 1079 => Loss: 0.20435762405395508\n",
      "Epoch 0:  21%|███████████████████▎                                                                         | 1080/5214 [51:22<3:16:37,  0.35it/s, v_num=3071, train_loss_step=0.204]batch: 1080 => Loss: 0.1283680945634842\n",
      "Epoch 0:  21%|███████████████████▎                                                                         | 1081/5214 [51:24<3:16:34,  0.35it/s, v_num=3071, train_loss_step=0.128]batch: 1081 => Loss: 0.11513407528400421\n",
      "Epoch 0:  21%|███████████████████▎                                                                         | 1082/5214 [51:27<3:16:31,  0.35it/s, v_num=3071, train_loss_step=0.115]batch: 1082 => Loss: 0.15598304569721222\n",
      "Epoch 0:  21%|███████████████████▎                                                                         | 1083/5214 [51:30<3:16:28,  0.35it/s, v_num=3071, train_loss_step=0.156]batch: 1083 => Loss: 0.1596691608428955\n",
      "Epoch 0:  21%|███████████████████▎                                                                         | 1084/5214 [51:33<3:16:25,  0.35it/s, v_num=3071, train_loss_step=0.160]batch: 1084 => Loss: 0.11124332994222641\n",
      "Epoch 0:  21%|███████████████████▎                                                                         | 1085/5214 [51:36<3:16:23,  0.35it/s, v_num=3071, train_loss_step=0.111]batch: 1085 => Loss: 0.1225854679942131\n",
      "Epoch 0:  21%|███████████████████▎                                                                         | 1086/5214 [51:39<3:16:20,  0.35it/s, v_num=3071, train_loss_step=0.123]batch: 1086 => Loss: 0.14999686181545258\n",
      "Epoch 0:  21%|███████████████████▍                                                                         | 1087/5214 [51:42<3:16:17,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 1087 => Loss: 0.19302202761173248\n",
      "Epoch 0:  21%|███████████████████▍                                                                         | 1088/5214 [51:44<3:16:14,  0.35it/s, v_num=3071, train_loss_step=0.193]batch: 1088 => Loss: 0.18604443967342377\n",
      "Epoch 0:  21%|███████████████████▍                                                                         | 1089/5214 [51:47<3:16:11,  0.35it/s, v_num=3071, train_loss_step=0.186]batch: 1089 => Loss: 0.15650616586208344\n",
      "Epoch 0:  21%|███████████████████▍                                                                         | 1090/5214 [51:50<3:16:08,  0.35it/s, v_num=3071, train_loss_step=0.157]batch: 1090 => Loss: 0.08639080077409744\n",
      "Epoch 0:  21%|███████████████████▎                                                                        | 1091/5214 [51:53<3:16:05,  0.35it/s, v_num=3071, train_loss_step=0.0864]batch: 1091 => Loss: 0.2622233033180237\n",
      "Epoch 0:  21%|███████████████████▍                                                                         | 1092/5214 [51:56<3:16:02,  0.35it/s, v_num=3071, train_loss_step=0.262]batch: 1092 => Loss: 0.1461162120103836\n",
      "Epoch 0:  21%|███████████████████▍                                                                         | 1093/5214 [51:59<3:16:00,  0.35it/s, v_num=3071, train_loss_step=0.146]batch: 1093 => Loss: 0.23677435517311096\n",
      "Epoch 0:  21%|███████████████████▌                                                                         | 1094/5214 [52:01<3:15:57,  0.35it/s, v_num=3071, train_loss_step=0.237]batch: 1094 => Loss: 0.15589213371276855\n",
      "Epoch 0:  21%|███████████████████▌                                                                         | 1095/5214 [52:04<3:15:54,  0.35it/s, v_num=3071, train_loss_step=0.156]batch: 1095 => Loss: 0.15473495423793793\n",
      "Epoch 0:  21%|███████████████████▌                                                                         | 1096/5214 [52:07<3:15:51,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 1096 => Loss: 0.09199327975511551\n",
      "Epoch 0:  21%|███████████████████▌                                                                         | 1097/5214 [52:10<3:15:48,  0.35it/s, v_num=3071, train_loss_step=0.092]batch: 1097 => Loss: 0.16719003021717072\n",
      "Epoch 0:  21%|███████████████████▌                                                                         | 1098/5214 [52:13<3:15:45,  0.35it/s, v_num=3071, train_loss_step=0.167]batch: 1098 => Loss: 0.17104332149028778\n",
      "Epoch 0:  21%|███████████████████▌                                                                         | 1099/5214 [52:16<3:15:42,  0.35it/s, v_num=3071, train_loss_step=0.171]batch: 1099 => Loss: 0.23121435940265656\n",
      "Epoch 0:  21%|███████████████████▌                                                                         | 1100/5214 [52:19<3:15:40,  0.35it/s, v_num=3071, train_loss_step=0.231]batch: 1100 => Loss: 0.15028129518032074\n",
      "Epoch 0:  21%|███████████████████▋                                                                         | 1101/5214 [52:21<3:15:37,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 1101 => Loss: 0.09503161162137985\n",
      "Epoch 0:  21%|███████████████████▋                                                                         | 1102/5214 [52:24<3:15:34,  0.35it/s, v_num=3071, train_loss_step=0.095]batch: 1102 => Loss: 0.1260286420583725\n",
      "Epoch 0:  21%|███████████████████▋                                                                         | 1103/5214 [52:27<3:15:31,  0.35it/s, v_num=3071, train_loss_step=0.126]batch: 1103 => Loss: 0.1749618649482727\n",
      "Epoch 0:  21%|███████████████████▋                                                                         | 1104/5214 [52:30<3:15:28,  0.35it/s, v_num=3071, train_loss_step=0.175]batch: 1104 => Loss: 0.12358725070953369\n",
      "Epoch 0:  21%|███████████████████▋                                                                         | 1105/5214 [52:33<3:15:25,  0.35it/s, v_num=3071, train_loss_step=0.124]batch: 1105 => Loss: 0.09814837574958801\n",
      "Epoch 0:  21%|███████████████████▌                                                                        | 1106/5214 [52:36<3:15:22,  0.35it/s, v_num=3071, train_loss_step=0.0981]batch: 1106 => Loss: 0.1414124220609665\n",
      "Epoch 0:  21%|███████████████████▋                                                                         | 1107/5214 [52:39<3:15:20,  0.35it/s, v_num=3071, train_loss_step=0.141]batch: 1107 => Loss: 0.16427695751190186\n",
      "Epoch 0:  21%|███████████████████▊                                                                         | 1108/5214 [52:41<3:15:17,  0.35it/s, v_num=3071, train_loss_step=0.164]batch: 1108 => Loss: 0.15454673767089844\n",
      "Epoch 0:  21%|███████████████████▊                                                                         | 1109/5214 [52:44<3:15:14,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 1109 => Loss: 0.14960680902004242\n",
      "Epoch 0:  21%|███████████████████▊                                                                         | 1110/5214 [52:47<3:15:11,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 1110 => Loss: 0.14073045551776886\n",
      "Epoch 0:  21%|███████████████████▊                                                                         | 1111/5214 [52:50<3:15:08,  0.35it/s, v_num=3071, train_loss_step=0.141]batch: 1111 => Loss: 0.14443275332450867\n",
      "Epoch 0:  21%|███████████████████▊                                                                         | 1112/5214 [52:53<3:15:05,  0.35it/s, v_num=3071, train_loss_step=0.144]batch: 1112 => Loss: 0.12300743162631989\n",
      "Epoch 0:  21%|███████████████████▊                                                                         | 1113/5214 [52:56<3:15:02,  0.35it/s, v_num=3071, train_loss_step=0.123]batch: 1113 => Loss: 0.2060420960187912\n",
      "Epoch 0:  21%|███████████████████▊                                                                         | 1114/5214 [52:58<3:15:00,  0.35it/s, v_num=3071, train_loss_step=0.206]batch: 1114 => Loss: 0.21484152972698212\n",
      "Epoch 0:  21%|███████████████████▉                                                                         | 1115/5214 [53:01<3:14:57,  0.35it/s, v_num=3071, train_loss_step=0.215]batch: 1115 => Loss: 0.16900449991226196\n",
      "Epoch 0:  21%|███████████████████▉                                                                         | 1116/5214 [53:04<3:14:54,  0.35it/s, v_num=3071, train_loss_step=0.169]batch: 1116 => Loss: 0.14496885240077972\n",
      "Epoch 0:  21%|███████████████████▉                                                                         | 1117/5214 [53:07<3:14:51,  0.35it/s, v_num=3071, train_loss_step=0.145]batch: 1117 => Loss: 0.12325916439294815\n",
      "Epoch 0:  21%|███████████████████▉                                                                         | 1118/5214 [53:10<3:14:48,  0.35it/s, v_num=3071, train_loss_step=0.123]batch: 1118 => Loss: 0.23059940338134766\n",
      "Epoch 0:  21%|███████████████████▉                                                                         | 1119/5214 [53:13<3:14:45,  0.35it/s, v_num=3071, train_loss_step=0.231]batch: 1119 => Loss: 0.15280166268348694\n",
      "Epoch 0:  21%|███████████████████▉                                                                         | 1120/5214 [53:16<3:14:42,  0.35it/s, v_num=3071, train_loss_step=0.153]batch: 1120 => Loss: 0.15864615142345428\n",
      "Epoch 0:  21%|███████████████████▉                                                                         | 1121/5214 [53:18<3:14:39,  0.35it/s, v_num=3071, train_loss_step=0.159]batch: 1121 => Loss: 0.1373872607946396\n",
      "Epoch 0:  22%|████████████████████                                                                         | 1122/5214 [53:21<3:14:36,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 1122 => Loss: 0.19059497117996216\n",
      "Epoch 0:  22%|████████████████████                                                                         | 1123/5214 [53:24<3:14:34,  0.35it/s, v_num=3071, train_loss_step=0.191]batch: 1123 => Loss: 0.10030566900968552\n",
      "Epoch 0:  22%|████████████████████                                                                         | 1124/5214 [53:27<3:14:31,  0.35it/s, v_num=3071, train_loss_step=0.100]batch: 1124 => Loss: 0.19207482039928436\n",
      "Epoch 0:  22%|████████████████████                                                                         | 1125/5214 [53:30<3:14:28,  0.35it/s, v_num=3071, train_loss_step=0.192]batch: 1125 => Loss: 0.16308341920375824\n",
      "Epoch 0:  22%|████████████████████                                                                         | 1126/5214 [53:33<3:14:25,  0.35it/s, v_num=3071, train_loss_step=0.163]batch: 1126 => Loss: 0.14606820046901703\n",
      "Epoch 0:  22%|████████████████████                                                                         | 1127/5214 [53:36<3:14:22,  0.35it/s, v_num=3071, train_loss_step=0.146]batch: 1127 => Loss: 0.20704369246959686\n",
      "Epoch 0:  22%|████████████████████                                                                         | 1128/5214 [53:38<3:14:19,  0.35it/s, v_num=3071, train_loss_step=0.207]batch: 1128 => Loss: 0.15277060866355896\n",
      "Epoch 0:  22%|████████████████████▏                                                                        | 1129/5214 [53:41<3:14:17,  0.35it/s, v_num=3071, train_loss_step=0.153]batch: 1129 => Loss: 0.11957760900259018\n",
      "Epoch 0:  22%|████████████████████▏                                                                        | 1130/5214 [53:44<3:14:14,  0.35it/s, v_num=3071, train_loss_step=0.120]batch: 1130 => Loss: 0.19276396930217743\n",
      "Epoch 0:  22%|████████████████████▏                                                                        | 1131/5214 [53:47<3:14:11,  0.35it/s, v_num=3071, train_loss_step=0.193]batch: 1131 => Loss: 0.11971590667963028\n",
      "Epoch 0:  22%|████████████████████▏                                                                        | 1132/5214 [53:50<3:14:08,  0.35it/s, v_num=3071, train_loss_step=0.120]batch: 1132 => Loss: 0.15407179296016693\n",
      "Epoch 0:  22%|████████████████████▏                                                                        | 1133/5214 [53:53<3:14:05,  0.35it/s, v_num=3071, train_loss_step=0.154]batch: 1133 => Loss: 0.10692650079727173\n",
      "Epoch 0:  22%|████████████████████▏                                                                        | 1134/5214 [53:56<3:14:02,  0.35it/s, v_num=3071, train_loss_step=0.107]batch: 1134 => Loss: 0.18680059909820557\n",
      "Epoch 0:  22%|████████████████████▏                                                                        | 1135/5214 [53:58<3:13:59,  0.35it/s, v_num=3071, train_loss_step=0.187]batch: 1135 => Loss: 0.14852648973464966\n",
      "Epoch 0:  22%|████████████████████▎                                                                        | 1136/5214 [54:01<3:13:57,  0.35it/s, v_num=3071, train_loss_step=0.149]batch: 1136 => Loss: 0.1917245239019394\n",
      "Epoch 0:  22%|████████████████████▎                                                                        | 1137/5214 [54:04<3:13:54,  0.35it/s, v_num=3071, train_loss_step=0.192]batch: 1137 => Loss: 0.15697860717773438\n",
      "Epoch 0:  22%|████████████████████▎                                                                        | 1138/5214 [54:07<3:13:51,  0.35it/s, v_num=3071, train_loss_step=0.157]batch: 1138 => Loss: 0.12240546196699142\n",
      "Epoch 0:  22%|████████████████████▎                                                                        | 1139/5214 [54:10<3:13:48,  0.35it/s, v_num=3071, train_loss_step=0.122]batch: 1139 => Loss: 0.1480991244316101\n",
      "Epoch 0:  22%|████████████████████▎                                                                        | 1140/5214 [54:13<3:13:45,  0.35it/s, v_num=3071, train_loss_step=0.148]batch: 1140 => Loss: 0.1761152148246765\n",
      "Epoch 0:  22%|████████████████████▎                                                                        | 1141/5214 [54:15<3:13:42,  0.35it/s, v_num=3071, train_loss_step=0.176]batch: 1141 => Loss: 0.1291758418083191\n",
      "Epoch 0:  22%|████████████████████▎                                                                        | 1142/5214 [54:18<3:13:39,  0.35it/s, v_num=3071, train_loss_step=0.129]batch: 1142 => Loss: 0.12087833136320114\n",
      "Epoch 0:  22%|████████████████████▍                                                                        | 1143/5214 [54:21<3:13:37,  0.35it/s, v_num=3071, train_loss_step=0.121]batch: 1143 => Loss: 0.18944118916988373\n",
      "Epoch 0:  22%|████████████████████▍                                                                        | 1144/5214 [54:24<3:13:34,  0.35it/s, v_num=3071, train_loss_step=0.189]batch: 1144 => Loss: 0.14214354753494263\n",
      "Epoch 0:  22%|████████████████████▍                                                                        | 1145/5214 [54:27<3:13:31,  0.35it/s, v_num=3071, train_loss_step=0.142]batch: 1145 => Loss: 0.14841072261333466\n",
      "Epoch 0:  22%|████████████████████▍                                                                        | 1146/5214 [54:30<3:13:28,  0.35it/s, v_num=3071, train_loss_step=0.148]batch: 1146 => Loss: 0.235452800989151\n",
      "Epoch 0:  22%|████████████████████▍                                                                        | 1147/5214 [54:33<3:13:25,  0.35it/s, v_num=3071, train_loss_step=0.235]batch: 1147 => Loss: 0.19168096780776978\n",
      "Epoch 0:  22%|████████████████████▍                                                                        | 1148/5214 [54:36<3:13:23,  0.35it/s, v_num=3071, train_loss_step=0.192]batch: 1148 => Loss: 0.11926877498626709\n",
      "Epoch 0:  22%|████████████████████▍                                                                        | 1149/5214 [54:38<3:13:20,  0.35it/s, v_num=3071, train_loss_step=0.119]batch: 1149 => Loss: 0.11557306349277496\n",
      "Epoch 0:  22%|████████████████████▌                                                                        | 1150/5214 [54:41<3:13:17,  0.35it/s, v_num=3071, train_loss_step=0.116]batch: 1150 => Loss: 0.13450144231319427\n",
      "Epoch 0:  22%|████████████████████▌                                                                        | 1151/5214 [54:44<3:13:14,  0.35it/s, v_num=3071, train_loss_step=0.135]batch: 1151 => Loss: 0.1590181142091751\n",
      "Epoch 0:  22%|████████████████████▌                                                                        | 1152/5214 [54:47<3:13:11,  0.35it/s, v_num=3071, train_loss_step=0.159]batch: 1152 => Loss: 0.17862568795681\n",
      "Epoch 0:  22%|████████████████████▌                                                                        | 1153/5214 [54:50<3:13:09,  0.35it/s, v_num=3071, train_loss_step=0.179]batch: 1153 => Loss: 0.2505382001399994\n",
      "Epoch 0:  22%|████████████████████▌                                                                        | 1154/5214 [54:53<3:13:06,  0.35it/s, v_num=3071, train_loss_step=0.251]batch: 1154 => Loss: 0.25746503472328186\n",
      "Epoch 0:  22%|████████████████████▌                                                                        | 1155/5214 [54:56<3:13:03,  0.35it/s, v_num=3071, train_loss_step=0.257]batch: 1155 => Loss: 0.1398349553346634\n",
      "Epoch 0:  22%|████████████████████▌                                                                        | 1156/5214 [54:58<3:13:00,  0.35it/s, v_num=3071, train_loss_step=0.140]batch: 1156 => Loss: 0.1372707486152649\n",
      "Epoch 0:  22%|████████████████████▋                                                                        | 1157/5214 [55:01<3:12:57,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 1157 => Loss: 0.13953061401844025\n",
      "Epoch 0:  22%|████████████████████▋                                                                        | 1158/5214 [55:04<3:12:54,  0.35it/s, v_num=3071, train_loss_step=0.140]batch: 1158 => Loss: 0.10819441080093384\n",
      "Epoch 0:  22%|████████████████████▋                                                                        | 1159/5214 [55:07<3:12:51,  0.35it/s, v_num=3071, train_loss_step=0.108]batch: 1159 => Loss: 0.16684870421886444\n",
      "Epoch 0:  22%|████████████████████▋                                                                        | 1160/5214 [55:10<3:12:49,  0.35it/s, v_num=3071, train_loss_step=0.167]batch: 1160 => Loss: 0.15633834898471832\n",
      "Epoch 0:  22%|████████████████████▋                                                                        | 1161/5214 [55:13<3:12:46,  0.35it/s, v_num=3071, train_loss_step=0.156]batch: 1161 => Loss: 0.16664302349090576\n",
      "Epoch 0:  22%|████████████████████▋                                                                        | 1162/5214 [55:16<3:12:43,  0.35it/s, v_num=3071, train_loss_step=0.167]batch: 1162 => Loss: 0.18166141211986542\n",
      "Epoch 0:  22%|████████████████████▋                                                                        | 1163/5214 [55:18<3:12:40,  0.35it/s, v_num=3071, train_loss_step=0.182]batch: 1163 => Loss: 0.20839424431324005\n",
      "Epoch 0:  22%|████████████████████▊                                                                        | 1164/5214 [55:21<3:12:37,  0.35it/s, v_num=3071, train_loss_step=0.208]batch: 1164 => Loss: 0.24904261529445648\n",
      "Epoch 0:  22%|████████████████████▊                                                                        | 1165/5214 [55:24<3:12:34,  0.35it/s, v_num=3071, train_loss_step=0.249]batch: 1165 => Loss: 0.146590456366539\n",
      "Epoch 0:  22%|████████████████████▊                                                                        | 1166/5214 [55:27<3:12:31,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 1166 => Loss: 0.11113549768924713\n",
      "Epoch 0:  22%|████████████████████▊                                                                        | 1167/5214 [55:30<3:12:29,  0.35it/s, v_num=3071, train_loss_step=0.111]batch: 1167 => Loss: 0.09482844918966293\n",
      "Epoch 0:  22%|████████████████████▌                                                                       | 1168/5214 [55:33<3:12:26,  0.35it/s, v_num=3071, train_loss_step=0.0948]batch: 1168 => Loss: 0.10625240951776505\n",
      "Epoch 0:  22%|████████████████████▊                                                                        | 1169/5214 [55:36<3:12:23,  0.35it/s, v_num=3071, train_loss_step=0.106]batch: 1169 => Loss: 0.18156445026397705\n",
      "Epoch 0:  22%|████████████████████▊                                                                        | 1170/5214 [55:38<3:12:20,  0.35it/s, v_num=3071, train_loss_step=0.182]batch: 1170 => Loss: 0.16458852589130402\n",
      "Epoch 0:  22%|████████████████████▉                                                                        | 1171/5214 [55:41<3:12:17,  0.35it/s, v_num=3071, train_loss_step=0.165]batch: 1171 => Loss: 0.20394682884216309\n",
      "Epoch 0:  22%|████████████████████▉                                                                        | 1172/5214 [55:44<3:12:14,  0.35it/s, v_num=3071, train_loss_step=0.204]batch: 1172 => Loss: 0.16690950095653534\n",
      "Epoch 0:  22%|████████████████████▉                                                                        | 1173/5214 [55:47<3:12:11,  0.35it/s, v_num=3071, train_loss_step=0.167]batch: 1173 => Loss: 0.1496909111738205\n",
      "Epoch 0:  23%|████████████████████▉                                                                        | 1174/5214 [55:50<3:12:08,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 1174 => Loss: 0.09240145236253738\n",
      "Epoch 0:  23%|████████████████████▋                                                                       | 1175/5214 [55:53<3:12:06,  0.35it/s, v_num=3071, train_loss_step=0.0924]batch: 1175 => Loss: 0.07185999304056168\n",
      "Epoch 0:  23%|████████████████████▊                                                                       | 1176/5214 [55:55<3:12:03,  0.35it/s, v_num=3071, train_loss_step=0.0719]batch: 1176 => Loss: 0.14516746997833252\n",
      "Epoch 0:  23%|████████████████████▉                                                                        | 1177/5214 [55:58<3:12:00,  0.35it/s, v_num=3071, train_loss_step=0.145]batch: 1177 => Loss: 0.1978112906217575\n",
      "Epoch 0:  23%|█████████████████████                                                                        | 1178/5214 [56:01<3:11:57,  0.35it/s, v_num=3071, train_loss_step=0.198]batch: 1178 => Loss: 0.16513602435588837\n",
      "Epoch 0:  23%|█████████████████████                                                                        | 1179/5214 [56:04<3:11:54,  0.35it/s, v_num=3071, train_loss_step=0.165]batch: 1179 => Loss: 0.15550275146961212\n",
      "Epoch 0:  23%|█████████████████████                                                                        | 1180/5214 [56:07<3:11:51,  0.35it/s, v_num=3071, train_loss_step=0.156]batch: 1180 => Loss: 0.15494266152381897\n",
      "Epoch 0:  23%|█████████████████████                                                                        | 1181/5214 [56:10<3:11:49,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 1181 => Loss: 0.17977644503116608\n",
      "Epoch 0:  23%|█████████████████████                                                                        | 1182/5214 [56:13<3:11:46,  0.35it/s, v_num=3071, train_loss_step=0.180]batch: 1182 => Loss: 0.14848065376281738\n",
      "Epoch 0:  23%|█████████████████████                                                                        | 1183/5214 [56:15<3:11:43,  0.35it/s, v_num=3071, train_loss_step=0.148]batch: 1183 => Loss: 0.1780003011226654\n",
      "Epoch 0:  23%|█████████████████████                                                                        | 1184/5214 [56:18<3:11:40,  0.35it/s, v_num=3071, train_loss_step=0.178]batch: 1184 => Loss: 0.1878427416086197\n",
      "Epoch 0:  23%|█████████████████████▏                                                                       | 1185/5214 [56:21<3:11:37,  0.35it/s, v_num=3071, train_loss_step=0.188]batch: 1185 => Loss: 0.15796710550785065\n",
      "Epoch 0:  23%|█████████████████████▏                                                                       | 1186/5214 [56:24<3:11:34,  0.35it/s, v_num=3071, train_loss_step=0.158]batch: 1186 => Loss: 0.1675630807876587\n",
      "Epoch 0:  23%|█████████████████████▏                                                                       | 1187/5214 [56:27<3:11:31,  0.35it/s, v_num=3071, train_loss_step=0.168]batch: 1187 => Loss: 0.1258648782968521\n",
      "Epoch 0:  23%|█████████████████████▏                                                                       | 1188/5214 [56:30<3:11:29,  0.35it/s, v_num=3071, train_loss_step=0.126]batch: 1188 => Loss: 0.11978542804718018\n",
      "Epoch 0:  23%|█████████████████████▏                                                                       | 1189/5214 [56:33<3:11:26,  0.35it/s, v_num=3071, train_loss_step=0.120]batch: 1189 => Loss: 0.15522317588329315\n",
      "Epoch 0:  23%|█████████████████████▏                                                                       | 1190/5214 [56:35<3:11:23,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 1190 => Loss: 0.12684479355812073\n",
      "Epoch 0:  23%|█████████████████████▏                                                                       | 1191/5214 [56:38<3:11:20,  0.35it/s, v_num=3071, train_loss_step=0.127]batch: 1191 => Loss: 0.19801123440265656\n",
      "Epoch 0:  23%|█████████████████████▎                                                                       | 1192/5214 [56:41<3:11:17,  0.35it/s, v_num=3071, train_loss_step=0.198]batch: 1192 => Loss: 0.15112674236297607\n",
      "Epoch 0:  23%|█████████████████████▎                                                                       | 1193/5214 [56:44<3:11:14,  0.35it/s, v_num=3071, train_loss_step=0.151]batch: 1193 => Loss: 0.13631971180438995\n",
      "Epoch 0:  23%|█████████████████████▎                                                                       | 1194/5214 [56:47<3:11:11,  0.35it/s, v_num=3071, train_loss_step=0.136]batch: 1194 => Loss: 0.16955558955669403\n",
      "Epoch 0:  23%|█████████████████████▎                                                                       | 1195/5214 [56:50<3:11:08,  0.35it/s, v_num=3071, train_loss_step=0.170]batch: 1195 => Loss: 0.13155193626880646\n",
      "Epoch 0:  23%|█████████████████████▎                                                                       | 1196/5214 [56:53<3:11:06,  0.35it/s, v_num=3071, train_loss_step=0.132]batch: 1196 => Loss: 0.11498836427927017\n",
      "Epoch 0:  23%|█████████████████████▎                                                                       | 1197/5214 [56:55<3:11:03,  0.35it/s, v_num=3071, train_loss_step=0.115]batch: 1197 => Loss: 0.07212510704994202\n",
      "Epoch 0:  23%|█████████████████████▏                                                                      | 1198/5214 [56:58<3:11:00,  0.35it/s, v_num=3071, train_loss_step=0.0721]batch: 1198 => Loss: 0.1880861073732376\n",
      "Epoch 0:  23%|█████████████████████▍                                                                       | 1199/5214 [57:01<3:10:57,  0.35it/s, v_num=3071, train_loss_step=0.188]batch: 1199 => Loss: 0.13420619070529938\n",
      "Epoch 0:  23%|█████████████████████▍                                                                       | 1200/5214 [57:04<3:10:54,  0.35it/s, v_num=3071, train_loss_step=0.134]batch: 1200 => Loss: 0.17401351034641266\n",
      "Epoch 0:  23%|█████████████████████▍                                                                       | 1201/5214 [57:07<3:10:51,  0.35it/s, v_num=3071, train_loss_step=0.174]batch: 1201 => Loss: 0.15970578789710999\n",
      "Epoch 0:  23%|█████████████████████▍                                                                       | 1202/5214 [57:10<3:10:49,  0.35it/s, v_num=3071, train_loss_step=0.160]batch: 1202 => Loss: 0.23895709216594696\n",
      "Epoch 0:  23%|█████████████████████▍                                                                       | 1203/5214 [57:13<3:10:46,  0.35it/s, v_num=3071, train_loss_step=0.239]batch: 1203 => Loss: 0.17512087523937225\n",
      "Epoch 0:  23%|█████████████████████▍                                                                       | 1204/5214 [57:15<3:10:43,  0.35it/s, v_num=3071, train_loss_step=0.175]batch: 1204 => Loss: 0.16914865374565125\n",
      "Epoch 0:  23%|█████████████████████▍                                                                       | 1205/5214 [57:18<3:10:40,  0.35it/s, v_num=3071, train_loss_step=0.169]batch: 1205 => Loss: 0.2308356761932373\n",
      "Epoch 0:  23%|█████████████████████▌                                                                       | 1206/5214 [57:21<3:10:37,  0.35it/s, v_num=3071, train_loss_step=0.231]batch: 1206 => Loss: 0.2348596602678299\n",
      "Epoch 0:  23%|█████████████████████▌                                                                       | 1207/5214 [57:24<3:10:34,  0.35it/s, v_num=3071, train_loss_step=0.235]batch: 1207 => Loss: 0.15253137052059174\n",
      "Epoch 0:  23%|█████████████████████▌                                                                       | 1208/5214 [57:27<3:10:31,  0.35it/s, v_num=3071, train_loss_step=0.153]batch: 1208 => Loss: 0.2205154150724411\n",
      "Epoch 0:  23%|█████████████████████▌                                                                       | 1209/5214 [57:30<3:10:29,  0.35it/s, v_num=3071, train_loss_step=0.221]batch: 1209 => Loss: 0.15905623137950897\n",
      "Epoch 0:  23%|█████████████████████▌                                                                       | 1210/5214 [57:32<3:10:26,  0.35it/s, v_num=3071, train_loss_step=0.159]batch: 1210 => Loss: 0.12872643768787384\n",
      "Epoch 0:  23%|█████████████████████▌                                                                       | 1211/5214 [57:35<3:10:23,  0.35it/s, v_num=3071, train_loss_step=0.129]batch: 1211 => Loss: 0.210135817527771\n",
      "Epoch 0:  23%|█████████████████████▌                                                                       | 1212/5214 [57:38<3:10:20,  0.35it/s, v_num=3071, train_loss_step=0.210]batch: 1212 => Loss: 0.20356671512126923\n",
      "Epoch 0:  23%|█████████████████████▋                                                                       | 1213/5214 [57:41<3:10:17,  0.35it/s, v_num=3071, train_loss_step=0.204]batch: 1213 => Loss: 0.1498655527830124\n",
      "Epoch 0:  23%|█████████████████████▋                                                                       | 1214/5214 [57:44<3:10:14,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 1214 => Loss: 0.09896429628133774\n",
      "Epoch 0:  23%|█████████████████████▋                                                                       | 1215/5214 [57:47<3:10:12,  0.35it/s, v_num=3071, train_loss_step=0.099]batch: 1215 => Loss: 0.16802971065044403\n",
      "Epoch 0:  23%|█████████████████████▋                                                                       | 1216/5214 [57:50<3:10:09,  0.35it/s, v_num=3071, train_loss_step=0.168]batch: 1216 => Loss: 0.13184970617294312\n",
      "Epoch 0:  23%|█████████████████████▋                                                                       | 1217/5214 [57:52<3:10:06,  0.35it/s, v_num=3071, train_loss_step=0.132]batch: 1217 => Loss: 0.136469766497612\n",
      "Epoch 0:  23%|█████████████████████▋                                                                       | 1218/5214 [57:55<3:10:03,  0.35it/s, v_num=3071, train_loss_step=0.136]batch: 1218 => Loss: 0.17598412930965424\n",
      "Epoch 0:  23%|█████████████████████▋                                                                       | 1219/5214 [57:58<3:10:00,  0.35it/s, v_num=3071, train_loss_step=0.176]batch: 1219 => Loss: 0.2088140994310379\n",
      "Epoch 0:  23%|█████████████████████▊                                                                       | 1220/5214 [58:01<3:09:57,  0.35it/s, v_num=3071, train_loss_step=0.209]batch: 1220 => Loss: 0.13080613315105438\n",
      "Epoch 0:  23%|█████████████████████▊                                                                       | 1221/5214 [58:04<3:09:55,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 1221 => Loss: 0.13892588019371033\n",
      "Epoch 0:  23%|█████████████████████▊                                                                       | 1222/5214 [58:07<3:09:52,  0.35it/s, v_num=3071, train_loss_step=0.139]batch: 1222 => Loss: 0.11832179874181747\n",
      "Epoch 0:  23%|█████████████████████▊                                                                       | 1223/5214 [58:10<3:09:49,  0.35it/s, v_num=3071, train_loss_step=0.118]batch: 1223 => Loss: 0.13026678562164307\n",
      "Epoch 0:  23%|█████████████████████▊                                                                       | 1224/5214 [58:12<3:09:46,  0.35it/s, v_num=3071, train_loss_step=0.130]batch: 1224 => Loss: 0.08458217978477478\n",
      "Epoch 0:  23%|█████████████████████▌                                                                      | 1225/5214 [58:15<3:09:43,  0.35it/s, v_num=3071, train_loss_step=0.0846]batch: 1225 => Loss: 0.14233019948005676\n",
      "Epoch 0:  24%|█████████████████████▊                                                                       | 1226/5214 [58:18<3:09:40,  0.35it/s, v_num=3071, train_loss_step=0.142]batch: 1226 => Loss: 0.13080179691314697\n",
      "Epoch 0:  24%|█████████████████████▉                                                                       | 1227/5214 [58:21<3:09:37,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 1227 => Loss: 0.17520716786384583\n",
      "Epoch 0:  24%|█████████████████████▉                                                                       | 1228/5214 [58:24<3:09:34,  0.35it/s, v_num=3071, train_loss_step=0.175]batch: 1228 => Loss: 0.1606488972902298\n",
      "Epoch 0:  24%|█████████████████████▉                                                                       | 1229/5214 [58:27<3:09:32,  0.35it/s, v_num=3071, train_loss_step=0.161]batch: 1229 => Loss: 0.17888937890529633\n",
      "Epoch 0:  24%|█████████████████████▉                                                                       | 1230/5214 [58:30<3:09:29,  0.35it/s, v_num=3071, train_loss_step=0.179]batch: 1230 => Loss: 0.15955059230327606\n",
      "Epoch 0:  24%|█████████████████████▉                                                                       | 1231/5214 [58:32<3:09:26,  0.35it/s, v_num=3071, train_loss_step=0.160]batch: 1231 => Loss: 0.12275034189224243\n",
      "Epoch 0:  24%|█████████████████████▉                                                                       | 1232/5214 [58:35<3:09:23,  0.35it/s, v_num=3071, train_loss_step=0.123]batch: 1232 => Loss: 0.22148370742797852\n",
      "Epoch 0:  24%|█████████████████████▉                                                                       | 1233/5214 [58:38<3:09:20,  0.35it/s, v_num=3071, train_loss_step=0.221]batch: 1233 => Loss: 0.17124409973621368\n",
      "Epoch 0:  24%|██████████████████████                                                                       | 1234/5214 [58:41<3:09:17,  0.35it/s, v_num=3071, train_loss_step=0.171]batch: 1234 => Loss: 0.17729531228542328\n",
      "Epoch 0:  24%|██████████████████████                                                                       | 1235/5214 [58:44<3:09:15,  0.35it/s, v_num=3071, train_loss_step=0.177]batch: 1235 => Loss: 0.10652432590723038\n",
      "Epoch 0:  24%|██████████████████████                                                                       | 1236/5214 [58:47<3:09:12,  0.35it/s, v_num=3071, train_loss_step=0.107]batch: 1236 => Loss: 0.15096446871757507\n",
      "Epoch 0:  24%|██████████████████████                                                                       | 1237/5214 [58:50<3:09:09,  0.35it/s, v_num=3071, train_loss_step=0.151]batch: 1237 => Loss: 0.1594170331954956\n",
      "Epoch 0:  24%|██████████████████████                                                                       | 1238/5214 [58:52<3:09:06,  0.35it/s, v_num=3071, train_loss_step=0.159]batch: 1238 => Loss: 0.2664870023727417\n",
      "Epoch 0:  24%|██████████████████████                                                                       | 1239/5214 [58:55<3:09:03,  0.35it/s, v_num=3071, train_loss_step=0.266]batch: 1239 => Loss: 0.20784644782543182\n",
      "Epoch 0:  24%|██████████████████████                                                                       | 1240/5214 [58:58<3:09:00,  0.35it/s, v_num=3071, train_loss_step=0.208]batch: 1240 => Loss: 0.1494847536087036\n",
      "Epoch 0:  24%|██████████████████████▏                                                                      | 1241/5214 [59:01<3:08:57,  0.35it/s, v_num=3071, train_loss_step=0.149]batch: 1241 => Loss: 0.14318744838237762\n",
      "Epoch 0:  24%|██████████████████████▏                                                                      | 1242/5214 [59:04<3:08:55,  0.35it/s, v_num=3071, train_loss_step=0.143]batch: 1242 => Loss: 0.14727340638637543\n",
      "Epoch 0:  24%|██████████████████████▏                                                                      | 1243/5214 [59:07<3:08:52,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 1243 => Loss: 0.14323821663856506\n",
      "Epoch 0:  24%|██████████████████████▏                                                                      | 1244/5214 [59:10<3:08:49,  0.35it/s, v_num=3071, train_loss_step=0.143]batch: 1244 => Loss: 0.17401526868343353\n",
      "Epoch 0:  24%|██████████████████████▏                                                                      | 1245/5214 [59:12<3:08:46,  0.35it/s, v_num=3071, train_loss_step=0.174]batch: 1245 => Loss: 0.18894337117671967\n",
      "Epoch 0:  24%|██████████████████████▏                                                                      | 1246/5214 [59:15<3:08:43,  0.35it/s, v_num=3071, train_loss_step=0.189]batch: 1246 => Loss: 0.12809915840625763\n",
      "Epoch 0:  24%|██████████████████████▏                                                                      | 1247/5214 [59:18<3:08:40,  0.35it/s, v_num=3071, train_loss_step=0.128]batch: 1247 => Loss: 0.18999464809894562\n",
      "Epoch 0:  24%|██████████████████████▎                                                                      | 1248/5214 [59:21<3:08:37,  0.35it/s, v_num=3071, train_loss_step=0.190]batch: 1248 => Loss: 0.13521941006183624\n",
      "Epoch 0:  24%|██████████████████████▎                                                                      | 1249/5214 [59:24<3:08:35,  0.35it/s, v_num=3071, train_loss_step=0.135]batch: 1249 => Loss: 0.15841275453567505\n",
      "Epoch 0:  24%|██████████████████████▎                                                                      | 1250/5214 [59:27<3:08:32,  0.35it/s, v_num=3071, train_loss_step=0.158]batch: 1250 => Loss: 0.13705894351005554\n",
      "Epoch 0:  24%|██████████████████████▎                                                                      | 1251/5214 [59:30<3:08:29,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 1251 => Loss: 0.23870234191417694\n",
      "Epoch 0:  24%|██████████████████████▎                                                                      | 1252/5214 [59:32<3:08:26,  0.35it/s, v_num=3071, train_loss_step=0.239]batch: 1252 => Loss: 0.18158966302871704\n",
      "Epoch 0:  24%|██████████████████████▎                                                                      | 1253/5214 [59:35<3:08:23,  0.35it/s, v_num=3071, train_loss_step=0.182]batch: 1253 => Loss: 0.14186836779117584\n",
      "Epoch 0:  24%|██████████████████████▎                                                                      | 1254/5214 [59:38<3:08:20,  0.35it/s, v_num=3071, train_loss_step=0.142]batch: 1254 => Loss: 0.2683480679988861\n",
      "Epoch 0:  24%|██████████████████████▍                                                                      | 1255/5214 [59:41<3:08:17,  0.35it/s, v_num=3071, train_loss_step=0.268]batch: 1255 => Loss: 0.14340193569660187\n",
      "Epoch 0:  24%|██████████████████████▍                                                                      | 1256/5214 [59:44<3:08:14,  0.35it/s, v_num=3071, train_loss_step=0.143]batch: 1256 => Loss: 0.1490975171327591\n",
      "Epoch 0:  24%|██████████████████████▍                                                                      | 1257/5214 [59:47<3:08:12,  0.35it/s, v_num=3071, train_loss_step=0.149]batch: 1257 => Loss: 0.14499294757843018\n",
      "Epoch 0:  24%|██████████████████████▍                                                                      | 1258/5214 [59:50<3:08:09,  0.35it/s, v_num=3071, train_loss_step=0.145]batch: 1258 => Loss: 0.19517898559570312\n",
      "Epoch 0:  24%|██████████████████████▍                                                                      | 1259/5214 [59:52<3:08:06,  0.35it/s, v_num=3071, train_loss_step=0.195]batch: 1259 => Loss: 0.12138509750366211\n",
      "Epoch 0:  24%|██████████████████████▍                                                                      | 1260/5214 [59:55<3:08:03,  0.35it/s, v_num=3071, train_loss_step=0.121]batch: 1260 => Loss: 0.17800746858119965\n",
      "Epoch 0:  24%|██████████████████████▍                                                                      | 1261/5214 [59:58<3:08:00,  0.35it/s, v_num=3071, train_loss_step=0.178]batch: 1261 => Loss: 0.12995301187038422\n",
      "Epoch 0:  24%|██████████████████████                                                                     | 1262/5214 [1:00:01<3:07:58,  0.35it/s, v_num=3071, train_loss_step=0.130]batch: 1262 => Loss: 0.20714834332466125\n",
      "Epoch 0:  24%|██████████████████████                                                                     | 1263/5214 [1:00:04<3:07:55,  0.35it/s, v_num=3071, train_loss_step=0.207]batch: 1263 => Loss: 0.11610256880521774\n",
      "Epoch 0:  24%|██████████████████████                                                                     | 1264/5214 [1:00:07<3:07:52,  0.35it/s, v_num=3071, train_loss_step=0.116]batch: 1264 => Loss: 0.10769837349653244\n",
      "Epoch 0:  24%|██████████████████████                                                                     | 1265/5214 [1:00:09<3:07:49,  0.35it/s, v_num=3071, train_loss_step=0.108]batch: 1265 => Loss: 0.12246965616941452\n",
      "Epoch 0:  24%|██████████████████████                                                                     | 1266/5214 [1:00:12<3:07:46,  0.35it/s, v_num=3071, train_loss_step=0.122]batch: 1266 => Loss: 0.14942488074302673\n",
      "Epoch 0:  24%|██████████████████████                                                                     | 1267/5214 [1:00:15<3:07:43,  0.35it/s, v_num=3071, train_loss_step=0.149]batch: 1267 => Loss: 0.14446024596691132\n",
      "Epoch 0:  24%|██████████████████████▏                                                                    | 1268/5214 [1:00:18<3:07:40,  0.35it/s, v_num=3071, train_loss_step=0.144]batch: 1268 => Loss: 0.15455082058906555\n",
      "Epoch 0:  24%|██████████████████████▏                                                                    | 1269/5214 [1:00:21<3:07:37,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 1269 => Loss: 0.18302033841609955\n",
      "Epoch 0:  24%|██████████████████████▏                                                                    | 1270/5214 [1:00:24<3:07:35,  0.35it/s, v_num=3071, train_loss_step=0.183]batch: 1270 => Loss: 0.17399945855140686\n",
      "Epoch 0:  24%|██████████████████████▏                                                                    | 1271/5214 [1:00:27<3:07:32,  0.35it/s, v_num=3071, train_loss_step=0.174]batch: 1271 => Loss: 0.17591559886932373\n",
      "Epoch 0:  24%|██████████████████████▏                                                                    | 1272/5214 [1:00:29<3:07:29,  0.35it/s, v_num=3071, train_loss_step=0.176]batch: 1272 => Loss: 0.18038409948349\n",
      "Epoch 0:  24%|██████████████████████▏                                                                    | 1273/5214 [1:00:32<3:07:26,  0.35it/s, v_num=3071, train_loss_step=0.180]batch: 1273 => Loss: 0.1351657658815384\n",
      "Epoch 0:  24%|██████████████████████▏                                                                    | 1274/5214 [1:00:35<3:07:23,  0.35it/s, v_num=3071, train_loss_step=0.135]batch: 1274 => Loss: 0.11575362831354141\n",
      "Epoch 0:  24%|██████████████████████▎                                                                    | 1275/5214 [1:00:38<3:07:20,  0.35it/s, v_num=3071, train_loss_step=0.116]batch: 1275 => Loss: 0.1714087575674057\n",
      "Epoch 0:  24%|██████████████████████▎                                                                    | 1276/5214 [1:00:41<3:07:17,  0.35it/s, v_num=3071, train_loss_step=0.171]batch: 1276 => Loss: 0.23334982991218567\n",
      "Epoch 0:  24%|██████████████████████▎                                                                    | 1277/5214 [1:00:44<3:07:15,  0.35it/s, v_num=3071, train_loss_step=0.233]batch: 1277 => Loss: 0.23680825531482697\n",
      "Epoch 0:  25%|██████████████████████▎                                                                    | 1278/5214 [1:00:47<3:07:12,  0.35it/s, v_num=3071, train_loss_step=0.237]batch: 1278 => Loss: 0.1594175100326538\n",
      "Epoch 0:  25%|██████████████████████▎                                                                    | 1279/5214 [1:00:49<3:07:09,  0.35it/s, v_num=3071, train_loss_step=0.159]batch: 1279 => Loss: 0.1762763112783432\n",
      "Epoch 0:  25%|██████████████████████▎                                                                    | 1280/5214 [1:00:52<3:07:06,  0.35it/s, v_num=3071, train_loss_step=0.176]batch: 1280 => Loss: 0.25919222831726074\n",
      "Epoch 0:  25%|██████████████████████▎                                                                    | 1281/5214 [1:00:55<3:07:03,  0.35it/s, v_num=3071, train_loss_step=0.259]batch: 1281 => Loss: 0.21263067424297333\n",
      "Epoch 0:  25%|██████████████████████▎                                                                    | 1282/5214 [1:00:58<3:07:00,  0.35it/s, v_num=3071, train_loss_step=0.213]batch: 1282 => Loss: 0.11849541962146759\n",
      "Epoch 0:  25%|██████████████████████▍                                                                    | 1283/5214 [1:01:01<3:06:58,  0.35it/s, v_num=3071, train_loss_step=0.118]batch: 1283 => Loss: 0.18399956822395325\n",
      "Epoch 0:  25%|██████████████████████▍                                                                    | 1284/5214 [1:01:04<3:06:55,  0.35it/s, v_num=3071, train_loss_step=0.184]batch: 1284 => Loss: 0.20926180481910706\n",
      "Epoch 0:  25%|██████████████████████▍                                                                    | 1285/5214 [1:01:07<3:06:52,  0.35it/s, v_num=3071, train_loss_step=0.209]batch: 1285 => Loss: 0.14756612479686737\n",
      "Epoch 0:  25%|██████████████████████▍                                                                    | 1286/5214 [1:01:09<3:06:49,  0.35it/s, v_num=3071, train_loss_step=0.148]batch: 1286 => Loss: 0.19894392788410187\n",
      "Epoch 0:  25%|██████████████████████▍                                                                    | 1287/5214 [1:01:12<3:06:46,  0.35it/s, v_num=3071, train_loss_step=0.199]batch: 1287 => Loss: 0.13372473418712616\n",
      "Epoch 0:  25%|██████████████████████▍                                                                    | 1288/5214 [1:01:15<3:06:43,  0.35it/s, v_num=3071, train_loss_step=0.134]batch: 1288 => Loss: 0.13670714199543\n",
      "Epoch 0:  25%|██████████████████████▍                                                                    | 1289/5214 [1:01:18<3:06:40,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 1289 => Loss: 0.10605248063802719\n",
      "Epoch 0:  25%|██████████████████████▌                                                                    | 1290/5214 [1:01:21<3:06:37,  0.35it/s, v_num=3071, train_loss_step=0.106]batch: 1290 => Loss: 0.15983395278453827\n",
      "Epoch 0:  25%|██████████████████████▌                                                                    | 1291/5214 [1:01:24<3:06:35,  0.35it/s, v_num=3071, train_loss_step=0.160]batch: 1291 => Loss: 0.12655317783355713\n",
      "Epoch 0:  25%|██████████████████████▌                                                                    | 1292/5214 [1:01:26<3:06:32,  0.35it/s, v_num=3071, train_loss_step=0.127]batch: 1292 => Loss: 0.14005611836910248\n",
      "Epoch 0:  25%|██████████████████████▌                                                                    | 1293/5214 [1:01:29<3:06:29,  0.35it/s, v_num=3071, train_loss_step=0.140]batch: 1293 => Loss: 0.17190420627593994\n",
      "Epoch 0:  25%|██████████████████████▌                                                                    | 1294/5214 [1:01:32<3:06:26,  0.35it/s, v_num=3071, train_loss_step=0.172]batch: 1294 => Loss: 0.15582028031349182\n",
      "Epoch 0:  25%|██████████████████████▌                                                                    | 1295/5214 [1:01:35<3:06:23,  0.35it/s, v_num=3071, train_loss_step=0.156]batch: 1295 => Loss: 0.25732848048210144\n",
      "Epoch 0:  25%|██████████████████████▌                                                                    | 1296/5214 [1:01:38<3:06:21,  0.35it/s, v_num=3071, train_loss_step=0.257]batch: 1296 => Loss: 0.12639953196048737\n",
      "Epoch 0:  25%|██████████████████████▋                                                                    | 1297/5214 [1:01:41<3:06:18,  0.35it/s, v_num=3071, train_loss_step=0.126]batch: 1297 => Loss: 0.12315626442432404\n",
      "Epoch 0:  25%|██████████████████████▋                                                                    | 1298/5214 [1:01:44<3:06:15,  0.35it/s, v_num=3071, train_loss_step=0.123]batch: 1298 => Loss: 0.14181780815124512\n",
      "Epoch 0:  25%|██████████████████████▋                                                                    | 1299/5214 [1:01:47<3:06:12,  0.35it/s, v_num=3071, train_loss_step=0.142]batch: 1299 => Loss: 0.1330510526895523\n",
      "Epoch 0:  25%|██████████████████████▋                                                                    | 1300/5214 [1:01:49<3:06:09,  0.35it/s, v_num=3071, train_loss_step=0.133]batch: 1300 => Loss: 0.14698131382465363\n",
      "Epoch 0:  25%|██████████████████████▋                                                                    | 1301/5214 [1:01:52<3:06:06,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 1301 => Loss: 0.18460297584533691\n",
      "Epoch 0:  25%|██████████████████████▋                                                                    | 1302/5214 [1:01:55<3:06:03,  0.35it/s, v_num=3071, train_loss_step=0.185]batch: 1302 => Loss: 0.12912951409816742\n",
      "Epoch 0:  25%|██████████████████████▋                                                                    | 1303/5214 [1:01:58<3:06:01,  0.35it/s, v_num=3071, train_loss_step=0.129]batch: 1303 => Loss: 0.15244042873382568\n",
      "Epoch 0:  25%|██████████████████████▊                                                                    | 1304/5214 [1:02:01<3:05:58,  0.35it/s, v_num=3071, train_loss_step=0.152]batch: 1304 => Loss: 0.21315915882587433\n",
      "Epoch 0:  25%|██████████████████████▊                                                                    | 1305/5214 [1:02:04<3:05:55,  0.35it/s, v_num=3071, train_loss_step=0.213]batch: 1305 => Loss: 0.07489698380231857\n",
      "Epoch 0:  25%|██████████████████████▌                                                                   | 1306/5214 [1:02:06<3:05:52,  0.35it/s, v_num=3071, train_loss_step=0.0749]batch: 1306 => Loss: 0.13657152652740479\n",
      "Epoch 0:  25%|██████████████████████▊                                                                    | 1307/5214 [1:02:09<3:05:49,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 1307 => Loss: 0.13050507009029388\n",
      "Epoch 0:  25%|██████████████████████▊                                                                    | 1308/5214 [1:02:12<3:05:46,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 1308 => Loss: 0.23786644637584686\n",
      "Epoch 0:  25%|██████████████████████▊                                                                    | 1309/5214 [1:02:15<3:05:43,  0.35it/s, v_num=3071, train_loss_step=0.238]batch: 1309 => Loss: 0.14748752117156982\n",
      "Epoch 0:  25%|██████████████████████▊                                                                    | 1310/5214 [1:02:18<3:05:41,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 1310 => Loss: 0.20666809380054474\n",
      "Epoch 0:  25%|██████████████████████▉                                                                    | 1311/5214 [1:02:21<3:05:38,  0.35it/s, v_num=3071, train_loss_step=0.207]batch: 1311 => Loss: 0.19265101850032806\n",
      "Epoch 0:  25%|██████████████████████▉                                                                    | 1312/5214 [1:02:24<3:05:35,  0.35it/s, v_num=3071, train_loss_step=0.193]batch: 1312 => Loss: 0.11837227642536163\n",
      "Epoch 0:  25%|██████████████████████▉                                                                    | 1313/5214 [1:02:26<3:05:32,  0.35it/s, v_num=3071, train_loss_step=0.118]batch: 1313 => Loss: 0.16836288571357727\n",
      "Epoch 0:  25%|██████████████████████▉                                                                    | 1314/5214 [1:02:29<3:05:29,  0.35it/s, v_num=3071, train_loss_step=0.168]batch: 1314 => Loss: 0.15728525817394257\n",
      "Epoch 0:  25%|██████████████████████▉                                                                    | 1315/5214 [1:02:32<3:05:26,  0.35it/s, v_num=3071, train_loss_step=0.157]batch: 1315 => Loss: 0.1465575248003006\n",
      "Epoch 0:  25%|██████████████████████▉                                                                    | 1316/5214 [1:02:35<3:05:23,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 1316 => Loss: 0.1805277317762375\n",
      "Epoch 0:  25%|██████████████████████▉                                                                    | 1317/5214 [1:02:38<3:05:20,  0.35it/s, v_num=3071, train_loss_step=0.181]batch: 1317 => Loss: 0.1775965690612793\n",
      "Epoch 0:  25%|███████████████████████                                                                    | 1318/5214 [1:02:41<3:05:18,  0.35it/s, v_num=3071, train_loss_step=0.178]batch: 1318 => Loss: 0.17784136533737183\n",
      "Epoch 0:  25%|███████████████████████                                                                    | 1319/5214 [1:02:44<3:05:15,  0.35it/s, v_num=3071, train_loss_step=0.178]batch: 1319 => Loss: 0.17952576279640198\n",
      "Epoch 0:  25%|███████████████████████                                                                    | 1320/5214 [1:02:46<3:05:12,  0.35it/s, v_num=3071, train_loss_step=0.180]batch: 1320 => Loss: 0.2010878622531891\n",
      "Epoch 0:  25%|███████████████████████                                                                    | 1321/5214 [1:02:49<3:05:09,  0.35it/s, v_num=3071, train_loss_step=0.201]batch: 1321 => Loss: 0.16111433506011963\n",
      "Epoch 0:  25%|███████████████████████                                                                    | 1322/5214 [1:02:52<3:05:06,  0.35it/s, v_num=3071, train_loss_step=0.161]batch: 1322 => Loss: 0.11641880124807358\n",
      "Epoch 0:  25%|███████████████████████                                                                    | 1323/5214 [1:02:55<3:05:03,  0.35it/s, v_num=3071, train_loss_step=0.116]batch: 1323 => Loss: 0.14175917208194733\n",
      "Epoch 0:  25%|███████████████████████                                                                    | 1324/5214 [1:02:58<3:05:00,  0.35it/s, v_num=3071, train_loss_step=0.142]batch: 1324 => Loss: 0.11654412746429443\n",
      "Epoch 0:  25%|███████████████████████▏                                                                   | 1325/5214 [1:03:01<3:04:57,  0.35it/s, v_num=3071, train_loss_step=0.117]batch: 1325 => Loss: 0.1980576515197754\n",
      "Epoch 0:  25%|███████████████████████▏                                                                   | 1326/5214 [1:03:03<3:04:55,  0.35it/s, v_num=3071, train_loss_step=0.198]batch: 1326 => Loss: 0.17273758351802826\n",
      "Epoch 0:  25%|███████████████████████▏                                                                   | 1327/5214 [1:03:06<3:04:52,  0.35it/s, v_num=3071, train_loss_step=0.173]batch: 1327 => Loss: 0.20213682949543\n",
      "Epoch 0:  25%|███████████████████████▏                                                                   | 1328/5214 [1:03:09<3:04:49,  0.35it/s, v_num=3071, train_loss_step=0.202]batch: 1328 => Loss: 0.17228853702545166\n",
      "Epoch 0:  25%|███████████████████████▏                                                                   | 1329/5214 [1:03:12<3:04:46,  0.35it/s, v_num=3071, train_loss_step=0.172]batch: 1329 => Loss: 0.1705295890569687\n",
      "Epoch 0:  26%|███████████████████████▏                                                                   | 1330/5214 [1:03:15<3:04:43,  0.35it/s, v_num=3071, train_loss_step=0.171]batch: 1330 => Loss: 0.12799982726573944\n",
      "Epoch 0:  26%|███████████████████████▏                                                                   | 1331/5214 [1:03:18<3:04:40,  0.35it/s, v_num=3071, train_loss_step=0.128]batch: 1331 => Loss: 0.17442741990089417\n",
      "Epoch 0:  26%|███████████████████████▏                                                                   | 1332/5214 [1:03:21<3:04:37,  0.35it/s, v_num=3071, train_loss_step=0.174]batch: 1332 => Loss: 0.11270365864038467\n",
      "Epoch 0:  26%|███████████████████████▎                                                                   | 1333/5214 [1:03:23<3:04:35,  0.35it/s, v_num=3071, train_loss_step=0.113]batch: 1333 => Loss: 0.16815808415412903\n",
      "Epoch 0:  26%|███████████████████████▎                                                                   | 1334/5214 [1:03:26<3:04:32,  0.35it/s, v_num=3071, train_loss_step=0.168]batch: 1334 => Loss: 0.11058356612920761\n",
      "Epoch 0:  26%|███████████████████████▎                                                                   | 1335/5214 [1:03:29<3:04:29,  0.35it/s, v_num=3071, train_loss_step=0.111]batch: 1335 => Loss: 0.16926321387290955\n",
      "Epoch 0:  26%|███████████████████████▎                                                                   | 1336/5214 [1:03:32<3:04:26,  0.35it/s, v_num=3071, train_loss_step=0.169]batch: 1336 => Loss: 0.12521377205848694\n",
      "Epoch 0:  26%|███████████████████████▎                                                                   | 1337/5214 [1:03:35<3:04:23,  0.35it/s, v_num=3071, train_loss_step=0.125]batch: 1337 => Loss: 0.14690418541431427\n",
      "Epoch 0:  26%|███████████████████████▎                                                                   | 1338/5214 [1:03:38<3:04:20,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 1338 => Loss: 0.11081528663635254\n",
      "Epoch 0:  26%|███████████████████████▎                                                                   | 1339/5214 [1:03:40<3:04:17,  0.35it/s, v_num=3071, train_loss_step=0.111]batch: 1339 => Loss: 0.1103498712182045\n",
      "Epoch 0:  26%|███████████████████████▍                                                                   | 1340/5214 [1:03:43<3:04:14,  0.35it/s, v_num=3071, train_loss_step=0.110]batch: 1340 => Loss: 0.15859435498714447\n",
      "Epoch 0:  26%|███████████████████████▍                                                                   | 1341/5214 [1:03:46<3:04:12,  0.35it/s, v_num=3071, train_loss_step=0.159]batch: 1341 => Loss: 0.20153282582759857\n",
      "Epoch 0:  26%|███████████████████████▍                                                                   | 1342/5214 [1:03:49<3:04:09,  0.35it/s, v_num=3071, train_loss_step=0.202]batch: 1342 => Loss: 0.11138999462127686\n",
      "Epoch 0:  26%|███████████████████████▍                                                                   | 1343/5214 [1:03:52<3:04:06,  0.35it/s, v_num=3071, train_loss_step=0.111]batch: 1343 => Loss: 0.11729574203491211\n",
      "Epoch 0:  26%|███████████████████████▍                                                                   | 1344/5214 [1:03:55<3:04:03,  0.35it/s, v_num=3071, train_loss_step=0.117]batch: 1344 => Loss: 0.18125389516353607\n",
      "Epoch 0:  26%|███████████████████████▍                                                                   | 1345/5214 [1:03:58<3:04:00,  0.35it/s, v_num=3071, train_loss_step=0.181]batch: 1345 => Loss: 0.22467906773090363\n",
      "Epoch 0:  26%|███████████████████████▍                                                                   | 1346/5214 [1:04:00<3:03:57,  0.35it/s, v_num=3071, train_loss_step=0.225]batch: 1346 => Loss: 0.12184145301580429\n",
      "Epoch 0:  26%|███████████████████████▌                                                                   | 1347/5214 [1:04:03<3:03:54,  0.35it/s, v_num=3071, train_loss_step=0.122]batch: 1347 => Loss: 0.1927463859319687\n",
      "Epoch 0:  26%|███████████████████████▌                                                                   | 1348/5214 [1:04:06<3:03:52,  0.35it/s, v_num=3071, train_loss_step=0.193]batch: 1348 => Loss: 0.17279170453548431\n",
      "Epoch 0:  26%|███████████████████████▌                                                                   | 1349/5214 [1:04:09<3:03:49,  0.35it/s, v_num=3071, train_loss_step=0.173]batch: 1349 => Loss: 0.12092796713113785\n",
      "Epoch 0:  26%|███████████████████████▌                                                                   | 1350/5214 [1:04:12<3:03:46,  0.35it/s, v_num=3071, train_loss_step=0.121]batch: 1350 => Loss: 0.1541203111410141\n",
      "Epoch 0:  26%|███████████████████████▌                                                                   | 1351/5214 [1:04:15<3:03:43,  0.35it/s, v_num=3071, train_loss_step=0.154]batch: 1351 => Loss: 0.09796839207410812\n",
      "Epoch 0:  26%|███████████████████████▌                                                                   | 1352/5214 [1:04:18<3:03:40,  0.35it/s, v_num=3071, train_loss_step=0.098]batch: 1352 => Loss: 0.1251654326915741\n",
      "Epoch 0:  26%|███████████████████████▌                                                                   | 1353/5214 [1:04:20<3:03:37,  0.35it/s, v_num=3071, train_loss_step=0.125]batch: 1353 => Loss: 0.183319553732872\n",
      "Epoch 0:  26%|███████████████████████▋                                                                   | 1354/5214 [1:04:23<3:03:34,  0.35it/s, v_num=3071, train_loss_step=0.183]batch: 1354 => Loss: 0.17079322040081024\n",
      "Epoch 0:  26%|███████████████████████▋                                                                   | 1355/5214 [1:04:26<3:03:32,  0.35it/s, v_num=3071, train_loss_step=0.171]batch: 1355 => Loss: 0.1789916753768921\n",
      "Epoch 0:  26%|███████████████████████▋                                                                   | 1356/5214 [1:04:29<3:03:29,  0.35it/s, v_num=3071, train_loss_step=0.179]batch: 1356 => Loss: 0.1846163272857666\n",
      "Epoch 0:  26%|███████████████████████▋                                                                   | 1357/5214 [1:04:32<3:03:26,  0.35it/s, v_num=3071, train_loss_step=0.185]batch: 1357 => Loss: 0.21761146187782288\n",
      "Epoch 0:  26%|███████████████████████▋                                                                   | 1358/5214 [1:04:35<3:03:23,  0.35it/s, v_num=3071, train_loss_step=0.218]batch: 1358 => Loss: 0.1254463940858841\n",
      "Epoch 0:  26%|███████████████████████▋                                                                   | 1359/5214 [1:04:38<3:03:20,  0.35it/s, v_num=3071, train_loss_step=0.125]batch: 1359 => Loss: 0.18616853654384613\n",
      "Epoch 0:  26%|███████████████████████▋                                                                   | 1360/5214 [1:04:40<3:03:17,  0.35it/s, v_num=3071, train_loss_step=0.186]batch: 1360 => Loss: 0.14029903709888458\n",
      "Epoch 0:  26%|███████████████████████▊                                                                   | 1361/5214 [1:04:43<3:03:14,  0.35it/s, v_num=3071, train_loss_step=0.140]batch: 1361 => Loss: 0.1344301551580429\n",
      "Epoch 0:  26%|███████████████████████▊                                                                   | 1362/5214 [1:04:46<3:03:12,  0.35it/s, v_num=3071, train_loss_step=0.134]batch: 1362 => Loss: 0.1654931604862213\n",
      "Epoch 0:  26%|███████████████████████▊                                                                   | 1363/5214 [1:04:49<3:03:09,  0.35it/s, v_num=3071, train_loss_step=0.165]batch: 1363 => Loss: 0.1601506471633911\n",
      "Epoch 0:  26%|███████████████████████▊                                                                   | 1364/5214 [1:04:52<3:03:06,  0.35it/s, v_num=3071, train_loss_step=0.160]batch: 1364 => Loss: 0.16778750717639923\n",
      "Epoch 0:  26%|███████████████████████▊                                                                   | 1365/5214 [1:04:55<3:03:03,  0.35it/s, v_num=3071, train_loss_step=0.168]batch: 1365 => Loss: 0.14966514706611633\n",
      "Epoch 0:  26%|███████████████████████▊                                                                   | 1366/5214 [1:04:58<3:03:00,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 1366 => Loss: 0.15576516091823578\n",
      "Epoch 0:  26%|███████████████████████▊                                                                   | 1367/5214 [1:05:00<3:02:57,  0.35it/s, v_num=3071, train_loss_step=0.156]batch: 1367 => Loss: 0.16342388093471527\n",
      "Epoch 0:  26%|███████████████████████▉                                                                   | 1368/5214 [1:05:03<3:02:55,  0.35it/s, v_num=3071, train_loss_step=0.163]batch: 1368 => Loss: 0.18525630235671997\n",
      "Epoch 0:  26%|███████████████████████▉                                                                   | 1369/5214 [1:05:06<3:02:52,  0.35it/s, v_num=3071, train_loss_step=0.185]batch: 1369 => Loss: 0.15905249118804932\n",
      "Epoch 0:  26%|███████████████████████▉                                                                   | 1370/5214 [1:05:09<3:02:49,  0.35it/s, v_num=3071, train_loss_step=0.159]batch: 1370 => Loss: 0.15295347571372986\n",
      "Epoch 0:  26%|███████████████████████▉                                                                   | 1371/5214 [1:05:12<3:02:46,  0.35it/s, v_num=3071, train_loss_step=0.153]batch: 1371 => Loss: 0.18319794535636902\n",
      "Epoch 0:  26%|███████████████████████▉                                                                   | 1372/5214 [1:05:15<3:02:43,  0.35it/s, v_num=3071, train_loss_step=0.183]batch: 1372 => Loss: 0.1905340552330017\n",
      "Epoch 0:  26%|███████████████████████▉                                                                   | 1373/5214 [1:05:18<3:02:40,  0.35it/s, v_num=3071, train_loss_step=0.191]batch: 1373 => Loss: 0.13624601066112518\n",
      "Epoch 0:  26%|███████████████████████▉                                                                   | 1374/5214 [1:05:20<3:02:37,  0.35it/s, v_num=3071, train_loss_step=0.136]batch: 1374 => Loss: 0.1784317046403885\n",
      "Epoch 0:  26%|███████████████████████▉                                                                   | 1375/5214 [1:05:23<3:02:35,  0.35it/s, v_num=3071, train_loss_step=0.178]batch: 1375 => Loss: 0.10450048744678497\n",
      "Epoch 0:  26%|████████████████████████                                                                   | 1376/5214 [1:05:26<3:02:32,  0.35it/s, v_num=3071, train_loss_step=0.105]batch: 1376 => Loss: 0.12794630229473114\n",
      "Epoch 0:  26%|████████████████████████                                                                   | 1377/5214 [1:05:29<3:02:29,  0.35it/s, v_num=3071, train_loss_step=0.128]batch: 1377 => Loss: 0.18520575761795044\n",
      "Epoch 0:  26%|████████████████████████                                                                   | 1378/5214 [1:05:32<3:02:26,  0.35it/s, v_num=3071, train_loss_step=0.185]batch: 1378 => Loss: 0.16605567932128906\n",
      "Epoch 0:  26%|████████████████████████                                                                   | 1379/5214 [1:05:35<3:02:23,  0.35it/s, v_num=3071, train_loss_step=0.166]batch: 1379 => Loss: 0.1472645103931427\n",
      "Epoch 0:  26%|████████████████████████                                                                   | 1380/5214 [1:05:37<3:02:20,  0.35it/s, v_num=3071, train_loss_step=0.147]batch: 1380 => Loss: 0.10923071950674057\n",
      "Epoch 0:  26%|████████████████████████                                                                   | 1381/5214 [1:05:40<3:02:17,  0.35it/s, v_num=3071, train_loss_step=0.109]batch: 1381 => Loss: 0.14081202447414398\n",
      "Epoch 0:  27%|████████████████████████                                                                   | 1382/5214 [1:05:43<3:02:14,  0.35it/s, v_num=3071, train_loss_step=0.141]batch: 1382 => Loss: 0.1726207733154297\n",
      "Epoch 0:  27%|████████████████████████▏                                                                  | 1383/5214 [1:05:46<3:02:11,  0.35it/s, v_num=3071, train_loss_step=0.173]batch: 1383 => Loss: 0.10335829108953476\n",
      "Epoch 0:  27%|████████████████████████▏                                                                  | 1384/5214 [1:05:49<3:02:09,  0.35it/s, v_num=3071, train_loss_step=0.103]batch: 1384 => Loss: 0.17526769638061523\n",
      "Epoch 0:  27%|████████████████████████▏                                                                  | 1385/5214 [1:05:52<3:02:06,  0.35it/s, v_num=3071, train_loss_step=0.175]batch: 1385 => Loss: 0.12701831758022308\n",
      "Epoch 0:  27%|████████████████████████▏                                                                  | 1386/5214 [1:05:55<3:02:03,  0.35it/s, v_num=3071, train_loss_step=0.127]batch: 1386 => Loss: 0.16249224543571472\n",
      "Epoch 0:  27%|████████████████████████▏                                                                  | 1387/5214 [1:05:57<3:02:00,  0.35it/s, v_num=3071, train_loss_step=0.162]batch: 1387 => Loss: 0.19574157893657684\n",
      "Epoch 0:  27%|████████████████████████▏                                                                  | 1388/5214 [1:06:00<3:01:57,  0.35it/s, v_num=3071, train_loss_step=0.196]batch: 1388 => Loss: 0.169326052069664\n",
      "Epoch 0:  27%|████████████████████████▏                                                                  | 1389/5214 [1:06:03<3:01:54,  0.35it/s, v_num=3071, train_loss_step=0.169]batch: 1389 => Loss: 0.15420220792293549\n",
      "Epoch 0:  27%|████████████████████████▎                                                                  | 1390/5214 [1:06:06<3:01:51,  0.35it/s, v_num=3071, train_loss_step=0.154]batch: 1390 => Loss: 0.14918652176856995\n",
      "Epoch 0:  27%|████████████████████████▎                                                                  | 1391/5214 [1:06:09<3:01:49,  0.35it/s, v_num=3071, train_loss_step=0.149]batch: 1391 => Loss: 0.11666536331176758\n",
      "Epoch 0:  27%|████████████████████████▎                                                                  | 1392/5214 [1:06:12<3:01:46,  0.35it/s, v_num=3071, train_loss_step=0.117]batch: 1392 => Loss: 0.1499250829219818\n",
      "Epoch 0:  27%|████████████████████████▎                                                                  | 1393/5214 [1:06:14<3:01:43,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 1393 => Loss: 0.12033021450042725\n",
      "Epoch 0:  27%|████████████████████████▎                                                                  | 1394/5214 [1:06:17<3:01:40,  0.35it/s, v_num=3071, train_loss_step=0.120]batch: 1394 => Loss: 0.18045909702777863\n",
      "Epoch 0:  27%|████████████████████████▎                                                                  | 1395/5214 [1:06:20<3:01:37,  0.35it/s, v_num=3071, train_loss_step=0.180]batch: 1395 => Loss: 0.1539580076932907\n",
      "Epoch 0:  27%|████████████████████████▎                                                                  | 1396/5214 [1:06:23<3:01:34,  0.35it/s, v_num=3071, train_loss_step=0.154]batch: 1396 => Loss: 0.1652807891368866\n",
      "Epoch 0:  27%|████████████████████████▍                                                                  | 1397/5214 [1:06:26<3:01:31,  0.35it/s, v_num=3071, train_loss_step=0.165]batch: 1397 => Loss: 0.16621516644954681\n",
      "Epoch 0:  27%|████████████████████████▍                                                                  | 1398/5214 [1:06:29<3:01:29,  0.35it/s, v_num=3071, train_loss_step=0.166]batch: 1398 => Loss: 0.11524980515241623\n",
      "Epoch 0:  27%|████████████████████████▍                                                                  | 1399/5214 [1:06:32<3:01:26,  0.35it/s, v_num=3071, train_loss_step=0.115]batch: 1399 => Loss: 0.28334707021713257\n",
      "Epoch 0:  27%|████████████████████████▍                                                                  | 1400/5214 [1:06:34<3:01:23,  0.35it/s, v_num=3071, train_loss_step=0.283]batch: 1400 => Loss: 0.20455287396907806\n",
      "Epoch 0:  27%|████████████████████████▍                                                                  | 1401/5214 [1:06:37<3:01:20,  0.35it/s, v_num=3071, train_loss_step=0.205]batch: 1401 => Loss: 0.11396092176437378\n",
      "Epoch 0:  27%|████████████████████████▍                                                                  | 1402/5214 [1:06:40<3:01:17,  0.35it/s, v_num=3071, train_loss_step=0.114]batch: 1402 => Loss: 0.11590193957090378\n",
      "Epoch 0:  27%|████████████████████████▍                                                                  | 1403/5214 [1:06:43<3:01:14,  0.35it/s, v_num=3071, train_loss_step=0.116]batch: 1403 => Loss: 0.14472214877605438\n",
      "Epoch 0:  27%|████████████████████████▌                                                                  | 1404/5214 [1:06:46<3:01:12,  0.35it/s, v_num=3071, train_loss_step=0.145]batch: 1404 => Loss: 0.10050556808710098\n",
      "Epoch 0:  27%|████████████████████████▌                                                                  | 1405/5214 [1:06:49<3:01:09,  0.35it/s, v_num=3071, train_loss_step=0.101]batch: 1405 => Loss: 0.1429259032011032\n",
      "Epoch 0:  27%|████████████████████████▌                                                                  | 1406/5214 [1:06:52<3:01:06,  0.35it/s, v_num=3071, train_loss_step=0.143]batch: 1406 => Loss: 0.12919418513774872\n",
      "Epoch 0:  27%|████████████████████████▌                                                                  | 1407/5214 [1:06:54<3:01:03,  0.35it/s, v_num=3071, train_loss_step=0.129]batch: 1407 => Loss: 0.10947324335575104\n",
      "Epoch 0:  27%|████████████████████████▌                                                                  | 1408/5214 [1:06:57<3:01:00,  0.35it/s, v_num=3071, train_loss_step=0.109]batch: 1408 => Loss: 0.09829594194889069\n",
      "Epoch 0:  27%|████████████████████████▎                                                                 | 1409/5214 [1:07:00<3:00:57,  0.35it/s, v_num=3071, train_loss_step=0.0983]batch: 1409 => Loss: 0.12212049961090088\n",
      "Epoch 0:  27%|████████████████████████▌                                                                  | 1410/5214 [1:07:03<3:00:54,  0.35it/s, v_num=3071, train_loss_step=0.122]batch: 1410 => Loss: 0.15128253400325775\n",
      "Epoch 0:  27%|████████████████████████▋                                                                  | 1411/5214 [1:07:06<3:00:52,  0.35it/s, v_num=3071, train_loss_step=0.151]batch: 1411 => Loss: 0.2176164835691452\n",
      "Epoch 0:  27%|████████████████████████▋                                                                  | 1412/5214 [1:07:09<3:00:49,  0.35it/s, v_num=3071, train_loss_step=0.218]batch: 1412 => Loss: 0.20891252160072327\n",
      "Epoch 0:  27%|████████████████████████▋                                                                  | 1413/5214 [1:07:12<3:00:46,  0.35it/s, v_num=3071, train_loss_step=0.209]batch: 1413 => Loss: 0.13670127093791962\n",
      "Epoch 0:  27%|████████████████████████▋                                                                  | 1414/5214 [1:07:14<3:00:43,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 1414 => Loss: 0.15188439190387726\n",
      "Epoch 0:  27%|████████████████████████▋                                                                  | 1415/5214 [1:07:17<3:00:40,  0.35it/s, v_num=3071, train_loss_step=0.152]batch: 1415 => Loss: 0.11358717828989029\n",
      "Epoch 0:  27%|████████████████████████▋                                                                  | 1416/5214 [1:07:20<3:00:37,  0.35it/s, v_num=3071, train_loss_step=0.114]batch: 1416 => Loss: 0.36676380038261414\n",
      "Epoch 0:  27%|████████████████████████▋                                                                  | 1417/5214 [1:07:23<3:00:35,  0.35it/s, v_num=3071, train_loss_step=0.367]batch: 1417 => Loss: 0.171174094080925\n",
      "Epoch 0:  27%|████████████████████████▋                                                                  | 1418/5214 [1:07:26<3:00:32,  0.35it/s, v_num=3071, train_loss_step=0.171]batch: 1418 => Loss: 0.14252685010433197\n",
      "Epoch 0:  27%|████████████████████████▊                                                                  | 1419/5214 [1:07:29<3:00:29,  0.35it/s, v_num=3071, train_loss_step=0.143]batch: 1419 => Loss: 0.12287886440753937\n",
      "Epoch 0:  27%|████████████████████████▊                                                                  | 1420/5214 [1:07:32<3:00:26,  0.35it/s, v_num=3071, train_loss_step=0.123]batch: 1420 => Loss: 0.2895679175853729\n",
      "Epoch 0:  27%|████████████████████████▊                                                                  | 1421/5214 [1:07:34<3:00:23,  0.35it/s, v_num=3071, train_loss_step=0.290]batch: 1421 => Loss: 0.16983678936958313\n",
      "Epoch 0:  27%|████████████████████████▊                                                                  | 1422/5214 [1:07:37<3:00:20,  0.35it/s, v_num=3071, train_loss_step=0.170]batch: 1422 => Loss: 0.15778741240501404\n",
      "Epoch 0:  27%|████████████████████████▊                                                                  | 1423/5214 [1:07:40<3:00:17,  0.35it/s, v_num=3071, train_loss_step=0.158]batch: 1423 => Loss: 0.13330920040607452\n",
      "Epoch 0:  27%|████████████████████████▊                                                                  | 1424/5214 [1:07:43<3:00:14,  0.35it/s, v_num=3071, train_loss_step=0.133]batch: 1424 => Loss: 0.24675622582435608\n",
      "Epoch 0:  27%|████████████████████████▊                                                                  | 1425/5214 [1:07:46<3:00:12,  0.35it/s, v_num=3071, train_loss_step=0.247]batch: 1425 => Loss: 0.12771232426166534\n",
      "Epoch 0:  27%|████████████████████████▉                                                                  | 1426/5214 [1:07:49<3:00:09,  0.35it/s, v_num=3071, train_loss_step=0.128]batch: 1426 => Loss: 0.11050989478826523\n",
      "Epoch 0:  27%|████████████████████████▉                                                                  | 1427/5214 [1:07:51<3:00:06,  0.35it/s, v_num=3071, train_loss_step=0.111]batch: 1427 => Loss: 0.1300102323293686\n",
      "Epoch 0:  27%|████████████████████████▉                                                                  | 1428/5214 [1:07:54<3:00:03,  0.35it/s, v_num=3071, train_loss_step=0.130]batch: 1428 => Loss: 0.1288454532623291\n",
      "Epoch 0:  27%|████████████████████████▉                                                                  | 1429/5214 [1:07:57<3:00:00,  0.35it/s, v_num=3071, train_loss_step=0.129]batch: 1429 => Loss: 0.15130601823329926\n",
      "Epoch 0:  27%|████████████████████████▉                                                                  | 1430/5214 [1:08:00<2:59:57,  0.35it/s, v_num=3071, train_loss_step=0.151]batch: 1430 => Loss: 0.13620509207248688\n",
      "Epoch 0:  27%|████████████████████████▉                                                                  | 1431/5214 [1:08:03<2:59:54,  0.35it/s, v_num=3071, train_loss_step=0.136]batch: 1431 => Loss: 0.13871318101882935\n",
      "Epoch 0:  27%|████████████████████████▉                                                                  | 1432/5214 [1:08:06<2:59:52,  0.35it/s, v_num=3071, train_loss_step=0.139]batch: 1432 => Loss: 0.22885814309120178\n",
      "Epoch 0:  27%|█████████████████████████                                                                  | 1433/5214 [1:08:09<2:59:49,  0.35it/s, v_num=3071, train_loss_step=0.229]batch: 1433 => Loss: 0.13144369423389435\n",
      "Epoch 0:  28%|█████████████████████████                                                                  | 1434/5214 [1:08:11<2:59:46,  0.35it/s, v_num=3071, train_loss_step=0.131]batch: 1434 => Loss: 0.14358976483345032\n",
      "Epoch 0:  28%|█████████████████████████                                                                  | 1435/5214 [1:08:14<2:59:43,  0.35it/s, v_num=3071, train_loss_step=0.144]batch: 1435 => Loss: 0.15130674839019775\n",
      "Epoch 0:  28%|█████████████████████████                                                                  | 1436/5214 [1:08:17<2:59:40,  0.35it/s, v_num=3071, train_loss_step=0.151]batch: 1436 => Loss: 0.20656493306159973\n",
      "Epoch 0:  28%|█████████████████████████                                                                  | 1437/5214 [1:08:20<2:59:37,  0.35it/s, v_num=3071, train_loss_step=0.207]batch: 1437 => Loss: 0.16065923869609833\n",
      "Epoch 0:  28%|█████████████████████████                                                                  | 1438/5214 [1:08:23<2:59:34,  0.35it/s, v_num=3071, train_loss_step=0.161]batch: 1438 => Loss: 0.1753949224948883\n",
      "Epoch 0:  28%|█████████████████████████                                                                  | 1439/5214 [1:08:26<2:59:32,  0.35it/s, v_num=3071, train_loss_step=0.175]batch: 1439 => Loss: 0.16144226491451263\n",
      "Epoch 0:  28%|█████████████████████████▏                                                                 | 1440/5214 [1:08:29<2:59:29,  0.35it/s, v_num=3071, train_loss_step=0.161]batch: 1440 => Loss: 0.158213809132576\n",
      "Epoch 0:  28%|█████████████████████████▏                                                                 | 1441/5214 [1:08:31<2:59:26,  0.35it/s, v_num=3071, train_loss_step=0.158]batch: 1441 => Loss: 0.136585995554924\n",
      "Epoch 0:  28%|█████████████████████████▏                                                                 | 1442/5214 [1:08:34<2:59:23,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 1442 => Loss: 0.24209216237068176\n",
      "Epoch 0:  28%|█████████████████████████▏                                                                 | 1443/5214 [1:08:37<2:59:20,  0.35it/s, v_num=3071, train_loss_step=0.242]batch: 1443 => Loss: 0.21354590356349945\n",
      "Epoch 0:  28%|█████████████████████████▏                                                                 | 1444/5214 [1:08:40<2:59:17,  0.35it/s, v_num=3071, train_loss_step=0.214]batch: 1444 => Loss: 0.15677395462989807\n",
      "Epoch 0:  28%|█████████████████████████▏                                                                 | 1445/5214 [1:08:43<2:59:14,  0.35it/s, v_num=3071, train_loss_step=0.157]batch: 1445 => Loss: 0.13698969781398773\n",
      "Epoch 0:  28%|█████████████████████████▏                                                                 | 1446/5214 [1:08:46<2:59:12,  0.35it/s, v_num=3071, train_loss_step=0.137]batch: 1446 => Loss: 0.15484078228473663\n",
      "Epoch 0:  28%|█████████████████████████▎                                                                 | 1447/5214 [1:08:49<2:59:09,  0.35it/s, v_num=3071, train_loss_step=0.155]batch: 1447 => Loss: 0.16169002652168274\n",
      "Epoch 0:  28%|█████████████████████████▎                                                                 | 1448/5214 [1:08:51<2:59:06,  0.35it/s, v_num=3071, train_loss_step=0.162]batch: 1448 => Loss: 0.15999500453472137\n",
      "Epoch 0:  28%|█████████████████████████▎                                                                 | 1449/5214 [1:08:54<2:59:03,  0.35it/s, v_num=3071, train_loss_step=0.160]batch: 1449 => Loss: 0.10678930580615997\n",
      "Epoch 0:  28%|█████████████████████████▎                                                                 | 1450/5214 [1:08:57<2:59:00,  0.35it/s, v_num=3071, train_loss_step=0.107]batch: 1450 => Loss: 0.18691101670265198\n",
      "Epoch 0:  28%|█████████████████████████▎                                                                 | 1451/5214 [1:09:00<2:58:57,  0.35it/s, v_num=3071, train_loss_step=0.187]batch: 1451 => Loss: 0.13850657641887665\n",
      "Epoch 0:  28%|█████████████████████████▎                                                                 | 1452/5214 [1:09:03<2:58:55,  0.35it/s, v_num=3071, train_loss_step=0.139]batch: 1452 => Loss: 0.22216077148914337\n",
      "Epoch 0:  28%|█████████████████████████▎                                                                 | 1453/5214 [1:09:06<2:58:52,  0.35it/s, v_num=3071, train_loss_step=0.222]batch: 1453 => Loss: 0.18319861590862274\n",
      "Epoch 0:  28%|█████████████████████████▍                                                                 | 1454/5214 [1:09:09<2:58:49,  0.35it/s, v_num=3071, train_loss_step=0.183]batch: 1454 => Loss: 0.1404302716255188\n",
      "Epoch 0:  28%|█████████████████████████▍                                                                 | 1455/5214 [1:09:11<2:58:46,  0.35it/s, v_num=3071, train_loss_step=0.140]batch: 1455 => Loss: 0.11047251522541046\n",
      "Epoch 0:  28%|█████████████████████████▍                                                                 | 1456/5214 [1:09:14<2:58:43,  0.35it/s, v_num=3071, train_loss_step=0.110]batch: 1456 => Loss: 0.12847848236560822\n",
      "Epoch 0:  28%|█████████████████████████▍                                                                 | 1457/5214 [1:09:17<2:58:40,  0.35it/s, v_num=3071, train_loss_step=0.128]batch: 1457 => Loss: 0.16400866210460663\n",
      "Epoch 0:  28%|█████████████████████████▍                                                                 | 1458/5214 [1:09:20<2:58:37,  0.35it/s, v_num=3071, train_loss_step=0.164]batch: 1458 => Loss: 0.2592501640319824\n",
      "Epoch 0:  28%|█████████████████████████▍                                                                 | 1459/5214 [1:09:23<2:58:35,  0.35it/s, v_num=3071, train_loss_step=0.259]batch: 1459 => Loss: 0.14972448348999023\n",
      "Epoch 0:  28%|█████████████████████████▍                                                                 | 1460/5214 [1:09:26<2:58:32,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 1460 => Loss: 0.25714680552482605\n",
      "Epoch 0:  28%|█████████████████████████▍                                                                 | 1461/5214 [1:09:29<2:58:29,  0.35it/s, v_num=3071, train_loss_step=0.257]batch: 1461 => Loss: 0.14459264278411865\n",
      "Epoch 0:  28%|█████████████████████████▌                                                                 | 1462/5214 [1:09:31<2:58:26,  0.35it/s, v_num=3071, train_loss_step=0.145]batch: 1462 => Loss: 0.15743371844291687\n",
      "Epoch 0:  28%|█████████████████████████▌                                                                 | 1463/5214 [1:09:34<2:58:23,  0.35it/s, v_num=3071, train_loss_step=0.157]batch: 1463 => Loss: 0.1719808429479599\n",
      "Epoch 0:  28%|█████████████████████████▌                                                                 | 1464/5214 [1:09:37<2:58:20,  0.35it/s, v_num=3071, train_loss_step=0.172]batch: 1464 => Loss: 0.1948585957288742\n",
      "Epoch 0:  28%|█████████████████████████▌                                                                 | 1465/5214 [1:09:40<2:58:18,  0.35it/s, v_num=3071, train_loss_step=0.195]batch: 1465 => Loss: 0.14939884841442108\n",
      "Epoch 0:  28%|█████████████████████████▌                                                                 | 1466/5214 [1:09:43<2:58:15,  0.35it/s, v_num=3071, train_loss_step=0.149]batch: 1466 => Loss: 0.1593354195356369\n",
      "Epoch 0:  28%|█████████████████████████▌                                                                 | 1467/5214 [1:09:46<2:58:12,  0.35it/s, v_num=3071, train_loss_step=0.159]batch: 1467 => Loss: 0.14831887185573578\n",
      "Epoch 0:  28%|█████████████████████████▌                                                                 | 1468/5214 [1:09:49<2:58:09,  0.35it/s, v_num=3071, train_loss_step=0.148]batch: 1468 => Loss: 0.155556321144104\n",
      "Epoch 0:  28%|█████████████████████████▋                                                                 | 1469/5214 [1:09:51<2:58:06,  0.35it/s, v_num=3071, train_loss_step=0.156]batch: 1469 => Loss: 0.16905830800533295\n",
      "Epoch 0:  28%|█████████████████████████▋                                                                 | 1470/5214 [1:09:54<2:58:03,  0.35it/s, v_num=3071, train_loss_step=0.169]batch: 1470 => Loss: 0.18655146658420563\n",
      "Epoch 0:  28%|█████████████████████████▋                                                                 | 1471/5214 [1:09:57<2:58:00,  0.35it/s, v_num=3071, train_loss_step=0.187]batch: 1471 => Loss: 0.13205839693546295\n",
      "Epoch 0:  28%|█████████████████████████▋                                                                 | 1472/5214 [1:10:00<2:57:58,  0.35it/s, v_num=3071, train_loss_step=0.132]batch: 1472 => Loss: 0.19133920967578888\n",
      "Epoch 0:  28%|█████████████████████████▋                                                                 | 1473/5214 [1:10:03<2:57:55,  0.35it/s, v_num=3071, train_loss_step=0.191]batch: 1473 => Loss: 0.14178137481212616\n",
      "Epoch 0:  28%|█████████████████████████▋                                                                 | 1474/5214 [1:10:06<2:57:52,  0.35it/s, v_num=3071, train_loss_step=0.142]batch: 1474 => Loss: 0.16543136537075043\n",
      "Epoch 0:  28%|█████████████████████████▋                                                                 | 1475/5214 [1:10:09<2:57:49,  0.35it/s, v_num=3071, train_loss_step=0.165]batch: 1475 => Loss: 0.12261109799146652\n",
      "Epoch 0:  28%|█████████████████████████▊                                                                 | 1476/5214 [1:10:11<2:57:46,  0.35it/s, v_num=3071, train_loss_step=0.123]batch: 1476 => Loss: 0.13761897385120392\n",
      "Epoch 0:  28%|█████████████████████████▊                                                                 | 1477/5214 [1:10:14<2:57:43,  0.35it/s, v_num=3071, train_loss_step=0.138]batch: 1477 => Loss: 0.17143605649471283\n",
      "Epoch 0:  28%|█████████████████████████▊                                                                 | 1478/5214 [1:10:17<2:57:41,  0.35it/s, v_num=3071, train_loss_step=0.171]batch: 1478 => Loss: 0.16082976758480072\n",
      "Epoch 0:  28%|█████████████████████████▊                                                                 | 1479/5214 [1:10:20<2:57:38,  0.35it/s, v_num=3071, train_loss_step=0.161]batch: 1479 => Loss: 0.21736465394496918\n",
      "Epoch 0:  28%|█████████████████████████▊                                                                 | 1480/5214 [1:10:23<2:57:35,  0.35it/s, v_num=3071, train_loss_step=0.217]batch: 1480 => Loss: 0.1892172396183014\n",
      "Epoch 0:  28%|█████████████████████████▊                                                                 | 1481/5214 [1:10:26<2:57:32,  0.35it/s, v_num=3071, train_loss_step=0.189]batch: 1481 => Loss: 0.15040884912014008\n",
      "Epoch 0:  28%|█████████████████████████▊                                                                 | 1482/5214 [1:10:29<2:57:29,  0.35it/s, v_num=3071, train_loss_step=0.150]batch: 1482 => Loss: 0.20904949307441711\n",
      "Epoch 0:  28%|█████████████████████████▉                                                                 | 1483/5214 [1:10:31<2:57:26,  0.35it/s, v_num=3071, train_loss_step=0.209]batch: 1483 => Loss: 0.11884503811597824\n",
      "Epoch 0:  28%|█████████████████████████▉                                                                 | 1484/5214 [1:10:34<2:57:24,  0.35it/s, v_num=3071, train_loss_step=0.119]batch: 1484 => Loss: 0.13503426313400269\n",
      "Epoch 0:  28%|█████████████████████████▉                                                                 | 1485/5214 [1:10:37<2:57:21,  0.35it/s, v_num=3071, train_loss_step=0.135]batch: 1485 => Loss: 0.15705643594264984\n",
      "Epoch 0:  29%|█████████████████████████▉                                                                 | 1486/5214 [1:10:40<2:57:18,  0.35it/s, v_num=3071, train_loss_step=0.157]batch: 1486 => Loss: 0.11152088642120361\n",
      "Epoch 0:  29%|█████████████████████████▉                                                                 | 1487/5214 [1:10:43<2:57:15,  0.35it/s, v_num=3071, train_loss_step=0.112]batch: 1487 => Loss: 0.1691792756319046\n",
      "Epoch 0:  29%|█████████████████████████▉                                                                 | 1488/5214 [1:10:46<2:57:12,  0.35it/s, v_num=3071, train_loss_step=0.169]batch: 1488 => Loss: 0.18897421658039093\n",
      "Epoch 0:  29%|█████████████████████████▉                                                                 | 1489/5214 [1:10:49<2:57:09,  0.35it/s, v_num=3071, train_loss_step=0.189]batch: 1489 => Loss: 0.2241935282945633\n",
      "Epoch 0:  29%|██████████████████████████                                                                 | 1490/5214 [1:10:51<2:57:06,  0.35it/s, v_num=3071, train_loss_step=0.224]"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1fa2d1-c995-479f-8188-6d0ed31234eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6073754b-7080-4222-8083-b84237a8b5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
