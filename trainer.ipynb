{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "472bd508-eefb-44cf-b137-edf503d5466a",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb0d977-e565-431b-98d4-19e7a2280bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import random\n",
    "import requests\n",
    "from transformers import AutoTokenizer, PretrainedConfig, CLIPTextModel, CLIPImageProcessor\n",
    "import diffusers\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    ControlNetModel,\n",
    "    DDPMScheduler,\n",
    "    StableDiffusionControlNetPipeline, \n",
    "    StableDiffusionControlNetImg2ImgPipeline,\n",
    "    UNet2DConditionModel,\n",
    "    UniPCMultistepScheduler,\n",
    ")\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torch, torchvision\n",
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import lightning as L\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "import torchvision.transforms as Tvt\n",
    "from torchvision.models.optical_flow import raft_small, raft_large\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54940875-fbb9-4f73-8314-0df92325358e",
   "metadata": {},
   "source": [
    "## Dataset & Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96897216-c4d7-48f6-8445-1eb2c86706ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_dataset(Dataset):\n",
    "    def __init__(self, train_dir, temporal_radius = 1):\n",
    "        self.train_dir = train_dir\n",
    "        self.temporal_radius = temporal_radius\n",
    "        self.video_names = os.listdir(os.path.join(train_dir, \"train_sharp\"))\n",
    "        self.eligible_frames = [i for i in range(self.temporal_radius, 100-self.temporal_radius)]\n",
    "        self.n_videos = len(self.video_names)\n",
    "        self.n_eligible_frames = len(self.eligible_frames)\n",
    "        self.n_total_eligible_images = self.n_videos * self.n_eligible_frames\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stable-diffusion-x4-upscaler\", subfolder=\"tokenizer\")\n",
    "        \n",
    "        self.lr_h_bound = 180 - 128\n",
    "        self.lr_w_bound = 320 - 128\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_total_eligible_images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid_name = '{:03d}'.format(idx//self.n_eligible_frames)\n",
    "        frame_name = '{:08d}.png'.format(self.temporal_radius + idx%self.n_eligible_frames)\n",
    "        lr_iminus1_frame_name = '{:08d}.png'.format(self.temporal_radius + (idx%self.n_eligible_frames)-1)\n",
    "        lr_iplus1_frame_name = '{:08d}.png'.format(self.temporal_radius + (idx%self.n_eligible_frames)+1)\n",
    "        \n",
    "        hr_frame = os.path.join(self.train_dir, \"train_sharp\", vid_name, frame_name)\n",
    "        lr_frame = os.path.join(self.train_dir, \"train_sharp_bicubic\", \"X4\", vid_name, frame_name)\n",
    "        lr_iminus1_frame = os.path.join(self.train_dir, \"train_sharp_bicubic\", \"X4\", vid_name, lr_iminus1_frame_name)\n",
    "        lr_iplus1_frame = os.path.join(self.train_dir, \"train_sharp_bicubic\", \"X4\", vid_name, lr_iplus1_frame_name)\n",
    "\n",
    "        hr_img = torchvision.io.read_image(hr_frame)\n",
    "        lr_img = torchvision.io.read_image(lr_frame)\n",
    "        lr_iminus1_img = torchvision.io.read_image(lr_iminus1_frame)\n",
    "        lr_iplus1_img = torchvision.io.read_image(lr_iplus1_frame)\n",
    "\n",
    "        ## Random Crop\n",
    "        x = random.randint(0, self.lr_h_bound)\n",
    "        y = random.randint(0, self.lr_w_bound)\n",
    "\n",
    "        hr_img = hr_img[:, x*4:(x*4)+512, y*4:y*4+512]\n",
    "        lr_img = lr_img[:, x:x+128, y:y+128]\n",
    "        lr_iminus1_img = lr_iminus1_img[:, x:x+128, y:y+128]\n",
    "        lr_iplus1_img = lr_iplus1_img[:, x:x+128, y:y+128]\n",
    "\n",
    "        captions = [\"\"]\n",
    "        text_inputs = self.tokenizer(captions, max_length=self.tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "        \n",
    "        return {\"hr_img\": hr_img, \"lr_img\": lr_img, \"lr_iminus1_img\": lr_iminus1_img, \"lr_iplus1_img\": lr_iplus1_img, \"text_encoder_inp_ids\": text_inputs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3a2456-57a1-4405-b01a-8a9573506af3",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a59cc88-8315-4054-a14d-f2e2dc186712",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VSRDiffuser(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model_id = \"stabilityai/stable-diffusion-x4-upscaler\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, subfolder=\"tokenizer\")\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(self.model_id, subfolder=\"text_encoder\")\n",
    "        self.noise_scheduler = DDPMScheduler.from_pretrained(self.model_id, subfolder=\"scheduler\")\n",
    "        self.vae = AutoencoderKL.from_pretrained(self.model_id, subfolder=\"vae\")\n",
    "        self.unet = UNet2DConditionModel.from_pretrained(self.model_id, subfolder=\"unet\") #learn\n",
    "\n",
    "\n",
    "        self.weight_dtype = torch.float32 \n",
    "        \n",
    "        #Conv for (i-1)th frame\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 3, stride=1, padding=1) #learn\n",
    "        #Conv for (i)th frame\n",
    "        self.conv2 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 3, stride=1, padding=1) #learn\n",
    "        #Conv for (i+1)th frame\n",
    "        self.conv3 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 3, stride=1, padding=1) #learn\n",
    "        #Pointwise Conv on all three Convs\n",
    "        self.conv4 = nn.Conv2d(in_channels = 9, out_channels = 3, kernel_size = 1, stride=1, padding=0) #learn\n",
    "        \n",
    "        #RAFT Model for Optical Flow estimation model (RAFT SMALL/RAFT LARGE)\n",
    "        self.RAFT = raft_small(pretrained=True, progress=False)\n",
    "        #self.RAFT = raft_large(pretrained=True, progress=False)\n",
    "\n",
    "        self.transforms1 = Tvt.Compose(\n",
    "            [\n",
    "                Tvt.ConvertImageDtype(self.weight_dtype),\n",
    "                Tvt.Normalize(mean=0.5, std=0.5)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Steps\n",
    "        # 1. Optical Flow b/w i and i-1th frames and motion compensation\n",
    "        # 2. Optical Flow b/w i and i+1th frames and motion compensation\n",
    "        # 3. Depthwise Sepearable and Pointwise seperable covolutions - conv1 to conv4\n",
    "        # 4. HR image -> VAE Encoder -> HR latent\n",
    "        # 5. Sample noise and convert HR latents -> noised latents\n",
    "        # 6. Take the denoising step with Unet\n",
    "        # 7. Calculate the loss and do back propagation\n",
    "\n",
    "        batch_lr_i = self.transforms1(batch[\"lr_img\"])\n",
    "        batch_lr_iminus1 = self.transforms1(batch[\"lr_iminus1_img\"])\n",
    "        batch_lr_iplus1 = self.transforms1(batch[\"lr_iplus1_img\"])\n",
    "        batch_hr_i = self.transforms1(batch[\"hr_img\"])\n",
    "        batch_text_input_ids = batch[\"text_encoder_inp_ids\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            list_of_flows_iminus1 = self.RAFT(batch_lr_iminus1, batch_lr_i)[-1]\n",
    "            list_of_flows_iplus1 = self.RAFT(batch_lr_iplus1, batch_lr_i)[-1]\n",
    "        \n",
    "        LRiminus1_hat = torch.nn.functional.grid_sample(batch_lr_iminus1, list_of_flows_iminus1.permute(0, 2, 3, 1))\n",
    "        LRiplus1_hat = torch.nn.functional.grid_sample(batch_lr_iplus1, list_of_flows_iplus1.permute(0, 2, 3, 1))\n",
    "\n",
    "        condition = self.conv4(torch.cat([self.conv1(LRiminus1_hat), self.conv2(batch_lr_i), self.conv3(LRiplus1_hat)], dim = 1))\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            hr_latent = self.vae.encode(batch_hr_i).latent_dist.sample()\n",
    "            hr_latent = hr_latent * self.vae.config.scaling_factor\n",
    "\n",
    "        bs = hr_latent.shape[0]\n",
    "        \n",
    "        encoder_hidden_states = self.text_encoder(batch_text_input_ids, return_dict=False)[0]\n",
    "        \n",
    "        timesteps = torch.randint(0, self.noise_scheduler.config.num_train_timesteps, (bs,), dtype=torch.int64).to('cuda')\n",
    "        noise = torch.randn(hr_latent.shape).to('cuda')\n",
    "        \n",
    "        noisy_hr_latent = self.noise_scheduler.add_noise(hr_latent, noise, timesteps)\n",
    "    \n",
    "        unet_input = torch.cat([noisy_hr_latent, condition], dim = 1)\n",
    "\n",
    "        noise_pred = self.unet(unet_input, timesteps, encoder_hidden_states, class_labels = torch.zeros(1).to(torch.int).to('cuda'), return_dict=False)[0]\n",
    "        \n",
    "        target = self.noise_scheduler.get_velocity(hr_latent, noise, timesteps)\n",
    "        \n",
    "        loss = torch.nn.functional.mse_loss(noise_pred.float(), target.float(), reduction=\"mean\")\n",
    "        print(\"batch: {} => Loss: {}\".format(batch_idx, loss))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "        optimizer = optimizer_class(\n",
    "            list(self.unet.parameters())\n",
    "            +list(self.conv1.parameters())\n",
    "            +list(self.conv2.parameters())\n",
    "            +list(self.conv3.parameters())\n",
    "            +list(self.conv4.parameters()),\n",
    "            lr=1e-5,\n",
    "            betas=(0.9, 0.999),\n",
    "            weight_decay=1e-2,\n",
    "            eps=1e-08,\n",
    "        )\n",
    "        return optimizer\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a8ba1-d8d5-453c-b7ed-cd90f5d26f7d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70e3f4e9-27f4-47d5-8476-4b70452d9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_dataset(train_dir = \"data/train\", temporal_radius = 1)\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded0236-1342-4939-9618-8b4c0dbceeed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61b02a8-7c44-4be2-b420-bc123126039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VSRDiffuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68cfa5cc-c87f-4b06-996e-d538d9e218f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.RAFT.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.vae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.text_encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71bf845-3ff6-4952-925e-706690943918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "211ba71a-8e6d-4e21-be6c-dfbe8d9f3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(project='VSR')\n",
    "checkpoint_callback = ModelCheckpoint(dirpath='checkpoints/unet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03105a1-a5ce-43f7-9d20-31fe7a676440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5315c9f3-8e6f-4c0f-b924-9d4d1543f1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(accelerator=\"gpu\", devices=1, strategy=\"ddp_notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1460cad-b18f-4f66-8b17-d225892d6430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type                 | Params\n",
      "------------------------------------------------------\n",
      "0 | text_encoder | CLIPTextModel        | 340 M \n",
      "1 | vae          | AutoencoderKL        | 55.3 M\n",
      "2 | unet         | UNet2DConditionModel | 473 M \n",
      "3 | conv1        | Conv2d               | 84    \n",
      "4 | conv2        | Conv2d               | 84    \n",
      "5 | conv3        | Conv2d               | 84    \n",
      "6 | conv4        | Conv2d               | 30    \n",
      "7 | RAFT         | RAFT                 | 990 K \n",
      "------------------------------------------------------\n",
      "473 M     Trainable params\n",
      "396 M     Non-trainable params\n",
      "870 M     Total params\n",
      "3,480.409 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                                                             | 0/6517 [00:00<?, ?it/s]batch: 0 => Loss: 0.5107361078262329\n",
      "Epoch 0:   0%|                                                                                                                         | 1/6517 [00:03<5:26:43,  0.33it/s, v_num=53]"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 74, in _wrap\n    fn(i, *args)\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 987, in _run\n    results = self._run_stage()\n              ^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 1033, in _run_stage\n    self.fit_loop.run()\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 205, in run\n    self.advance()\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 363, in advance\n    self.epoch_loop.run(self._data_fetcher)\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 140, in run\n    self.advance(data_fetcher)\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 250, in advance\n    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 190, in run\n    self._optimizer_step(batch_idx, closure)\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 268, in _optimizer_step\n    call._call_lightning_module_hook(\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/core/module.py\", line 1303, in optimizer_step\n    optimizer.step(closure=optimizer_closure)\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 152, in step\n    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/strategies/ddp.py\", line 270, in optimizer_step\n    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 239, in optimizer_step\n    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 122, in optimizer_step\n    return optimizer.step(closure=closure, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 373, in wrapper\n    out = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n    ret = func(self, *args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/optim/adamw.py\", line 161, in step\n    loss = closure()\n           ^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 108, in _wrap_closure\n    closure_result = closure()\n                     ^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 144, in __call__\n    self._result = self.closure(*args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 129, in closure\n    step_output = self._step_fn()\n                  ^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 318, in _training_step\n    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 309, in _call_strategy_hook\n    output = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 390, in training_step\n    return self._forward_redirection(self.model, self.lightning_module, \"training_step\", *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 642, in __call__\n    wrapper_output = wrapper_module(*args, **kwargs)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1519, in forward\n    else self._run_ddp_forward(*inputs, **kwargs)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1355, in _run_ddp_forward\n    return self.module(*inputs, **kwargs)  # type: ignore[index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 635, in wrapped_forward\n    out = method(*_args, **_kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_2746/132663358.py\", line 76, in training_step\n    noise_pred = self.unet(unet_input, timesteps, encoder_hidden_states, class_labels = torch.zeros(1).to(torch.int).to('cuda'), return_dict=False)[0]\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/diffusers/models/unets/unet_2d_condition.py\", line 1292, in forward\n    sample = upsample_block(\n             ^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/diffusers/models/unets/unet_2d_blocks.py\", line 2664, in forward\n    hidden_states = resnet(hidden_states, temb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/diffusers/models/resnet.py\", line 333, in forward\n    hidden_states = self.nonlinearity(hidden_states)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 393, in forward\n    return F.silu(input, inplace=self.inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/functional.py\", line 2072, in silu\n    return torch._C._nn.silu(input)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 21.96 GiB of which 39.06 MiB is free. Including non-PyTorch memory, this process has 21.92 GiB memory in use. Of the allocated memory 21.18 GiB is allocated by PyTorch, and 338.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/multiprocessing.py:144\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m process_context \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mstart_processes(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapping_function,\n\u001b[1;32m    138\u001b[0m     args\u001b[38;5;241m=\u001b[39mprocess_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we will join ourselves to get the process references\u001b[39;00m\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocs \u001b[38;5;241m=\u001b[39m process_context\u001b[38;5;241m.\u001b[39mprocesses\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mprocess_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    147\u001b[0m worker_output \u001b[38;5;241m=\u001b[39m return_queue\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:163\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    161\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n\u001b[1;32m    162\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m original_trace\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[38;5;241m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 74, in _wrap\n    fn(i, *args)\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 580, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 987, in _run\n    results = self._run_stage()\n              ^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 1033, in _run_stage\n    self.fit_loop.run()\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 205, in run\n    self.advance()\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 363, in advance\n    self.epoch_loop.run(self._data_fetcher)\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 140, in run\n    self.advance(data_fetcher)\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 250, in advance\n    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 190, in run\n    self._optimizer_step(batch_idx, closure)\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 268, in _optimizer_step\n    call._call_lightning_module_hook(\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 157, in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/core/module.py\", line 1303, in optimizer_step\n    optimizer.step(closure=optimizer_closure)\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 152, in step\n    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/strategies/ddp.py\", line 270, in optimizer_step\n    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 239, in optimizer_step\n    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 122, in optimizer_step\n    return optimizer.step(closure=closure, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 373, in wrapper\n    out = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n    ret = func(self, *args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/optim/adamw.py\", line 161, in step\n    loss = closure()\n           ^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 108, in _wrap_closure\n    closure_result = closure()\n                     ^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 144, in __call__\n    self._result = self.closure(*args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 129, in closure\n    step_output = self._step_fn()\n                  ^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 318, in _training_step\n    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 309, in _call_strategy_hook\n    output = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 390, in training_step\n    return self._forward_redirection(self.model, self.lightning_module, \"training_step\", *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 642, in __call__\n    wrapper_output = wrapper_module(*args, **kwargs)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1519, in forward\n    else self._run_ddp_forward(*inputs, **kwargs)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 1355, in _run_ddp_forward\n    return self.module(*inputs, **kwargs)  # type: ignore[index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 635, in wrapped_forward\n    out = method(*_args, **_kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_2746/132663358.py\", line 76, in training_step\n    noise_pred = self.unet(unet_input, timesteps, encoder_hidden_states, class_labels = torch.zeros(1).to(torch.int).to('cuda'), return_dict=False)[0]\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/diffusers/models/unets/unet_2d_condition.py\", line 1292, in forward\n    sample = upsample_block(\n             ^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/diffusers/models/unets/unet_2d_blocks.py\", line 2664, in forward\n    hidden_states = resnet(hidden_states, temb)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/diffusers/models/resnet.py\", line 333, in forward\n    hidden_states = self.nonlinearity(hidden_states)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 393, in forward\n    return F.silu(input, inplace=self.inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nd2794/miniconda3/envs/vsr/lib/python3.11/site-packages/torch/nn/functional.py\", line 2072, in silu\n    return torch._C._nn.silu(input)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 21.96 GiB of which 39.06 MiB is free. Including non-PyTorch memory, this process has 21.92 GiB memory in use. Of the allocated memory 21.18 GiB is allocated by PyTorch, and 338.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150feee6-bcdb-495a-86c0-5fec92add644",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0594532-95dd-4837-8f82-5ed90028171a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
